#  Running TPU training workloads on GKE

This reference guide compiles best practices, prescriptive guidance, and code samples for running large-scale machine learning training workloads with [TPU v4 and TPU v5e on Google Kubernetes Engine (GKE)](https://cloud.google.com/tpu/docs/tpus-in-gke).

The guide covers two main topics:
- **Configuring a GKE based environment for large scale training on Cloud TPUs**
  - This section describes how to configure a GKE cluster to optimize it for running large-scale machine learning training workloads on [Cloud TPUs](https://cloud.google.com/tpu).
- **Defining, Submitting, and Monitoring Training Jobs**
  - This section provides guidance on how to define, submit, and manage training jobs using the Kubernetes [JobSet](https://github.com/kubernetes-sigs/jobset) and [Kueue](https://github.com/kubernetes-sigs/kueue) APIs.


## Architecture of the training environment

The diagram below depicts a high-level architecture of the training environment.


![arch](images/training-cluster.png)

The foundation of the environment is a regional, VPC-native GKE cluster. The cluster has two types of node pools: 
- A single node pool with CPU-only nodes and 
- Several [TPU node pools](https://cloud.google.com/kubernetes-engine/docs/concepts/tpus)

This cluster topology supports running both [single-slice and multislice TPU](https://cloud.google.com/tpu/docs/multislice-introduction) training jobs.

Following are the components supporting the environment:

- [Cloud Storage](https://cloud.google.com/storage) buckets for saving training datasets and artifacts produced by training jobs (such as logs and checkpoints)
- [Cloud Artifact Registry](https://cloud.google.com/artifact-registry) for packaging and managing the training, data processing, and other components of a training workload as Docker container images.
- [Vertex AI TensorBoard](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-introduction) for tracking and visualizing training metrics.
- [Cloud Monitoring](https://cloud.google.com/monitoring) for collecting and analyzing non-functional performance metrics
- [Cloud Logging](https://cloud.google.com/logging) for managing logs produced by training workloads.
- Training workloads [impersonate an Identity and Access Management (IAM) service accounts](https://cloud.google.com/iam/docs/service-account-impersonation) to access Google Cloud services, such as Cloud Storage and Vertex AI TensorBoard.


## Training workload processing 

The following diagram illustrates the process of submitting and processing training workloads in the training environment.

![training workloads](images/workload-processing.png)

In this guide we advocate using the [Kubernetes JobSet API](https://github.com/kubernetes-sigs/jobset) as the preferred method of coordinating large-scale distributed machine learning training workloads on Kubernetes. When combined with the [Kubernetes Kueue](https://github.com/kubernetes-sigs/kueue) job queuing API, it provides flexible and comprehensive training job orchestration.

The training environment's **Kueue** configuration  consists of a single [ClusterQueue](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/) and multiple [LocalQueues](https://kueue.sigs.k8s.io/docs/concepts/local_queue/). This topology provides basic multi-tenancy and supports managing and prioritizing jobs submitted by multiple teams.

All training workloads are represented as JobSet resources. A JobSet resource may contain multiple job types, such as a core distributed training job and an auxiliary job that manages TensorBoard logs and other artifacts generated by the training job.

JobSet workloads are submitted to a namespaced LocalQueue that points to a ClusterQueue. As illustrated in the diagram, in our reference implementation, there is a single cluster queue.

Kueue monitors when resources (such as TPU slices) required by a workload (JobSet) are available, and then decides when to admit the workload and how to allocate the workload's components to the cluster's node pools. 

For example, a training workload can contain two types of jobs:
- A multislice distributed training job
- A job that uploads TensorBoard logs generated by the training job to Vertex AI TensorBoard

When all the resources required by this workload become available, the training job's workers are started on the requested number of TPU slices. The TensorBoard uploader is started on one of the nodes in the CPU node pool.

If the compute resources required by other submitted workloads are not available, these workloads are queued and scheduled for admission based on the priorities that have been defined in the Kueue configuration.

To submit a JobSet-defined workload, you need to create a YAML JobSet resource definition. There are a few different ways to do this. In this guide, we demonstrate two approaches:
- Using [Kustomize](https://kustomize.io/), which helps you create YAML JobSet resource definitions directly.
- Using  [xpk](https://github.com/google/maxtext/tree/main/xpk), which provides an easy-to-use Python-based CLI.


## Setup 

The deployment process is automated using [Cloud Build](https://cloud.google.com/build), [Terraform](https://cloud.google.com/docs/terraform), and [Kustomize](https://kustomize.io/). The Cloud Build configuration file  defines two deployment stages:


In the first stage a Terraform configuration is applied, which:

- [ ] Creates a network, a subnet, and IP ranges for GKE pods and services.
- [ ] Creates a VPC-native cluster.
- [ ] Creates a node pool with nodes equipped with CPUs only.
- [ ] Creates a specified number of TPU node pools.
- [ ] Creates an IAM service account for Workload Identity and an IAM service account to be used as a custom node pool service account.
- [ ] Configures the cluster for Workload Identity.
- [ ] Creates a Google Cloud Storage bucket.
- [ ] Creates a Vertex TensorBoard instance
- [ ] Creates an Artifact Registry

In the second stage, the [JobSet](https://github.com/kubernetes-sigs/jobset) and [Kueue](https://kueue.sigs.k8s.io/docs/concepts/cluster_queue/) custom resources are installed and Kueue is configured as described in the previous section. 


> [!WARNING]
>  Your project must have sufficient [quota to provision TPU resources](https://cloud.google.com/tpu/docs/quota). Else, you can [request for a higher quota limit](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota).


### Configure pre-requisites

Before submitting the Cloud Build build, you need to:

- [ ] Create a new Google Cloud project or select an existing one.
- [ ] Enable the necessary services.
- [ ] Configure an automation service account and an automation Google Cloud storage bucket.


The following services are required by the base environment:
- `cloudbuild.googleapis.com`
- `artifactregistry.googleapis.com`
- `cloudkms.googleapis.com`
- `cloudresourcemanager.googleapis.com`
- `container.googleapis.com`
- `compute.googleapis.com`
- `container.googleapis.com`
- `iam.googleapis.com`
- `iamcredentials.googleapis.com`
- `serviceusage.googleapis.com`
- `stackdriver.googleapis.com`
- `storage-component.googleapis.com`
- `storage.googleapis.com`
- `sts.googleapis.com`
- `aiplatform.googleapis.com`

You also need a GCS bucket that will be used for managing Terraform state and other Terraform artifacts and a service account that will be impersonated by Terraform when provisioning the environment. The service account should have the following project level roles:
- `iam.securityAdmin`
- `iam.serviceAccountAdmin`
- `compute.networkAdmin`
- `container.admin`
- `iam.serviceAccountUser`
- `storage.admin`
- `artifactregistry.admin`
- `aiplatform.user`


#### Configuring the prerequisites using the bootstrap Terraform 

The prerequisites may need to be configured by your GCP organization administrator. If you have access to a project where you are a project owner, you can configure the prerequisites using the Terraform configuration in the `environment/0-bootstrap` folder.

1. Clone this repo
2. Change the current folder to `ai-infrastructureenvironment/tpu-training-on-gke/environment/0-bootstrap`
3. Copy the `terraform.tfvars.tmpl` file to `terraform.tfvars`
4. Modify the `terraform.tfvars` file to reflect your environment
  - Set `project_id` to your project ID
  - Set `automation_bucket` to the name of a bucket you want to create in your project
  - Set `location` to a location where you want to create the automation bucket
  - Set `automation_sa_name` to the automation service account name in your environment
5. Execute the `terraform init` command
6. Execute the `terraform apply` command

Besides enabling the necessary services and setting up an automation service account and an automation GCS bucket, the Terraform configuration has generated prepopulated template files for configuring the Terraform backend and providers, which can be utilized in the following setup stages. These template files are stored in the `gs://<YOUR-AUTOMATION-BUCKET/providers` folder.

#### Granting Cloud Build impersonating rights

To be able to impersonate the automation service account, the Cloud Build service account needs to have the `iam.serviceAccountTokenCreator` rights on the automation service account.

```shell
AUTOMATION_SERVICE_ACCOUNT=<AUTOMATTION_SERVICE_ACOUNT_EMAIL>
CLOUD_BUILD_SERVICE_ACCOUNT=<PROJECT_NUMBER>@cloudbuild.gserviceaccount.com

gcloud iam service-accounts add-iam-policy-binding $AUTOMATION_SERVICE_ACCOUNT --member="serviceAccount:$CLOUD_BUILD_SERVICE_ACCOUNT" --role='roles/iam.serviceAccountTokenCreator'
```

Replace <PROJECT_NUMBER> with your project number. Replace <AUTOMATION_SERVICE_ACCOUNT_EMAIL> with the email of your automation service account. If you created the automation service account using the bootstrap Terraform you can retrieve its email by executing the `terraform output automation_sa` command from the `environment\0-bootstrap` folder.


### Deploy 

#### Clone the GitHub repo. 

If you haven't already run the bootstrap stage, please clone this repository now.

```bash
git clone https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples.git
```

Change the current directory, to `ai-infrastructure/tpu-training-on-gke/environment`.

#### Configure build parameters

If you used the `bootstrap` configuration to configure the prerequisites, copy the `providers\providers.tf` and `providers\backend.tf` files from the `providers` folder in your automation bucket to the `1-base-environment` folder. Modify the `backend.tf` by setting the `prefix` field to the name of a folder in the automation bucket where you want to store your Terraform configuration's state. For example, if you want to manage the Terraform state in the `tf_state/gke_tpu_training` subfolder of the automation bucket set the `prefix` field to `tf_state/gke_tpu_training`.

If the automation bucket and the automation service account were provided to you by your administrator, rename the `backend.tf.tmpl` and the `providers.tf.tmpl` files to `backend.tf` and `providers.tf` and update them with your settings.

To configure the Terraform steps in the build, rename the `terraform.tfvars.tmpl` file in the `1-base-infrastructure` folder to `terraform.tfvars`. Make modifications to the `terraform.tfvars` file to align it with your specific environment. At the very least, you should set the following variables:

- `project_id` - your project ID
- `region` - your region for a VPC and a GKE cluster
- `prefix` - the prefix that will be added to the default names of resources provisioned by the configuration
- `tensorboard_config.region` - the region of a TensorBoard instance
- `cpu_node_pools` - The `terraform.tfvars.tmpl` template provides an example configuration for a single autoscaling node pool.  
- `tpu_node_pools` - The  template shows an example configuration for two TPU node pools with  v4-16 pod slices. Modify the `tpu_node_pools` variable to provision different TPU node pool configurations, as described below.

If you wish to modify other default settings, such as the default name suffixes for a cluster or GCS bucket names, you can override the defaults specified in the `variables.tf` file within your `terraform.tfvars` file.

When configuring TPU node pools, ensure that you set the TPU type to one of the following values:


| TPU type name | Slice type | Slice topology | TPU VM type | Number of VMs in a slice | Number of chips in a VM |
| ------------- | -----------|----------------|-------------|--------------------------| ------------------------|
| v5litepod-16 | tpu-v5-lite-podslice | 4x4 | ct5lp-hightpu-4t | 4 | 4 |
| v5litepod-32 | tpu-v5-lite-podslice | 4x8 | ct5lp-hightpu-4t | 8 | 4 |
| v5litepod-64 | tpu-v5-lite-podslice | 8x8 | ct5lp-hightpu-4t | 16 | 4 |
| v5litepod-128 | tpu-v5-lite-podslice | 8x16 | ct5lp-hightpu-4t | 32 | 4 |
| v5litepod-256 | tpu-v5-lite-podslice | 26x16 | ct5lp-hightpu-4t | 64 | 4 |
| v4-8| tpu-v4-podslice | 2x2x1 | ct4p-hightpu-4t | 1 | 4 |
| v4-16| tpu-v4-podslice | 2x2x2 | ct4p-hightpu-4t | 2 | 4 |
| v4-32| tpu-v4-podslice | 2x2x4 | ct4p-hightpu-4t | 4 | 4 |
| v4-64| tpu-v4-podslice | 2x4x4 | ct4p-hightpu-4t | 8 | 4 |
| v4-128| tpu-v4-podslice | 4x4x4 | ct4p-hightpu-4t | 16 | 4 |
| v4-256| tpu-v4-podslice | 4x4x8 | ct4p-hightpu-4t | 32| 4 |
| v4-512| tpu-v4-podslice | 4x8x8 | ct4p-hightpu-4t | 64 | 4 |
| v4-1024| tpu-v4-podslice | 8x8x8 | ct4p-hightpu-4t | 128 | 4 |
| v4-1536| tpu-v4-podslice | 8x8x12 | ct4p-hightpu-4t | 192 | 4 |
| v4-2048| tpu-v4-podslice | 8x8x16 | ct4p-hightpu-4t | 256 | 4 |
| v4-4096| tpu-v4-podslice | 8x16x16 | ct4p-hightpu-4t | 512 | 4 |


You also need to set a couple of parameters that configure the installation and configuration of the **JobSet** and **Kueue** APIs.

These parameters are passed to Cloud Build through [Cloud Build substitutions](https://cloud.google.com/build/docs/configuring-builds/substitute-variable-values). 

- `JOBSET_API_VERSION` - the version of the [JobSet API](https://github.com/kubernetes-sigs/jobset/releases) to install. The examples in this repo have been tested with `v0.3.0`
- `KUEUE_API_VERSION` - the version of the [Kueue API](https://github.com/kubernetes-sigs/kueue/releases) to install. The examples have been test with `v0.5.1`


#### Submit the build

To initiate the build, execute the following command:

```
export JOBSET_API_VERSION=v0.3.0
export KUEUE_API_VERSION=v0.5.1 

gcloud builds submit \
  --config cloudbuild.provision.yaml \
  --substitutions _JOBSET_API_VERSION=$JOBSET_API_VERSION,_KUEUE_API_VERSION=$KUEUE_API_VERSION \
  --timeout "2h" \
  --machine-type=e2-highcpu-32 
```

To track the progress of the build, you can either follow the link displayed in Cloud Shell or visit the Cloud Build page on the [Google Cloud Console](https://console.cloud.google.com/cloud-build).


## Training workloads examples

The [`examples`](examples/) folder contains code samples that demonstrate how to configure, submit and manage a number of different training workloads.

- [Examples (`examples/`)](examples/README.md) demonstrates two approaches to orchestrate large-scale distributed training workloads on GKE using JobSet and **`xpk`**
  - [JobSet (`examples/jobset`)](examples/jobset/README.md) shows configuring and running training workloads with **JobSet** and **Kueue** APIs using **Kustomize**.
  - [xpk (`examples/xpk`)](examples/xpk/README.md) (Accelerated Processing Kit) shows examples of configuring and running training workloads using **xpk** for same as with JobSet APIs. 

> [!IMPORTANT]
> Refer to the [README](main/examples/README.md) in `examples` folder for detailed instructions.

## Cleanup Environment 

To destroy the environment and clean up all the provisioned resources:

```bash
gcloud builds submit \
  --config cloudbuild.destroy.yaml \
  --timeout "2h" \
  --machine-type=e2-highcpu-32 
```


