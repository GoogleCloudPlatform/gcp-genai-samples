{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9K302jR6r3yU",
      "metadata": {
        "id": "9K302jR6r3yU"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cfc1dcf",
      "metadata": {},
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Ken Lee](https://github.com/kenleejr) |\n",
        "| Reviewers(s) | [Abhishek Bhagwat](https://github.com/Abhishekbhagwat)|\n",
        "| Last updated | 2024-10-07 |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TMrCwE6KVsW3",
      "metadata": {
        "collapsed": true,
        "id": "TMrCwE6KVsW3"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq llama-index \\\n",
        "llama-index-llms-vertex \\\n",
        "llama-index-embeddings-vertex \\\n",
        "python-louvain \\\n",
        "tiktoken \\\n",
        "aiofiles \\\n",
        "annotated-types \\\n",
        "python-fasthtml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HFXYv9sOVkYq",
      "metadata": {
        "id": "HFXYv9sOVkYq"
      },
      "source": [
        "# RAG Evaluation Dataset Curation\n",
        "Once businesses start to expose a Chatbot or RAG application to real end-users, they will start to observe real queries and conversations. The questions users ask these bots can reveal so much about users, the products they are using/purchasing, and the business itself. This data is valuable for both understanding users and their needs and assessing if the Chatbot you've built is working properly over time. \n",
        "\n",
        "The goal of this notebook is to use Gemini to turbo-charge analysis and summarization of real user queries and conversations from an in-production RAG system/Chatbot. We can then use this analysis to identify a representative set of questions which can be used as an evaluation dataset for the RAG system. This notebook can serve as the foundation of a continuous evaluation practice for RAG systems. \n",
        "\n",
        "![image_link](./cluster_diagram.png)\n",
        "\n",
        "Along the way, we want to learn:\n",
        "- What are the general classes of questions people are asking?\n",
        "  - What problems do people have?\n",
        "- What topics are being discussed?\n",
        "- What sentiments are being expressed?\n",
        "\n",
        "This notebook was inspired by an article by [Weights and Biases](https://wandb.ai/wandbot/wandbot-eval/reports/How-to-Evaluate-an-LLM-Part-1-Building-an-Evaluation-Dataset-for-our-LLM-System--Vmlldzo1NTAwNTcy). We take some of the ideas and go a few steps further by using Gemini to analyze and extract metadata about the clusters we find and use that to inform our choosing of an evaluation dataset. Gemini's large context allows us to perform EDA on clusters and questions extremely quickly, even for very large question sets. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DGTF2qUYVp0x",
      "metadata": {
        "id": "DGTF2qUYVp0x"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ODApYPsB6nNR",
      "metadata": {
        "id": "ODApYPsB6nNR"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"<enter-your-project-id>\" # @param\n",
        "CSV_PATH = \"./curate_evals_example.csv\" # @param\n",
        "TEST_RUN = False # @param\n",
        "CLUSTERING_NEIGHBORHOOD_SIZE = 5 # @param"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7GLS_O8p1hSW",
      "metadata": {
        "id": "7GLS_O8p1hSW"
      },
      "source": [
        "## Dataset\n",
        "For this demo we are using a hypothetical dataset of questions concerning Google Cloud Services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "I9Ll-ZSrQtcI",
      "metadata": {
        "id": "I9Ll-ZSrQtcI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "if TEST_RUN:\n",
        "  df = pd.DataFrame({\"Prompt\": [\"What is RAG?\", \"What is life?\", \"What is football?\", \"Who am I?\"],\n",
        "                   \"answer\": [\"Retrieval Augmented Generation\", \"Love\", \"National Football League\", \"Human\"]})\n",
        "else:\n",
        "  df = pd.read_csv(CSV_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cH5k_Mv0ABpW",
      "metadata": {
        "id": "cH5k_Mv0ABpW"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q9nFwjmP72Ka",
      "metadata": {
        "id": "Q9nFwjmP72Ka"
      },
      "source": [
        "### Clean up the dataset\n",
        "- Remove very short and very long queries\n",
        "- Remove near duplicates\n",
        "\n",
        "In a real RAG system you will often see single word queries or typos being entered. Other times, people may copy and paste entire documents on purpose or on accident. We want to remove those..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "l4H8oYXq6wOK",
      "metadata": {
        "id": "l4H8oYXq6wOK"
      },
      "outputs": [],
      "source": [
        "df[\"question_len\"] = df[\"Question\"].apply(lambda x: len(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "TBi-emAT_MdY",
      "metadata": {
        "id": "TBi-emAT_MdY"
      },
      "outputs": [],
      "source": [
        "# Discard questions with too little or too many characters\n",
        "df = df[(df.question_len > 5) & (df.question_len < 1000)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xhgpghHBN7-E",
      "metadata": {
        "id": "xhgpghHBN7-E"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YZP8uPVh4K97",
      "metadata": {
        "id": "YZP8uPVh4K97"
      },
      "outputs": [],
      "source": [
        "df.question_len.hist(bins=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lIB2TGLKRiWu",
      "metadata": {
        "id": "lIB2TGLKRiWu"
      },
      "source": [
        "## Embed the Questions\n",
        "\n",
        "For `TASK_TYPE`, we choose `RETRIEVAL_DOCUMENT` because we will be clustering based on retrieval similarity of questions against each other. From google cloud's documentation:\n",
        "\n",
        "Used to convey intended downstream application to help the model produce better embeddings. If left blank, the default used is RETRIEVAL_QUERY.\n",
        "\n",
        "RETRIEVAL_QUERY\n",
        "\n",
        "RETRIEVAL_DOCUMENT\n",
        "\n",
        "SEMANTIC_SIMILARITY\n",
        "\n",
        "CLASSIFICATION\n",
        "\n",
        "CLUSTERING\n",
        "\n",
        "QUESTION_ANSWERING\n",
        "\n",
        "FACT_VERIFICATION\n",
        "\n",
        "CODE_RETRIEVAL_QUERY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VSOR8rf3TZDq",
      "metadata": {
        "id": "VSOR8rf3TZDq"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from typing import List, Optional,  Tuple\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from google.cloud import storage\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "def embed_text(\n",
        "    texts: List[str] = [\"banana muffins? \", \"banana bread? banana muffins?\"],\n",
        "    task: str = \"RETRIEVAL_DOCUMENT\",\n",
        "    model_name: str = \"text-embedding-004\",\n",
        "    dimensionality: Optional[int] = 768,\n",
        ") -> List[List[float]]:\n",
        "    \"\"\"Embeds texts with a pre-trained, foundational model.\"\"\"\n",
        "\n",
        "    model = TextEmbeddingModel.from_pretrained(model_name)\n",
        "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
        "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
        "    embeddings = model.get_embeddings(inputs, **kwargs)\n",
        "    return [embedding.values for embedding in embeddings]\n",
        "\n",
        "async def embed_text_async(\n",
        "    model: TextEmbeddingModel,\n",
        "    texts: List[str] = [\"banana muffins? \", \"banana bread? banana muffins?\"],\n",
        "    task: str = \"RETRIEVAL_DOCUMENT\",\n",
        "    dimensionality: Optional[int] = 768,):\n",
        "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
        "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
        "    embeddings = await model.get_embeddings_async(texts, **kwargs)\n",
        "    return [embedding.values for embedding in embeddings]\n",
        "\n",
        "model_name = \"text-embedding-004\"\n",
        "embedding_model = TextEmbeddingModel.from_pretrained(model_name)\n",
        "embedded_qs = await tqdm_asyncio.gather(*[embed_text_async(embedding_model,\n",
        "                                        [x[\"Question\"]]) for i, x in df.iterrows()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a3pUtDfnU_UM",
      "metadata": {
        "id": "a3pUtDfnU_UM"
      },
      "outputs": [],
      "source": [
        "embedded_qs_flattened = [q[0] for q in embedded_qs]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kgTbuokcRl1L",
      "metadata": {
        "id": "kgTbuokcRl1L"
      },
      "source": [
        "## Cluster the Questions\n",
        "\n",
        "You can use whatever clustering algorithm you want here but Louvain community detection works well and is very fast."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NAlsWHDXz5v9",
      "metadata": {
        "id": "NAlsWHDXz5v9"
      },
      "source": [
        "### Vector-based Retrieval Clustering\n",
        "1. Store your embedded question set in a vector index\n",
        "2. Query the vector index with each question in the dataset, retrieving a topk-sized neighborhood of questions around the query question.\n",
        "3. Form a graph of questions by adding an edge between the query question and each of the retrieved questions\n",
        "4. Perform Louvain or Leiden community detection on the graph to create clusters of questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ML6i053tW3vQ",
      "metadata": {
        "collapsed": true,
        "id": "ML6i053tW3vQ"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    Settings,\n",
        "    SimpleDirectoryReader,\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        "    Document\n",
        ")\n",
        "from llama_index.llms.vertex import Vertex\n",
        "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
        "from vertexai.generative_models import HarmCategory, HarmBlockThreshold\n",
        "import networkx as nx\n",
        "from community import community_louvain # pip install python-louvain\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "\n",
        "credentials = google.auth.default()[0]\n",
        "request = google.auth.transport.requests.Request()\n",
        "credentials.refresh(request)\n",
        "\n",
        "\n",
        "query_list = df[\"Question\"].tolist()\n",
        "query_docs = [Document(text=t) for t in query_list] # To make it LlamaIndex compatible\n",
        "embed_model = VertexTextEmbedding(credentials=credentials, model_name=\"text-embedding-004\")\n",
        "llm = Vertex(model=\"gemini-1.5-pro\",\n",
        "             temperature=0.2,\n",
        "             max_tokens=8192,\n",
        "             safety_settings={\n",
        "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        }\n",
        ")\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "\n",
        "# Form a local vector index with all our questions\n",
        "vector_index = VectorStoreIndex.from_documents(query_docs)\n",
        "vector_retriever = vector_index.as_retriever(similarity_top_k=CLUSTERING_NEIGHBORHOOD_SIZE)\n",
        "\n",
        "\n",
        "# Create a similarity graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Get a neighborhood of similar questions by querying the vector index\n",
        "similar_texts = await tqdm_asyncio.gather(*[vector_retriever.aretrieve(text) for i, text in enumerate(query_list)])\n",
        "\n",
        "for i, text in enumerate(query_list):\n",
        "  for s in similar_texts[i]:\n",
        "    G.add_edge(text, s.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "UKhkdk7UcV88",
      "metadata": {
        "collapsed": true,
        "id": "UKhkdk7UcV88"
      },
      "outputs": [],
      "source": [
        "# Apply Louvain Community Detection\n",
        "partition = community_louvain.best_partition(G)\n",
        "df[\"cluster_idx\"] = df[\"Question\"].map(partition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ryVyUMhCcb-o",
      "metadata": {
        "id": "ryVyUMhCcb-o"
      },
      "outputs": [],
      "source": [
        "grouped_df = pd.DataFrame(df.groupby(\"cluster_idx\")['Question'].apply(list)).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kyapkNuvKRZZ",
      "metadata": {
        "collapsed": true,
        "id": "kyapkNuvKRZZ"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K5UnIAP7KEX5",
      "metadata": {
        "id": "K5UnIAP7KEX5"
      },
      "outputs": [],
      "source": [
        "grouped_df[\"num_questions\"] = grouped_df[\"Question\"].apply(len)\n",
        "grouped_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RzT0fZC-RsaB",
      "metadata": {
        "id": "RzT0fZC-RsaB"
      },
      "source": [
        "## Analyze Clusters Using Gemini\n",
        "\n",
        "We can use Gemini to extract summaries, topics, relevant questions, sentiment...whatever we want from each cluster.\n",
        "\n",
        "This allows us to quickly identify higher level patterns about what people are asking, what problems they are facing, their frustrations, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8Usbh9JmRvpB",
      "metadata": {
        "id": "8Usbh9JmRvpB"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from vertexai.generative_models import HarmCategory, HarmBlockThreshold\n",
        "from llama_index.core.program import LLMTextCompletionProgram\n",
        "from llama_index.core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Annotated\n",
        "from enum import Enum\n",
        "from annotated_types import Len\n",
        "\n",
        "num_clusters = grouped_df.shape[0]\n",
        "\n",
        "class Sentiment(Enum):\n",
        "  POSITIVE = \"positive\"\n",
        "  NEGATIVE = \"negative\"\n",
        "  NEUTRAL = \"neutral\"\n",
        "\n",
        "class ClusterSummary(BaseModel):\n",
        "  '''A cluster summary, list of topics, most representative questions, and sentiment associated with a cluster of questions from chat sessions.'''\n",
        "  summary_desc: str\n",
        "  topics: List[str]\n",
        "  most_representative_qs: Annotated[List[str], Len(3, 8)]\n",
        "  sentiment: Sentiment\n",
        "\n",
        "\n",
        "boring_prompt = \"\"\"Please provide a brief summary which captures the nature of the given cluster of questions below in the form of \"Questions concerning ____\".\n",
        "                  \\n Cluster questions:\n",
        "                  \\n {questions_list}\n",
        "                  \\n The clusters titles should not be generic such as \"Google Cloud AI\" or \"Gemini\".\n",
        "                  \\n They need to be specific in order to distinguish the clusters from others which may be similar.\n",
        "                  \\n Also include a list of topic phrases which the questions address, the most representative questions of the cluster, and an overall sentiment. Be sure to follow a consistent format.\"\"\"\n",
        "\n",
        "movie_prompt = \"\"\"You are an expert movie producer for famous movies.\n",
        "                  \\n Please provide a quipy, movie title which captures the essence of the given cluster of questions below.\n",
        "                  \\n Example:\n",
        "                  \\n How does RAG work on Vertex?\n",
        "                  \\n Where can I find documentation on Vertex AI Generative model API?\n",
        "                  \\n What are the pitfals of Gemini vs. Gemma?\n",
        "                  \\n Answer:\n",
        "                  \\n movie title: \"Into the Vertex\"\n",
        "                  \\n representative qs: How does RAG work on Vertex?\n",
        "                  \\n topics: Vertex AI, Vertex AI Generative Model\n",
        "                  \\n sentiment: neutral\n",
        "                  \\n Cluster questions:\n",
        "                  \\n {questions_list}\n",
        "                  \\n Also include a list of topic phrases which the questions address, the most representative questions of the cluster, and an overall sentiment. Be sure to follow a consistent format. \"\"\"\n",
        "\n",
        "async def summarize_cluster(questions: List[str]):\n",
        "  questions_list = \"\\n\".join(questions)\n",
        "  llm_program = LLMTextCompletionProgram.from_defaults(\n",
        "        output_parser=PydanticOutputParser(ClusterSummary),\n",
        "        prompt_template_str=boring_prompt,\n",
        "        verbose=True,\n",
        "    )\n",
        "  try:\n",
        "    cluster_summary = await llm_program.acall(questions_list=questions_list)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None\n",
        "  return cluster_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9EUj5xwpgMzU",
      "metadata": {
        "id": "9EUj5xwpgMzU"
      },
      "outputs": [],
      "source": [
        "# Summarize each cluster individually\n",
        "cluster_summaries = await tqdm_asyncio.gather(*[summarize_cluster(q[\"Question\"]) for idx, q in grouped_df.iterrows()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3l0OnOUO9pXh",
      "metadata": {
        "collapsed": true,
        "id": "3l0OnOUO9pXh"
      },
      "outputs": [],
      "source": [
        "cluster_summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "k0U7a6nqZeMP",
      "metadata": {
        "id": "k0U7a6nqZeMP"
      },
      "outputs": [],
      "source": [
        "just_summaries = [c.summary_desc if c else None for c in cluster_summaries]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "oLp3ceBVSEo2",
      "metadata": {
        "id": "oLp3ceBVSEo2"
      },
      "outputs": [],
      "source": [
        "df_grouped_by_cluster = df.groupby(\"cluster_idx\").agg(\"count\")\n",
        "df_grouped_by_cluster[\"cluster_summary\"] = cluster_summaries\n",
        "df_grouped_by_cluster[\"just_summary\"] = just_summaries\n",
        "df_grouped_by_cluster[\"questions_list\"] = grouped_df[\"Question\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "tt3kS_tPA6XL",
      "metadata": {
        "id": "tt3kS_tPA6XL"
      },
      "outputs": [],
      "source": [
        "from fasthtml.common import *\n",
        "from fasthtml.fastapp import *\n",
        "from random import sample\n",
        "from fasthtml.components import Zero_md\n",
        "\n",
        "tlink = Script(src=\"https://cdn.tailwindcss.com\")\n",
        "dlink = Link(rel=\"stylesheet\", href=\"https://cdn.jsdelivr.net/npm/daisyui@4.11.1/dist/full.min.css\")\n",
        "app = FastHTML(hdrs=(dlink, tlink))\n",
        "\n",
        "def Markdown(md, css = ''):\n",
        "    css_template = Template(Style(css), data_append=True)\n",
        "    return Zero_md(css_template, Script(md, type=\"text/markdown\"))\n",
        "\n",
        "def MarkdownWOutBackground(md: str):\n",
        "    css = '.markdown-body {background-color: unset !important; color: unset !important;} .markdown-body table {color: black !important;}'\n",
        "    markdown_wout_background = partial(Markdown, css=css)\n",
        "    return markdown_wout_background(md)\n",
        "\n",
        "def stat_card(num_questions: int):\n",
        "  return Div(\n",
        "    Div('Total Questions', cls='stat-title'),\n",
        "    Div(f'{num_questions}', cls='stat-value'),\n",
        "    cls='stat'\n",
        "  )\n",
        "\n",
        "def cluster_card(cluster_summary: ClusterSummary, questions_list: List[str]):\n",
        "  if cluster_summary.sentiment == Sentiment.NEGATIVE:\n",
        "    badge_color = \"error\"\n",
        "  elif cluster_summary.sentiment == Sentiment.NEUTRAL:\n",
        "    badge_color = \"neutral\"\n",
        "  else:\n",
        "    badge_color = \"success\"\n",
        "  return Div(\n",
        "              Div(\n",
        "                  H2(cluster_summary.summary_desc, cls='card-title'),\n",
        "                  Div(\n",
        "                      stat_card(len(questions_list)),\n",
        "                      Div(cluster_summary.sentiment, cls=f'badge badge-{badge_color}'),\n",
        "                      cls=\"flex flex-row items-center\"\n",
        "                  ),\n",
        "                  H4(\"Representative Questions:\", cls=\"font-bold\"),\n",
        "                  Ul(\n",
        "                      *[Li(q) for q in cluster_summary.most_representative_qs],\n",
        "                      cls='list-disc list-inside mt-2'\n",
        "                  ),\n",
        "                  H4(\"Topics Discussed:\", cls=\"font-bold\"),\n",
        "                  Ul(\n",
        "                      *[Li(t) for t in cluster_summary.topics],\n",
        "                      cls='list-disc list-inside mt-2'\n",
        "                  ),\n",
        "                  cls='card-body'\n",
        "              ),\n",
        "              cls='card bg-base-100 shadow-xl'\n",
        "          )\n",
        "\n",
        "@app.get(\"/\")\n",
        "def cluster_analysis():\n",
        "    return Div(\n",
        "              *[cluster_card(c, q) for c, q in zip(cluster_summaries, df_grouped_by_cluster[\"questions_list\"])],\n",
        "              cls=\"grid grid-cols-2 gap-2\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DjLQVD7MRVn1",
      "metadata": {
        "id": "DjLQVD7MRVn1"
      },
      "source": [
        "## Gemini-generated Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a4ZayTxHe8V",
      "metadata": {
        "id": "1a4ZayTxHe8V"
      },
      "outputs": [],
      "source": [
        "from starlette.testclient import TestClient\n",
        "client = TestClient(app)\n",
        "r = client.get(\"/\")\n",
        "show(r.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_azGz91Do81r",
      "metadata": {
        "id": "_azGz91Do81r"
      },
      "source": [
        "## Sample Questions from Each Cluster to create the Eval Dataset\n",
        "- We can sample randomly proportional to each cluster's size\n",
        "- Or we can take samples from the most representative questions Gemini identified\n",
        "\n",
        "Probably need to sit down with an SME and compare both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "vPMNsdMuBWq9",
      "metadata": {
        "collapsed": true,
        "id": "vPMNsdMuBWq9"
      },
      "outputs": [],
      "source": [
        "# Calculate the total number of questions\n",
        "total_questions = df_grouped_by_cluster['question_len'].sum()\n",
        "\n",
        "# Calculate the fraction of questions for each row\n",
        "df_grouped_by_cluster['cluster_fraction'] = df_grouped_by_cluster['question_len'] / total_questions\n",
        "\n",
        "# Function to sample from a list based on the fraction\n",
        "def sample_questions(row, num_samples):\n",
        "    return np.random.choice(row['questions_list'],\n",
        "                            size=int(num_samples * row['cluster_fraction']),\n",
        "                            replace=False).tolist()\n",
        "\n",
        "# Specify the total number of samples you want\n",
        "total_samples = 50\n",
        "\n",
        "# Apply the sampling function to each row\n",
        "df_grouped_by_cluster['proportional_sampled_questions'] = df_grouped_by_cluster.apply(lambda row: sample_questions(row, total_samples), axis=1)\n",
        "\n",
        "# Unroll the DataFrame\n",
        "df_grouped_by_cluster = df_grouped_by_cluster.reset_index()\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "unrolled_proportional_df = df_grouped_by_cluster.apply(lambda x: pd.Series({\n",
        "    'cluster_title': [x[\"just_summary\"]] * len(x['proportional_sampled_questions']),\n",
        "    'sampled_question': x['proportional_sampled_questions']\n",
        "}), axis=1)\n",
        "\n",
        "# Concatenate the series and reset the index\n",
        "unrolled_proportional_df = pd.concat([unrolled_proportional_df['cluster_title'].explode(),\n",
        "                         unrolled_proportional_df['sampled_question'].explode()],\n",
        "                        axis=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nI5MjuDiHZLa",
      "metadata": {
        "collapsed": true,
        "id": "nI5MjuDiHZLa"
      },
      "outputs": [],
      "source": [
        "unrolled_proportional_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "GhK3POqVH1mX",
      "metadata": {
        "collapsed": true,
        "id": "GhK3POqVH1mX"
      },
      "outputs": [],
      "source": [
        "df_grouped_by_cluster[\"gemini_representative_questions_len\"] = df_grouped_by_cluster[\"cluster_summary\"].apply(lambda x: len(x.most_representative_qs))\n",
        "df_grouped_by_cluster[\"gemini_representative_questions\"] = df_grouped_by_cluster[\"cluster_summary\"].apply(lambda x: x.most_representative_qs)\n",
        "# Print the resulting DataFrame\n",
        "unrolled_gemini_df = df_grouped_by_cluster.apply(lambda x: pd.Series({\n",
        "    'cluster_title': [x[\"just_summary\"]] * len(x['gemini_representative_questions']),\n",
        "    'representative_question': x['gemini_representative_questions']\n",
        "}), axis=1)\n",
        "\n",
        "# Concatenate the series and reset the index\n",
        "unrolled_gemini_df = pd.concat([unrolled_gemini_df['cluster_title'].explode(),\n",
        "                         unrolled_gemini_df['representative_question'].explode()],\n",
        "                        axis=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2Wxw8XJtWt",
      "metadata": {
        "id": "8e2Wxw8XJtWt"
      },
      "outputs": [],
      "source": [
        "unrolled_gemini_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90LThbX4ov-5",
      "metadata": {
        "id": "90LThbX4ov-5"
      },
      "source": [
        "### Save Results to CSV\n",
        "- We do need to obtain ground truth answers\n",
        "- But we can be confident we are putting the effort towards relevant, representative questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "l_fF_K2Tebib",
      "metadata": {
        "id": "l_fF_K2Tebib"
      },
      "outputs": [],
      "source": [
        "unrolled_gemini_df.to_csv(\"representative_eval_questions.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UDNXBCiOcLbK",
        "Gw_Og38p4uZE"
      ],
      "name": "curate_new_evals_share.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "curate-evals",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
