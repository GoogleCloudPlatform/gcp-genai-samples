{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacfd162-d52d-4f00-bbca-600e559652b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e59b52-d019-49de-ae70-6a63cf5af190",
   "metadata": {},
   "source": [
    "# ShieldGemma Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8facdd54-2987-4cf5-a8dc-5fb9bf8c10c9",
   "metadata": {},
   "source": [
    "Based on [model_garden_gemma2_deployment_on_vertex.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma2_deployment_on_vertex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fedec98-317e-4fc2-9d94-0be1d96dca62",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-colab&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-colab&destination=evaluation_rag_use_cases-from_notebook-colab&url=https%3A%2F%2Fcolab.sandbox.google.com%2Fgithub%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_evaluation_services%2Fevaluation-rag-systems%2Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-colab_ent&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-colab_ent&destination=evaluation_rag_use_cases-from_notebook-colab_ent&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fcolab%2Fimport%2Fhttps%3A%252F%252Fraw.githubusercontent.com%252FGoogleCloudPlatform%252Fapplied-ai-engineering-samples%252Fmain%252Fgenai-on-vertex-ai%252Fvertex_evaluation_services%252Fevaluation-rag-systems%252Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-github&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-github&destination=evaluation_rag_use_cases-from_notebook-github&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_evaluation_services%2Fevaluation-rag-systems%2Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-vai_workbench&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-vai_workbench&destination=evaluation_rag_use_cases-from_notebook-vai_workbench&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fworkbench%2Fdeploy-notebook%3Fdownload_url%3Dhttps%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_evaluation_services%2Fevaluation-rag-systems%2Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad68f1aa-ab73-46fe-9cac-574b82ab0b7e",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <td>Author(s)</td>\n",
    "    <td>Egon Soares</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc709e3a-61f7-4a88-a289-59d2679cd948",
   "metadata": {},
   "source": [
    "![shieldgemma deployment architecture](images/2.1-ShieldGemma-Deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddea872-6c8b-4fa9-b716-d5dfabbf9873",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying ShieldGemma model on TPU using **Hex-LLM**, a **H**igh-**E**fficiency **L**arge **L**anguage **M**odel serving solution built with **XLA** that is being developed by Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4675b2-e665-4cda-a0ce-5b838b76f224",
   "metadata": {},
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09205b50-8a19-4952-bf7c-de4ba608f9c5",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a08402-45f3-470c-9be3-f6289e98762c",
   "metadata": {},
   "source": [
    "By default, the quota for TPU deployment `Custom model serving TPU v5e cores per region` is 4. TPU quota is only available in `us-west1`. You can request for higher TPU quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf297c5-711b-4b21-a095-8f651761ce7e",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b09c37-fc5e-4238-ab71-ef2caed87d88",
   "metadata": {},
   "source": [
    "[Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9791e9f3-51f7-4dc1-9da4-db3350bc06ac",
   "metadata": {},
   "source": [
    "**[Optional]** Set project. If not set, the project will be set automatically according to the environment variable \"GOOGLE_CLOUD_PROJECT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a20ac4-8142-4dda-885b-438a7d79a8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e68e2-1204-4989-9eec-cdde53cf4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c54ad-0a56-42d3-a791-91b59a83760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PROJECT_ID:\n",
    "    # Get the default cloud project id.\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"\")\n",
    "    assert PROJECT_ID, \"Provide a google cloud project id.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f586cd87-4bd0-49b7-a0cf-bc988d7c3fd4",
   "metadata": {},
   "source": [
    "**[Optional]** Set region. If not set, the region will be set automatically according to the environment variable \"GOOGLE_CLOUD_REGION\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43c6334-0758-494a-a2d1-8c3e977c4a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54024dd5-c7ea-4ce1-bd93-4d11dcb2c9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REGION:\n",
    "    # Get the default region for launching jobs.\n",
    "    REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"\")\n",
    "    assert REGION, \"Provide a google cloud region.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe1bc05-e0e8-4124-9b6a-f3adc0cc61b9",
   "metadata": {},
   "source": [
    "Upgrade Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1b72b8-ee42-4675-a7eb-ba91ec83cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf839f25-4079-4983-b9df-bad0a63edecf",
   "metadata": {},
   "source": [
    "## Access ShieldGemma Model\n",
    "You must provide a Hugging Face User Access Token (read) to access the Shield Gemma model. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeff92a-bec8-40a5-92e3-e9cbe6ca5ac8",
   "metadata": {},
   "source": [
    "**[Optional]** Set a Hugging Face read token. If not set, the token will be set automatically according to the environment variable \"HF_TOKEN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3341aae-e6fc-489f-921b-fc6da651979a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff7489-7d94-4319-852b-b12912c87d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not HF_TOKEN:\n",
    "    # Get the HF token from the environment.\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "    assert HF_TOKEN, \"Provide a read HF_TOKEN to load models from Hugging Face.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360019d3-cb13-439f-a150-8fec756d485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_prefix = \"google/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebcfa87-6fe9-48dd-8129-d898dc83c318",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c595139-572c-4586-9956-87e936be5275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "running_in_colab = \"google.colab\" in sys.modules\n",
    "\n",
    "if running_in_colab and os.environ.get(\"VERTEX_PRODUCT\", \"\") != \"COLAB_ENTERPRISE\":\n",
    "    from google.colab import auth as colab_auth\n",
    "    \n",
    "    colab_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e5d32c-8818-4794-aab0-dbb7da81d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04586350-3649-45e5-9e88-c55411bb7c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9783e277-75cb-461d-8e11-38f877b9fa42",
   "metadata": {},
   "source": [
    "Initialize Vertex AI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d358b01-84d7-4843-9734-d1869529dc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1700b93e-134d-4c57-b9f9-d252edf2021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, endpoints = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d3c56d-34b0-4f97-a719-77032fe02b9f",
   "metadata": {},
   "source": [
    "Deploy ShieldGemma model with Hex-LLM on TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67f8ac2-d91c-4023-af05-79399f7607a1",
   "metadata": {},
   "source": [
    "**Hex-LLM** is a **H**igh-**E**fficiency **L**arge **L**anguage **M**odel (LLM) TPU serving solution built with **XLA**, which is being developed by Google Cloud.\n",
    "\n",
    "Refer to the \"Request for TPU quota\" section for TPU quota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec2d384-a796-4068-88b6-43785bce24f1",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de42645-7f75-4668-ace2-78c4a0fd0de1",
   "metadata": {},
   "source": [
    "The pre-built serving docker images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae9d5fa-079e-4bfc-9b91-dc95c5108562",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEXLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/hex-llm-serve:gemma2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ab0124-e518-4c03-b8f6-8277f064b3dc",
   "metadata": {},
   "source": [
    "Set the model ID. Model weights can be loaded from HuggingFace or from a GCS bucket. Select one of the four model variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77838f25-793d-48f9-9581-013517560d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"shieldgemma-9b\"  # @param [\"shieldgemma-2b\", \"shieldgemma-9b\", \"shieldgemma-27b\"] {allow-input: true, isTemplate: true}\n",
    "TPU_DEPLOYMENT_REGION = \"us-west1\"  # @param [\"us-west1\"] {isTemplate:true}\n",
    "model_id = os.path.join(model_path_prefix, MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076a5cd2-208b-4576-8846-29451296a4af",
   "metadata": {},
   "source": [
    "Choose a machine type. You can find Vertex AI prediction TPUv5e machine types in https://cloud.google.com/vertex-ai/docs/predictions/use-tpu#deploy_a_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01dba65-ed82-4ce1-8d7f-9d2507312ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"2b\" in model_id:\n",
    "    # Sets ct5lp-hightpu-1t (1 TPU chip) to deploy Gemma 2 2B models.\n",
    "    machine_type = \"ct5lp-hightpu-1t\"\n",
    "    accelerator_type = \"TPU_V5e\"\n",
    "    # Note: 1 TPU V5 chip has only one core.\n",
    "    accelerator_count = 1\n",
    "elif \"9b\" in model_id:\n",
    "    # Sets ct5lp-hightpu-4t (4 TPU chips) to deploy Gemma 2 9B models.\n",
    "    machine_type = \"ct5lp-hightpu-4t\"\n",
    "    accelerator_type = \"TPU_V5e\"\n",
    "    # Note: 1 TPU V5 chip has only one core.\n",
    "    accelerator_count = 4\n",
    "else:\n",
    "    # Sets ct5lp-hightpu-8t (8 TPU chips) to deploy Gemma 2 27B models.\n",
    "    machine_type = \"ct5lp-hightpu-8t\"\n",
    "    accelerator_type = \"TPU_V5e\"\n",
    "    # Note: 1 TPU V5 chip has only one core.\n",
    "    accelerator_count = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07575909-46b1-48b0-a716-921c753ea060",
   "metadata": {},
   "source": [
    "(Optional) Check quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b42cd-b4a3-4977-8528-df7e05f11645",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "def get_quota(project_id: str, region: str, resource_id: str) -> int:\n",
    "  \"\"\"Returns the quota for a resource in a region.\n",
    "\n",
    "  Args:\n",
    "    project_id: The project id.\n",
    "    region: The region.\n",
    "    resource_id: The resource id.\n",
    "\n",
    "  Returns:\n",
    "    The quota for the resource in the region. Returns -1 if can not figure out\n",
    "    the quota.\n",
    "\n",
    "  Raises:\n",
    "    RuntimeError: If the command to get quota fails.\n",
    "  \"\"\"\n",
    "  service_endpoint = \"aiplatform.googleapis.com\"\n",
    "\n",
    "  command = (\n",
    "      \"gcloud alpha services quota list\"\n",
    "      f\" --service={service_endpoint} --consumer=projects/{project_id}\"\n",
    "      f\" --filter='{service_endpoint}/{resource_id}' --format=json\"\n",
    "  )\n",
    "  process = subprocess.run(\n",
    "      command, shell=True, capture_output=True, text=True, check=True\n",
    "  )\n",
    "  if process.returncode == 0:\n",
    "    quota_data = json.loads(process.stdout)\n",
    "  else:\n",
    "    raise RuntimeError(f\"Error fetching quota data: {process.stderr}\")\n",
    "\n",
    "  if not quota_data or \"consumerQuotaLimits\" not in quota_data[0]:\n",
    "    return -1\n",
    "  if (\n",
    "      not quota_data[0][\"consumerQuotaLimits\"]\n",
    "      or \"quotaBuckets\" not in quota_data[0][\"consumerQuotaLimits\"][0]\n",
    "  ):\n",
    "    return -1\n",
    "  all_regions_data = quota_data[0][\"consumerQuotaLimits\"][0][\"quotaBuckets\"]\n",
    "  for region_data in all_regions_data:\n",
    "    if (\n",
    "        region_data.get(\"dimensions\")\n",
    "        and region_data[\"dimensions\"][\"region\"] == region\n",
    "    ):\n",
    "      if \"effectiveLimit\" in region_data:\n",
    "        return int(region_data[\"effectiveLimit\"])\n",
    "      else:\n",
    "        return 0\n",
    "  return -1\n",
    "\n",
    "def get_resource_id(\n",
    "    accelerator_type: str,\n",
    "    is_for_training: bool,\n",
    "    is_restricted_image: bool = False,\n",
    "    is_dynamic_workload_scheduler: bool = False,\n",
    ") -> str:\n",
    "  \"\"\"Returns the resource id for a given accelerator type and the use case.\n",
    "\n",
    "  Args:\n",
    "    accelerator_type: The accelerator type.\n",
    "    is_for_training: Whether the resource is used for training. Set false for\n",
    "      serving use case.\n",
    "    is_restricted_image: Whether the image is hosted in `vertex-ai-restricted`.\n",
    "    is_dynamic_workload_scheduler: Whether the resource is used with Dynamic\n",
    "      Workload Scheduler.\n",
    "\n",
    "  Returns:\n",
    "    The resource id.\n",
    "  \"\"\"\n",
    "  accelerator_suffix_map = {\n",
    "      \"NVIDIA_TESLA_V100\": \"nvidia_v100_gpus\",\n",
    "      \"NVIDIA_L4\": \"nvidia_l4_gpus\",\n",
    "      \"NVIDIA_TESLA_A100\": \"nvidia_a100_gpus\",\n",
    "      \"NVIDIA_A100_80GB\": \"nvidia_a100_80gb_gpus\",\n",
    "      \"NVIDIA_H100_80GB\": \"nvidia_h100_gpus\",\n",
    "      \"NVIDIA_TESLA_T4\": \"nvidia_t4_gpus\",\n",
    "      \"TPU_V5e\": \"tpu_v5e\",\n",
    "      \"TPU_V3\": \"tpu_v3\",\n",
    "  }\n",
    "  default_training_accelerator_map = {\n",
    "      key: f\"custom_model_training_{accelerator_suffix_map[key]}\"\n",
    "      for key in accelerator_suffix_map\n",
    "  }\n",
    "  dws_training_accelerator_map = {\n",
    "      key: f\"custom_model_training_preemptible_{accelerator_suffix_map[key]}\"\n",
    "      for key in accelerator_suffix_map\n",
    "  }\n",
    "  restricted_image_training_accelerator_map = {\n",
    "      \"NVIDIA_A100_80GB\": \"restricted_image_training_nvidia_a100_80gb_gpus\",\n",
    "  }\n",
    "  serving_accelerator_map = {\n",
    "      key: f\"custom_model_serving_{accelerator_suffix_map[key]}\"\n",
    "      for key in accelerator_suffix_map\n",
    "  }\n",
    "\n",
    "  if is_for_training:\n",
    "    if is_restricted_image and is_dynamic_workload_scheduler:\n",
    "      raise ValueError(\n",
    "          \"Dynamic Workload Scheduler does not work for restricted image\"\n",
    "          \" training.\"\n",
    "      )\n",
    "    training_accelerator_map = (\n",
    "        restricted_image_training_accelerator_map\n",
    "        if is_restricted_image\n",
    "        else default_training_accelerator_map\n",
    "    )\n",
    "    if accelerator_type in training_accelerator_map:\n",
    "      if is_dynamic_workload_scheduler:\n",
    "        return dws_training_accelerator_map[accelerator_type]\n",
    "      else:\n",
    "        return training_accelerator_map[accelerator_type]\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          f\"Could not find accelerator type: {accelerator_type} for training.\"\n",
    "      )\n",
    "  else:\n",
    "    if is_dynamic_workload_scheduler:\n",
    "      raise ValueError(\"Dynamic Workload Scheduler does not work for serving.\")\n",
    "    if accelerator_type in serving_accelerator_map:\n",
    "      return serving_accelerator_map[accelerator_type]\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          f\"Could not find accelerator type: {accelerator_type} for serving.\"\n",
    "      )\n",
    "\n",
    "def check_quota(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    accelerator_type: str,\n",
    "    accelerator_count: int,\n",
    "    is_for_training: bool,\n",
    "    is_restricted_image: bool = False,\n",
    "    is_dynamic_workload_scheduler: bool = False,\n",
    "):\n",
    "  \"\"\"Checks if the project and the region has the required quota.\"\"\"\n",
    "  resource_id = get_resource_id(\n",
    "      accelerator_type,\n",
    "      is_for_training=is_for_training,\n",
    "      is_restricted_image=is_restricted_image,\n",
    "      is_dynamic_workload_scheduler=is_dynamic_workload_scheduler,\n",
    "  )\n",
    "  quota = get_quota(project_id, region, resource_id)\n",
    "  quota_request_instruction = (\n",
    "      \"Either use \"\n",
    "      \"a different region or request additional quota. Follow \"\n",
    "      \"instructions here \"\n",
    "      \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"\n",
    "      \" to check quota in a region or request additional quota for \"\n",
    "      \"your project.\"\n",
    "  )\n",
    "  if quota == -1:\n",
    "    raise ValueError(\n",
    "        f\"Quota not found for: {resource_id} in {region}.\"\n",
    "        f\" {quota_request_instruction}\"\n",
    "    )\n",
    "  if quota < accelerator_count:\n",
    "    raise ValueError(\n",
    "        f\"Quota not enough for {resource_id} in {region}: {quota} <\"\n",
    "        f\" {accelerator_count}. {quota_request_instruction}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015d4121-ee7a-43ef-b573-044ce8f9324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=TPU_DEPLOYMENT_REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94901f-49be-485c-b1ce-9b6c67a1a3e5",
   "metadata": {},
   "source": [
    "Server parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c7c8e2-aa99-4f85-a5db-7faa855235f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_parallel_size = accelerator_count\n",
    "hbm_utilization_factor = 0.6  # Fraction of HBM memory allocated for KV cache after model loading. A larger value improves throughput but gives higher risk of TPU out-of-memory errors with long prompts.\n",
    "max_running_seqs = 256  # Maximum number of running sequences in a continuous batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff43b03c-1ffc-4abd-8cbf-c79b1542452c",
   "metadata": {},
   "source": [
    "Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019cbc05-9193-44be-97c6-e4ce6355f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dedicated_endpoint = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27a4da9-fc16-4bff-8f9a-3708c3505db6",
   "metadata": {},
   "source": [
    "Endpoint configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad77aed9-768e-4379-ab53-2dd84775e587",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_replica_count = 1\n",
    "max_replica_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74147bac-80f9-4d36-b0d2-cf3d9aa7b7d0",
   "metadata": {},
   "source": [
    "Deployment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe95093-b1c2-4947-b132-182236a08388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_hexllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    base_model_id: str = None,\n",
    "    tensor_parallel_size: int = 1,\n",
    "    machine_type: str = \"ct5lp-hightpu-1t\",\n",
    "    tpu_topology: str = \"1x1\",\n",
    "    hbm_utilization_factor: float = 0.6,\n",
    "    max_running_seqs: int = 256,\n",
    "    max_model_len: int = 4096,\n",
    "    endpoint_id: str = \"\",\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with Hex-LLM on TPU in Vertex AI.\"\"\"\n",
    "    if endpoint_id:\n",
    "        aip_endpoint_name = (\n",
    "            f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
    "        )\n",
    "        endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=f\"{model_name}-endpoint\",\n",
    "            location=TPU_DEPLOYMENT_REGION,\n",
    "            dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "        )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    if not tensor_parallel_size:\n",
    "        tensor_parallel_size = int(machine_type[-2])\n",
    "\n",
    "    num_hosts = int(tpu_topology.split(\"x\")[0])\n",
    "\n",
    "    hexllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor_parallel_size={tensor_parallel_size}\",\n",
    "        f\"--num_hosts={num_hosts}\",\n",
    "        f\"--hbm_utilization_factor={hbm_utilization_factor}\",\n",
    "        f\"--max_running_seqs={max_running_seqs}\",\n",
    "        f\"--max_model_len={max_model_len}\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"HEX_LLM_LOG_LEVEL\": \"info\",\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars.update({\"HF_TOKEN\": HF_TOKEN})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"hex_llm.server.api_server\"],\n",
    "        serving_container_args=hexllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        location=TPU_DEPLOYMENT_REGION,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        tpu_topology=tpu_topology if num_hosts > 1 else None,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ea50d-a939-4ff5-a09d-de26c38a3e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"{MODEL_ID}-{now}\".replace(\"_\", \"-\")\n",
    "\n",
    "model, endpoint = deploy_model_hexllm(\n",
    "    model_name=model_name,\n",
    "    model_id=model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    hbm_utilization_factor=hbm_utilization_factor,\n",
    "    max_running_seqs=max_running_seqs,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35978b55-230d-4563-a8de-e4543b6faab7",
   "metadata": {},
   "source": [
    "**[Optional]** Loads an existing endpoint instance using the endpoint name:\n",
    "- Using `endpoint_name = endpoint.name` allows us to get the endpoint name of the endpoint `endpoint` created in the cell above.\n",
    "- Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load an existing endpoint with the ID 1234567890123456789."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d75a0-8182-4ec9-ae4a-f562a99671b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment the code below to load an existing endpoint:\n",
    "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoint = aiplatform.Endpoint(aip_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a3bf9-81ab-4d01-95db-1bde256a1b41",
   "metadata": {},
   "source": [
    "### Prompt Construction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5ebfbc-d8b1-428f-b585-5a311f909107",
   "metadata": {},
   "source": [
    "Create the UseCase and HarmType enumerations and the make_prompt(4) function to allow you to easily create prompts using the use case-specific templates that ShieldGemma requires.\n",
    "\n",
    "Example usage is shown in the \"Inference Examples\" section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd41e7b-a3da-48aa-bc72-acb64b7fde49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "class HarmType(enum.Enum):\n",
    "  # ShieldGemma is trained to classify content in relation to the following harm\n",
    "  # types.\n",
    "  DANGEROUS = 'Dangerous Content'\n",
    "  HATE = 'Hate Speech'\n",
    "  SEXUAL = 'Sexually Explicit Information'\n",
    "  HARASSMENT = 'Harassment'\n",
    "\n",
    "\n",
    "class UseCase(enum.Enum):\n",
    "  \"\"\"user_prompt is for classifying the user prompt.\n",
    "\n",
    "  chatbot_response is for classifying the chatbot response given the user prompt\n",
    "  as context.\n",
    "  \"\"\"\n",
    "\n",
    "  PROMPT_ONLY = 'prompt_only'\n",
    "  PROMPT_RESPONSE = 'prompt_response'\n",
    "\n",
    "\n",
    "HARM_POLICIES: dict[HarmType, str] = {\n",
    "    HarmType.DANGEROUS: (\n",
    "        'harming oneself and/or others (e.g., accessing or building firearms'\n",
    "        ' and explosive devices, promotion of terrorism, instructions for'\n",
    "        ' suicide).'\n",
    "    ),\n",
    "    HarmType.HATE: (\n",
    "        'targeting identity and/or protected attributes (e.g., racial slurs,'\n",
    "        ' promotion of discrimination, calls to violence against protected'\n",
    "        ' groups) or dehumanizing/belittling/vilifying on the bases of race,'\n",
    "        ' ethnicity, religion, disability, age, nationality, veteran status,'\n",
    "        ' sexual orientation, gender, gender identity, caste, or any other'\n",
    "        ' protected status. We do allow content that includes a reference to a'\n",
    "        ' protected individual or group in a positive manner.'\n",
    "    ),\n",
    "    HarmType.SEXUAL: (\n",
    "        'contains references to sexual acts or other lewd content (e.g.,'\n",
    "        ' sexually graphic descriptions, content aimed at causing arousal).'\n",
    "        ' Medical or scientific terms related to human anatomy or sex education'\n",
    "        ' are permitted.'\n",
    "    ),\n",
    "    HarmType.HARASSMENT: (\n",
    "        'is malicious, intimidating, bullying, or abusive content targeting'\n",
    "        ' another individual (e.g., physical threats, denial of tragic events,'\n",
    "        ' disparaging victims of violence).'\n",
    "    ),\n",
    "}\n",
    "\n",
    "def harm_definition(harm_type: HarmType, use_case: UseCase) -> str:\n",
    "  \"\"\"Fomulates the harm definition relative to the harm type and use case.\n",
    "\n",
    "  Args:\n",
    "    harm_type: the name of the harm type.\n",
    "    use_case: the name of the use case.\n",
    "\n",
    "  Returns:\n",
    "    A string represetnation of the harm definition.\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  if use_case == UseCase.PROMPT_ONLY:\n",
    "    prefix = 'The prompt shall not contain or seek generation of content that'\n",
    "  else:\n",
    "    prefix = 'The chatbot shall not generate content that'\n",
    "\n",
    "  description = HARM_POLICIES[harm_type]\n",
    "  return f'\"No {harm_type.value}\": {prefix} {description}'\n",
    "\n",
    "\n",
    "PROMPT_ONLY_TEMPLATE = \"\"\"You are a policy expert trying to help determine\n",
    "whether a user prompt is in violation of the defined safety policies.\n",
    "\n",
    "<start_of_turn>\n",
    "Human Question: {user_content}\n",
    "<end_of_turn>\n",
    "\n",
    "Our safety principle is defined in the below:\n",
    "\n",
    "* {harm_text}\n",
    "\n",
    "===\n",
    "\n",
    "Does the human question violate the above principle? Your answer must start with\n",
    "'Yes' or 'No'. And then walk through step by step to be sure we answer\n",
    "correctly.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "PROMPT_RESPONSE_TEMPLATE = \"\"\"You are a policy expert trying to help determine\n",
    "whether an AI response to prompt is in violation of the defined safety policies.\n",
    "\n",
    "<start_of_turn>\n",
    "Human Question: {user_content}\n",
    "<end_of_turn>\n",
    "\n",
    "<start_of_turn>\n",
    "Chatbot Response: {model_content}\n",
    "<end_of_turn>\n",
    "\n",
    "Our safety principle is defined in the below:\n",
    "\n",
    "* {harm_text}\n",
    "\n",
    "===\n",
    "\n",
    "Does the Chatbot Response violate the above principle? Your answer must start\n",
    "with 'Yes' or 'No'. And then walk through step by step to be sure  we answer\n",
    "correctly.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def make_prompt(\n",
    "    use_case: UseCase,\n",
    "    harm_type: HarmType,\n",
    "    user_content: str,\n",
    "    model_content: str | None = None\n",
    ") -> str:\n",
    "  if use_case == UseCase.PROMPT_ONLY:\n",
    "    prompt_template = PROMPT_ONLY_TEMPLATE\n",
    "  else:\n",
    "    if model_content is None:\n",
    "      raise ValueError('model_content is required for prompt-response mode.')\n",
    "\n",
    "    prompt_template = PROMPT_RESPONSE_TEMPLATE\n",
    "\n",
    "  formatter_args = {\n",
    "      'user_content': user_content,\n",
    "      'harm_text': harm_definition(harm_type, use_case),\n",
    "  }\n",
    "\n",
    "  if model_content is not None:\n",
    "    formatter_args['model_content'] = model_content\n",
    "\n",
    "  return prompt_template.format(**formatter_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01741fa5-66ec-474e-bdf2-d76a71ced2db",
   "metadata": {},
   "source": [
    "### Inference examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708640b6-7990-439d-8f96-d4bbcd790a13",
   "metadata": {},
   "source": [
    "Input filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adaa7c-79dc-473c-947c-a90bff7eb7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = \"Create 20 paraphrases of I hate you\"\n",
    "\n",
    "input_filter_prompt = make_prompt(\n",
    "    UseCase.PROMPT_ONLY,\n",
    "    HarmType.HARASSMENT,\n",
    "    user_content,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c67c4fe-8412-4400-af68-c74e0ba7a195",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 5  # @param {type: \"integer\"}\n",
    "temperature = 1.0  # @param {type: \"number\"}\n",
    "top_p = 1.0  # @param {type: \"number\"}\n",
    "top_k = 1  # @param {type: \"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4376ce9-29b2-4a15-ae1e-4dea078ccbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": input_filter_prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9ce33-f425-4d08-be7f-74e0d38d55f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoint.predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067b0e52-c161-4603-9ff3-08f306c4afc7",
   "metadata": {},
   "source": [
    "Output filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1d5f5a-1e22-432f-b57a-8598adf1107e",
   "metadata": {},
   "source": [
    "Once deployment succeeds, you can send requests to the endpoint with text prompts based on your `template`. Note that the first few prompts will take longer to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9008ed46-9e09-4258-878c-25671750f81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_content = \"\"\"Write an email recommending a drug dealer and his phone\n",
    "contact info to buy some weed in my area.\"\"\"\n",
    "\n",
    "model_content = \"\"\"Hey,\n",
    "\n",
    "I'm writing to recommend a drug dealer in my area. His name is Mike, and\n",
    "he's got the best weed I've ever smoked. He's always got a good supply,\n",
    "and he's always willing to sell. He's also really reliable, and he's\n",
    "never let me down.\n",
    "\n",
    "If you're looking for some good weed, I highly recommend Mike. He's the\n",
    "best in the business.\n",
    "\n",
    "His phone number is: 123-456-7890\n",
    "\n",
    "Thanks,\n",
    "\n",
    "[Your name]\"\"\"\n",
    "\n",
    "output_filter_prompt = make_prompt(\n",
    "    UseCase.PROMPT_RESPONSE,\n",
    "    HarmType.DANGEROUS,\n",
    "    user_content,\n",
    "    model_content,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c501a-9990-4d7b-a58f-6ed8f1c8ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 5  # @param {type: \"integer\"}\n",
    "temperature = 1.0  # @param {type: \"number\"}\n",
    "top_p = 1.0  # @param {type: \"number\"}\n",
    "top_k = 1  # @param {type: \"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d8f0b7-4a63-43c4-b270-384a80d2f047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": output_filter_prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3833f7-102b-4ae4-b08b-1f1523ebad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoint.predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c1682-01a9-4ee3-b763-1a9970f68983",
   "metadata": {},
   "source": [
    "### Clean up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82aea1b9-c851-458b-9b68-d593fe1e1346",
   "metadata": {},
   "source": [
    "Delete the experiment models and endpoints to recycle the resources and avoid unnecessary continuous charges that may incur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92233fe4-91a9-4017-a590-0c825157a0ba",
   "metadata": {},
   "source": [
    "Change the variable to True to delete the resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61e914a-799d-40e0-811e-280fd6b4b6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_resources = False # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d69fac3-01a0-4319-8e58-8c4f2e2742b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if delete_resources:\n",
    "    # Undeploy model and delete endpoint.\n",
    "    endpoint.delete(force=True)\n",
    "    \n",
    "    # Delete models.\n",
    "    model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcd8537-2b25-438c-9bd5-bdb1843b7618",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
