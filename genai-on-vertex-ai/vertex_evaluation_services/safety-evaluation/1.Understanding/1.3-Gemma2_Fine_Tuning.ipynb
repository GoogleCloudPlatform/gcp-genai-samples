{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e15ee8-63a2-4b38-b0d8-abc3456f236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd5394-cd60-4ca8-9c2a-66001d781b08",
   "metadata": {},
   "source": [
    "# Gemma2 Fine Tuning with LoRA and deployment on Vertex AI  with vLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7638b6f2-f0be-47b2-b1f1-bddd46e163e8",
   "metadata": {},
   "source": [
    "Based on [model_garden_gemma2_finetuning_on_vertex.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma2_finetuning_on_vertex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d1fc14-f24e-4f34-a93a-1a9a158f1a94",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-colab&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-colab&destination=evaluation_rag_use_cases-from_notebook-colab&url=https%3A%2F%2Fcolab.sandbox.google.com%2Fgithub%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_evaluation_services%2Fevaluation-rag-systems%2Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-colab_ent&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-colab_ent&destination=evaluation_rag_use_cases-from_notebook-colab_ent&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fcolab%2Fimport%2Fhttps%3A%252F%252Fraw.githubusercontent.com%252FGoogleCloudPlatform%252Fapplied-ai-engineering-samples%252Fmain%252Fgenai-on-vertex-ai%252Fvertex_evaluation_services%252Fevaluation-rag-systems%252Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-github&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-github&destination=evaluation_rag_use_cases-from_notebook-github&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_evaluation_services%2Fevaluation-rag-systems%2Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-vai_workbench&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-vai_workbench&destination=evaluation_rag_use_cases-from_notebook-vai_workbench&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fworkbench%2Fdeploy-notebook%3Fdownload_url%3Dhttps%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_evaluation_services%2Fevaluation-rag-systems%2Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba8b57d2-37d0-4c06-af8b-2c759b4b4065",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <td>Author(s)</td>\n",
    "    <td>Egon Soares</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8cbcc-54e2-4328-8d93-738cebb0db1a",
   "metadata": {},
   "source": [
    "![gemma2 fine tuning architecture](images/1.3-gemma2-fine-tuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4800c5-1378-427c-84ab-041d29ad9168",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates finetuning and deploying Gemma 2 models with [Vertex AI Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job). All of the examples in this notebook use parameter efficient finetuning methods [PEFT (LoRA)](https://github.com/huggingface/peft) to reduce training and storage costs. LoRA (Low-Rank Adaptation) is one approach of Parameter Efficient FineTuning (PEFT), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during finetuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685).\n",
    "\n",
    "\n",
    "After tuning, we can deploy models on Vertex with GPU.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Finetune and deploy Gemma 2 models with Vertex AI Custom Training Jobs.\n",
    "- Send prediction requests to your finetuned Gemma 2 model.\n",
    "\n",
    "### File a bug\n",
    "\n",
    "File a bug on [GitHub](https://github.com/GoogleCloudPlatform/vertex-ai-samples/issues/new) if you encounter any issue with the notebook.\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a263ee-a556-4647-84ce-84a01dd7b728",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a93efd-2a80-4392-83a3-a67f8b5f34e6",
   "metadata": {},
   "source": [
    "### Install Python Packages for Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81841ff-6747-452d-b491-e29be1bebbeb",
   "metadata": {},
   "source": [
    "Install google-cloud-aiplatform and dataset validation packages and restart the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b27ba-9dd9-4e88-918b-289e1fd70609",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade --quiet 'google-cloud-aiplatform>=1.66.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16e1fe-e82e-41bd-8390-c66e4713648b",
   "metadata": {},
   "source": [
    "Load local tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1551105a-ef53-4fc5-9922-7b05cadc54ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b87863e-8a46-4587-8bb6-764d2d90fde5",
   "metadata": {},
   "source": [
    "### Setup Google Cloud Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae191019-29f0-49f5-8aee-a7e17e77be63",
   "metadata": {},
   "source": [
    "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "2. For finetuning, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Frestricted_image_training_nvidia_a100_80gb_gpus)** to check if your project already has the required 8 Nvidia A100 80 GB GPUs in the us-central1 region. If yes, then run this notebook in the us-central1 region. If you do not have 8 Nvidia A100 80 GPUs or have more GPU requirements than this, then schedule your job with Nvidia H100 GPUs via Dynamic Workload Scheduler using [these instructions](https://cloud.google.com/vertex-ai/docs/training/schedule-jobs-dws). For Dynamic Workload Scheduler, check the [us-central1](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) or [europe-west4](https://console.cloud.google.com/iam-admin/quotas?location=europe-west4&metric=aiplatform.googleapis.com%2Fcustom_model_training_preemptible_nvidia_h100_gpus) quota for Nvidia H100 GPUs. If you do not have enough GPUs, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request quota.\n",
    "\n",
    "3. For serving, **[click here](https://console.cloud.google.com/iam-admin/quotas?location=us-central1&metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_l4_gpus)** to check if your project already has the required 1 L4 GPU in the us-central1 region.  If yes, then run this notebook in the us-central1 region. If you need more L4 GPUs for your project, then you can follow [these instructions](https://cloud.google.com/docs/quotas/view-manage#viewing_your_quota_console) to request more. Alternatively, if you want to run predictions with A100 80GB or H100 GPUs, we recommend using the regions listed below. **NOTE:** Make sure you have associated quota in selected regions. Click the links to see your current quota for each GPU type: [Nvidia A100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_a100_80gb_gpus), [Nvidia H100 80GB](https://console.cloud.google.com/iam-admin/quotas?metric=aiplatform.googleapis.com%2Fcustom_model_serving_nvidia_h100_gpus).\n",
    "\n",
    "> | Machine Type | Accelerator Type | Recommended Regions |\n",
    "| ----------- | ----------- | ----------- | \n",
    "| a2-ultragpu-1g | 1 NVIDIA_A100_80GB | us-central1, us-east4, europe-west4, asia-southeast1, us-east4 |\n",
    "| a3-highgpu-2g | 2 NVIDIA_H100_80GB | us-west1, asia-southeast1 |\n",
    "| a3-highgpu-4g | 4 NVIDIA_H100_80GB | us-west1, asia-southeast1 |\n",
    "| a3-highgpu-8g | 8 NVIDIA_H100_80GB | us-central1, us-west1, europe-west4, asia-southeast1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b582623-4691-41b1-ae52-a51269c79399",
   "metadata": {},
   "source": [
    "4. **[Optional]** Set project. If not set, the project will be set automatically according to the environment variable \"GOOGLE_CLOUD_PROJECT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db6289f-ad67-42c8-9c69-c48b7608fe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f03ee-4abf-4b14-a0b1-6243fea98052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3382ab8-7cf3-43f8-aff6-77f292ba56cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PROJECT_ID:\n",
    "    # Get the default cloud project id.\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"\")\n",
    "    assert PROJECT_ID, \"Provide a google cloud project id.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2256a356-2db4-48a7-8777-b676ea7aae7c",
   "metadata": {},
   "source": [
    "5. **[Optional]** Set region. If not set, the region will be set automatically according to the environment variable \"GOOGLE_CLOUD_REGION\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc892a9-6d2f-45d2-b5e1-bf98bf8c83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf70fe-da8d-417f-9fb2-fb005ed4cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REGION:\n",
    "    # Get the default region for launching jobs.\n",
    "    REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"\")\n",
    "    assert REGION, \"Provide a google cloud region.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8296a-cce9-4659-89ec-b3895ec38ee8",
   "metadata": {},
   "source": [
    "7. **[Optional]** Set a Hugging Face read token. If not set, the token will be set automatically according to the environment variable \"HF_TOKEN\". You must provide a Hugging Face User Access Token (read) to access the Gemma 2 models. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154c5b6c-28ac-467c-8342-c98ac490bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9868a765-f3ac-4d6c-ac9b-c6f5d557ecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not HF_TOKEN:\n",
    "    # Get the HF token from the environment.\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "    assert HF_TOKEN, \"Provide a read HF_TOKEN to load models from Hugging Face.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c864d514-2903-4651-90aa-1e519c771d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_prefix = \"google/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a65280-0060-4588-902b-324356c1dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "running_in_colab = \"google.colab\" in sys.modules\n",
    "\n",
    "if running_in_colab and os.environ.get(\"VERTEX_PRODUCT\", \"\") != \"COLAB_ENTERPRISE\":\n",
    "    from google.colab import auth as colab_auth\n",
    "    \n",
    "    colab_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8780acd1-b565-4323-8331-e4cee54b2025",
   "metadata": {},
   "source": [
    "8. **[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3d84c9-0c62-4d27-949f-0194663b9ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ea6f4a-a4b2-4a67-bcb6-adcdf60b9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42e4ce3-b045-4734-b8d0-b61d677aab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"gemma2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5bed79-1a5e-4c01-ba96-8d6e6045b309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07e439e-5bf0-4977-8d94-6bdda43ea5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19eae63-d102-4636-9e5a-5647cf91c987",
   "metadata": {},
   "source": [
    "Gets the default service account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ee0737-1c4e-4483-8423-38302520efef",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8301b-08df-40c5-b4d9-03a6d2160eeb",
   "metadata": {},
   "source": [
    "Provision permissions to the SERVICE_ACCOUNT with the GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07eb424a-d536-404b-be1c-f112fa422093",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --condition=None --role=\"roles/storage.admin\"\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --condition=None --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a48f88-e20d-40af-9445-fabdc57d65ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, endpoints = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def0993c-edbc-4b6a-bd91-5e87b1ec21a4",
   "metadata": {},
   "source": [
    "## Finetune with HuggingFace PEFT and Deploy with vLLM on GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39c89b4-af15-4b8e-9137-9b747d57180d",
   "metadata": {},
   "source": [
    "### Set dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50120e70-122d-406d-a2d0-a11bd81db6d4",
   "metadata": {},
   "source": [
    "Use the Vertex AI SDK to create and run the custom training jobs.\n",
    "\n",
    "This notebook uses [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset as an example.\n",
    "You can set `dataset_name` to any existing [Hugging Face dataset](https://huggingface.co/datasets) name, and set `instruct_column_in_dataset` to the name of the dataset column containing training data. The [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) has only one column `text`, and therefore we set `instruct_column_in_dataset` to `text` in this notebook.\n",
    "\n",
    "### (Optional) Prepare a custom JSONL dataset for finetuning\n",
    "\n",
    "You can prepare a JSONL file where each line is a valid JSON string as your custom training dataset. For example, here is one line from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset:\n",
    "```\n",
    "{\"text\": \"### Human: Hola### Assistant: \\u00a1Hola! \\u00bfEn qu\\u00e9 puedo ayudarte hoy?\"}\n",
    "```\n",
    "\n",
    "The JSON object has a key `text`, which should match `instruct_column_in_dataset`; The value should be one training data point, i.e. a string. After you prepared your JSONL file, you can either upload it to [Hugging Face datasets](https://huggingface.co/datasets) or [Google Cloud Storage](https://cloud.google.com/storage).\n",
    "\n",
    "- To upload a JSONL dataset to [Hugging Face datasets](https://huggingface.co/datasets), follow the instructions on [Uploading Datasets](https://huggingface.co/docs/hub/en/datasets-adding). Then, set `dataset_name` to the name of your newly created dataset on Hugging Face.\n",
    "\n",
    "- To upload a JSONL dataset to [Google Cloud Storage](https://cloud.google.com/storage), follow the instructions on [Upload objects from a filesystem](https://cloud.google.com/storage/docs/uploading-objects). Then, set `dataset_name` to the `gs://` URI to your JSONL file. For example: `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`.\n",
    "\n",
    "Optionally update the `instruct_column_in_dataset` field below if your JSON objects use a key other than the default `text`.\n",
    "\n",
    "### (Optional) Format your data with custom JSON template\n",
    "\n",
    "Sometimes, your dataset might have multiple text columns and you want to construct the training data with a template. You can prepare a JSON template in the following format:\n",
    "\n",
    "```\n",
    "{\n",
    "  \"description\": \"Template that accepts text-bison format.\",\n",
    "  \"source\": \"https://cloud.google.com/vertex-ai/generative-ai/docs/models/tune-text-models-supervised#dataset-format\",\n",
    "  \"prompt_input\": \"\\n\\n<|start_header_id|>user<|end_header_id|>\\n\\n{input_text}<|eot_id|>\\n\\n<|start_header_id|>assistant<|end_header_id|>\\n\\n{output_text}<|eot_id|>\",\n",
    "  \"instruction_separator\": \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n",
    "  \"response_separator\": \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "}\n",
    "```\n",
    "\n",
    "As an example, the template above can be used to format the following training data (this line comes from `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`):\n",
    "\n",
    "```\n",
    "{\"input_text\":\"TRANSCRIPT: \\nREASON FOR EVALUATION:,\\n\\n LABEL:\",\"output_text\":\"Chiropractic\"}\n",
    "```\n",
    "\n",
    "This example template simply concatenates `input_text` with `output_text` with some special tokens in between.\n",
    "To try such custom dataset, you can make the following changes:\n",
    "1. Set `template` to `llama3-text-bison`\n",
    "1. Set `train_dataset_name` to `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl`\n",
    "1. Set `train_split_name` to `train`\n",
    "1. Set `eval_dataset_name` to `gs://cloud-samples-data/vertex-ai/model-evaluation/peft_eval_sample.jsonl`\n",
    "1. Set `eval_split_name` to `train` (**NOT** `test`)\n",
    "1. Set `instruct_column_in_dataset` as `input_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256989e4-fcb3-4d1d-874b-f7a1b18f1776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template name or gs:// URI to a custom template.\n",
    "template = \"openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "\n",
    "# Hugging Face dataset name or gs:// URI to a custom JSONL dataset.\n",
    "train_dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "train_split_name = \"train\"  # @param {type:\"string\"}\n",
    "eval_dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "eval_split_name = \"test\"  # @param {type:\"string\"}\n",
    "\n",
    "# Name of the dataset column containing training text input.\n",
    "instruct_column_in_dataset = \"text\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb9d442-5b0a-46c8-905d-9dada0fc11ab",
   "metadata": {},
   "source": [
    "### Set model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23a22d-148c-4501-bfb7-5a6f2447c2a7",
   "metadata": {},
   "source": [
    "Select a model variant of Gemma 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4732e-4bf3-4d8b-bb50-fd33b1b439ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_id = \"gemma-2-2b-it\"  # @param [\"gemma-2-2b\", \"gemma-2-2b-it\", \"gemma-2-9b\", \"gemma-2-9b-it\", \"gemma-2-27b\", \"gemma-2-27b-it\"] {isTemplate: true}\n",
    "pretrained_model_id = os.path.join(model_path_prefix, base_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34966821-07ff-4a45-b3db-cc3a24cb4626",
   "metadata": {},
   "source": [
    "### Finetune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0782a152-0411-4b12-942e-44671e817857",
   "metadata": {},
   "source": [
    "This section demonstrates how to finetune the Gemma 2 model and merge the finetuned LoRA adapter with the base model on Vertex AI. It uses the Vertex AI SDK to create and run the custom training jobs.\n",
    "\n",
    "The training job takes approximately between 10 to 20 mins to set-up. Once done, the training job is expected to take around 20 mins with the default configuration. To find the training time, throughput, and memory usage of your training job, you can go to the training logs and check the log line of the last training epoch.\n",
    "\n",
    "**Note**:\n",
    "1. We recommend setting `finetuning_precision_mode` to `4bit` because it enables using fewer hardware resources for finetuning.\n",
    "1. If `max_steps > 0`, it takes precedence over `epochs`. One can set a small `max_steps` value to quickly check the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b96d227-2efb-4085-a537-c2b9d16cb118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform.compat.types import \\\n",
    "    custom_job as gca_custom_job_compat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b20b751-baea-445a-ad34-1188393dea7b",
   "metadata": {},
   "source": [
    "Accelerator type to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48678471-b3b9-4563-80c3-a0ae8b3921b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_type = \"NVIDIA_A100_80GB\"  # @param [\"NVIDIA_A100_80GB\", \"NVIDIA_H100_80GB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c11673c-ddec-428e-9451-95cf0ca43d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pre-built training docker image.\n",
    "if accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "    repo = \"us-docker.pkg.dev/vertex-ai-restricted\"\n",
    "    is_restricted_image = True\n",
    "    is_dynamic_workload_scheduler = False\n",
    "    dws_kwargs = {}\n",
    "else:\n",
    "    repo = \"us-docker.pkg.dev/vertex-ai\"\n",
    "    is_restricted_image = False\n",
    "    is_dynamic_workload_scheduler = True\n",
    "    dws_kwargs = {\n",
    "        \"max_wait_duration\": 1800,  # 30 minutes\n",
    "        \"scheduling_strategy\": gca_custom_job_compat.Scheduling.Strategy.FLEX_START,\n",
    "    }\n",
    "\n",
    "TRAIN_DOCKER_URI = (\n",
    "    f\"{repo}/vertex-vision-model-garden-dockers/pytorch-peft-train:stable_20240909\"\n",
    ")\n",
    "\n",
    "# Worker pool spec.\n",
    "if accelerator_type == \"NVIDIA_A100_80GB\":\n",
    "    per_node_accelerator_count = 8\n",
    "    machine_type = \"a2-ultragpu-8g\"\n",
    "elif accelerator_type == \"NVIDIA_H100_80GB\":\n",
    "    per_node_accelerator_count = 8\n",
    "    machine_type = \"a3-highgpu-8g\"\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Recommended machine settings not found for: {accelerator_type}. To use another accelerator type, edit this code block to pass in an appropriate `machine_type`, `accelerator_type`, and `per_node_accelerator_count` to the deploy_model_vllm function by clicking `Show Code` and then modifying the code.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6a3e5-6e5f-4a54-9ac8-0e5ed4b28af3",
   "metadata": {},
   "source": [
    "Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681ca2b5-3cc8-4574-940c-7df7fa0a16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for finetuning.\n",
    "per_device_train_batch_size = 1  # @param{type:\"integer\"}\n",
    "# Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n",
    "gradient_accumulation_steps = 4  # @param{type:\"integer\"}\n",
    "# Maximum sequence length.\n",
    "max_seq_length = 4096  # @param{type:\"integer\"}\n",
    "# Setting a positive `max_steps` here will override `num_epochs`.\n",
    "max_steps = -1  # @param{type:\"integer\"}\n",
    "num_epochs = 1.0  # @param{type:\"number\"}\n",
    "# Precision mode for finetuning.\n",
    "finetuning_precision_mode = \"4bit\"  # @param [\"4bit\", \"8bit\", \"float16\"]\n",
    "# Learning rate.\n",
    "learning_rate = 5e-5  # @param{type:\"number\"}\n",
    "# The scheduler type to use.\n",
    "lr_scheduler_type = \"cosine\"  # @param{type:\"string\"}\n",
    "# LoRA parameters.\n",
    "lora_rank = 16  # @param{type:\"integer\"}\n",
    "lora_alpha = 32  # @param{type:\"integer\"}\n",
    "lora_dropout = 0.05  # @param{type:\"number\"}\n",
    "# Activates gradient checkpointing for the current model (may be referred to as activation checkpointing or checkpoint activations in other frameworks).\n",
    "enable_gradient_checkpointing = True\n",
    "# Attention implementation to use in the model.\n",
    "attn_implementation = \"eager\"\n",
    "# The optimizer for which to schedule the learning rate.\n",
    "optimizer = \"paged_adamw_32bit\"\n",
    "# Define the proportion of training to be dedicated to a linear warmup where learning rate gradually increases.\n",
    "warmup_ratio = \"0.01\"\n",
    "# The list or string of integrations to report the results and logs to.\n",
    "report_to = \"tensorboard\"\n",
    "# Number of updates steps before two checkpoint saves.\n",
    "save_steps = 10\n",
    "# Number of update steps between two logs.\n",
    "logging_steps = save_steps\n",
    "# Train precision of the model.\n",
    "train_precision = \"bfloat16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e10db4-4468-41fe-836e-0f265d629a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "def get_quota(project_id: str, region: str, resource_id: str) -> int:\n",
    "  \"\"\"Returns the quota for a resource in a region.\n",
    "\n",
    "  Args:\n",
    "    project_id: The project id.\n",
    "    region: The region.\n",
    "    resource_id: The resource id.\n",
    "\n",
    "  Returns:\n",
    "    The quota for the resource in the region. Returns -1 if can not figure out\n",
    "    the quota.\n",
    "\n",
    "  Raises:\n",
    "    RuntimeError: If the command to get quota fails.\n",
    "  \"\"\"\n",
    "  service_endpoint = \"aiplatform.googleapis.com\"\n",
    "\n",
    "  command = (\n",
    "      \"gcloud alpha services quota list\"\n",
    "      f\" --service={service_endpoint} --consumer=projects/{project_id}\"\n",
    "      f\" --filter='{service_endpoint}/{resource_id}' --format=json\"\n",
    "  )\n",
    "  process = subprocess.run(\n",
    "      command, shell=True, capture_output=True, text=True, check=True\n",
    "  )\n",
    "  if process.returncode == 0:\n",
    "    quota_data = json.loads(process.stdout)\n",
    "  else:\n",
    "    raise RuntimeError(f\"Error fetching quota data: {process.stderr}\")\n",
    "\n",
    "  if not quota_data or \"consumerQuotaLimits\" not in quota_data[0]:\n",
    "    return -1\n",
    "  if (\n",
    "      not quota_data[0][\"consumerQuotaLimits\"]\n",
    "      or \"quotaBuckets\" not in quota_data[0][\"consumerQuotaLimits\"][0]\n",
    "  ):\n",
    "    return -1\n",
    "  all_regions_data = quota_data[0][\"consumerQuotaLimits\"][0][\"quotaBuckets\"]\n",
    "  for region_data in all_regions_data:\n",
    "    if (\n",
    "        region_data.get(\"dimensions\")\n",
    "        and region_data[\"dimensions\"][\"region\"] == region\n",
    "    ):\n",
    "      if \"effectiveLimit\" in region_data:\n",
    "        return int(region_data[\"effectiveLimit\"])\n",
    "      else:\n",
    "        return 0\n",
    "  return -1\n",
    "\n",
    "def get_resource_id(\n",
    "    accelerator_type: str,\n",
    "    is_for_training: bool,\n",
    "    is_restricted_image: bool = False,\n",
    "    is_dynamic_workload_scheduler: bool = False,\n",
    ") -> str:\n",
    "  \"\"\"Returns the resource id for a given accelerator type and the use case.\n",
    "\n",
    "  Args:\n",
    "    accelerator_type: The accelerator type.\n",
    "    is_for_training: Whether the resource is used for training. Set false for\n",
    "      serving use case.\n",
    "    is_restricted_image: Whether the image is hosted in `vertex-ai-restricted`.\n",
    "    is_dynamic_workload_scheduler: Whether the resource is used with Dynamic\n",
    "      Workload Scheduler.\n",
    "\n",
    "  Returns:\n",
    "    The resource id.\n",
    "  \"\"\"\n",
    "  accelerator_suffix_map = {\n",
    "      \"NVIDIA_TESLA_V100\": \"nvidia_v100_gpus\",\n",
    "      \"NVIDIA_L4\": \"nvidia_l4_gpus\",\n",
    "      \"NVIDIA_TESLA_A100\": \"nvidia_a100_gpus\",\n",
    "      \"NVIDIA_A100_80GB\": \"nvidia_a100_80gb_gpus\",\n",
    "      \"NVIDIA_H100_80GB\": \"nvidia_h100_gpus\",\n",
    "      \"NVIDIA_TESLA_T4\": \"nvidia_t4_gpus\",\n",
    "      \"TPU_V5e\": \"tpu_v5e\",\n",
    "      \"TPU_V3\": \"tpu_v3\",\n",
    "  }\n",
    "  default_training_accelerator_map = {\n",
    "      key: f\"custom_model_training_{accelerator_suffix_map[key]}\"\n",
    "      for key in accelerator_suffix_map\n",
    "  }\n",
    "  dws_training_accelerator_map = {\n",
    "      key: f\"custom_model_training_preemptible_{accelerator_suffix_map[key]}\"\n",
    "      for key in accelerator_suffix_map\n",
    "  }\n",
    "  restricted_image_training_accelerator_map = {\n",
    "      \"NVIDIA_A100_80GB\": \"restricted_image_training_nvidia_a100_80gb_gpus\",\n",
    "  }\n",
    "  serving_accelerator_map = {\n",
    "      key: f\"custom_model_serving_{accelerator_suffix_map[key]}\"\n",
    "      for key in accelerator_suffix_map\n",
    "  }\n",
    "\n",
    "  if is_for_training:\n",
    "    if is_restricted_image and is_dynamic_workload_scheduler:\n",
    "      raise ValueError(\n",
    "          \"Dynamic Workload Scheduler does not work for restricted image\"\n",
    "          \" training.\"\n",
    "      )\n",
    "    training_accelerator_map = (\n",
    "        restricted_image_training_accelerator_map\n",
    "        if is_restricted_image\n",
    "        else default_training_accelerator_map\n",
    "    )\n",
    "    if accelerator_type in training_accelerator_map:\n",
    "      if is_dynamic_workload_scheduler:\n",
    "        return dws_training_accelerator_map[accelerator_type]\n",
    "      else:\n",
    "        return training_accelerator_map[accelerator_type]\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          f\"Could not find accelerator type: {accelerator_type} for training.\"\n",
    "      )\n",
    "  else:\n",
    "    if is_dynamic_workload_scheduler:\n",
    "      raise ValueError(\"Dynamic Workload Scheduler does not work for serving.\")\n",
    "    if accelerator_type in serving_accelerator_map:\n",
    "      return serving_accelerator_map[accelerator_type]\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          f\"Could not find accelerator type: {accelerator_type} for serving.\"\n",
    "      )\n",
    "\n",
    "def check_quota(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    accelerator_type: str,\n",
    "    accelerator_count: int,\n",
    "    is_for_training: bool,\n",
    "    is_restricted_image: bool = False,\n",
    "    is_dynamic_workload_scheduler: bool = False,\n",
    "):\n",
    "  \"\"\"Checks if the project and the region has the required quota.\"\"\"\n",
    "  resource_id = get_resource_id(\n",
    "      accelerator_type,\n",
    "      is_for_training=is_for_training,\n",
    "      is_restricted_image=is_restricted_image,\n",
    "      is_dynamic_workload_scheduler=is_dynamic_workload_scheduler,\n",
    "  )\n",
    "  quota = get_quota(project_id, region, resource_id)\n",
    "  quota_request_instruction = (\n",
    "      \"Either use \"\n",
    "      \"a different region or request additional quota. Follow \"\n",
    "      \"instructions here \"\n",
    "      \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"\n",
    "      \" to check quota in a region or request additional quota for \"\n",
    "      \"your project.\"\n",
    "  )\n",
    "  if quota == -1:\n",
    "    raise ValueError(\n",
    "        f\"Quota not found for: {resource_id} in {region}.\"\n",
    "        f\" {quota_request_instruction}\"\n",
    "    )\n",
    "  if quota < accelerator_count:\n",
    "    raise ValueError(\n",
    "        f\"Quota not enough for {resource_id} in {region}: {quota} <\"\n",
    "        f\" {accelerator_count}. {quota_request_instruction}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3f363e-84ce-45f9-aa6b-b34938e2a4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "replica_count = 1\n",
    "\n",
    "check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=per_node_accelerator_count * replica_count,\n",
    "    is_for_training=True,\n",
    "    is_restricted_image=is_restricted_image,\n",
    "    is_dynamic_workload_scheduler=is_dynamic_workload_scheduler,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b365a24c-5760-4eef-8e27-5db956c1c5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "job_name = f\"gemma2-lora-train-{now}\".replace(\"_\", \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5b6e7b-b3d6-48aa-87a7-1e1f13f7a971",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_output_dir = os.path.join(STAGING_BUCKET, job_name)\n",
    "# Create a GCS folder to store the LORA adapter.\n",
    "lora_output_dir = os.path.join(base_output_dir, \"adapter\")\n",
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# finetuned LORA adapter.\n",
    "merged_model_output_dir = os.path.join(base_output_dir, \"merged-model\")\n",
    "\n",
    "eval_args = [\n",
    "    f\"--eval_dataset_path={eval_dataset_name}\",\n",
    "    f\"--eval_column={instruct_column_in_dataset}\",\n",
    "    f\"--eval_template={template}\",\n",
    "    f\"--eval_split={eval_split_name}\",\n",
    "    f\"--eval_steps={save_steps}\",\n",
    "    \"--eval_tasks=builtin_eval\",\n",
    "    \"--eval_metric_name=loss\",\n",
    "]\n",
    "\n",
    "train_job_args = [\n",
    "    \"--config_file=vertex_vision_model_garden_peft/deepspeed_zero2_8gpu.yaml\",\n",
    "    \"--task=instruct-lora\",\n",
    "    \"--completion_only=True\",\n",
    "    f\"--pretrained_model_id={pretrained_model_id}\",\n",
    "    f\"--dataset_name={train_dataset_name}\",\n",
    "    f\"--train_split_name={train_split_name}\",\n",
    "    f\"--instruct_column_in_dataset={instruct_column_in_dataset}\",\n",
    "    f\"--output_dir={lora_output_dir}\",\n",
    "    f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
    "    f\"--per_device_train_batch_size={per_device_train_batch_size}\",\n",
    "    f\"--gradient_accumulation_steps={gradient_accumulation_steps}\",\n",
    "    f\"--lora_rank={lora_rank}\",\n",
    "    f\"--lora_alpha={lora_alpha}\",\n",
    "    f\"--lora_dropout={lora_dropout}\",\n",
    "    f\"--max_steps={max_steps}\",\n",
    "    f\"--max_seq_length={max_seq_length}\",\n",
    "    f\"--learning_rate={learning_rate}\",\n",
    "    f\"--lr_scheduler_type={lr_scheduler_type}\",\n",
    "    f\"--precision_mode={finetuning_precision_mode}\",\n",
    "    f\"--train_precision={train_precision}\",\n",
    "    f\"--enable_gradient_checkpointing={enable_gradient_checkpointing}\",\n",
    "    f\"--num_epochs={num_epochs}\",\n",
    "    f\"--attn_implementation={attn_implementation}\",\n",
    "    f\"--optimizer={optimizer}\",\n",
    "    f\"--warmup_ratio={warmup_ratio}\",\n",
    "    f\"--report_to={report_to}\",\n",
    "    f\"--logging_output_dir={base_output_dir}\",\n",
    "    f\"--save_steps={save_steps}\",\n",
    "    f\"--logging_steps={logging_steps}\",\n",
    "    f\"--template={template}\",\n",
    "    f\"--huggingface_access_token={HF_TOKEN}\",\n",
    "] + eval_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ddda30-5466-40fc-85c8-d0630efb4598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass training arguments and launch job.\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "\n",
    "print(\"Running training job with args:\")\n",
    "print(\" \\\\\\n\".join(train_job_args))\n",
    "train_job.run(\n",
    "    args=train_job_args,\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=per_node_accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    base_output_dir=base_output_dir,\n",
    "    sync=False,  # Non-blocking call to run.\n",
    "    **dws_kwargs,\n",
    ")\n",
    "\n",
    "# Wait until resource has been created.\n",
    "train_job.wait_for_resource_creation()\n",
    "\n",
    "print(\"LoRA adapter will be saved in:\", lora_output_dir)\n",
    "print(\"Trained and merged models will be saved in:\", merged_model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6752c213-8fb5-4403-a5e7-f1b3c6bf4276",
   "metadata": {},
   "source": [
    "### Run Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e02a427-c852-4aac-a687-508218607043",
   "metadata": {},
   "source": [
    "This section shows how to launch TensorBoard in a [Cloud Shell](https://cloud.google.com/shell/docs).\n",
    "1. Click the Cloud Shell icon(![terminal](https://github.com/google/material-design-icons/blob/master/png/action/terminal/materialicons/24dp/1x/baseline_terminal_black_24dp.png?raw=true)) on the top right to open the Cloud Shell.\n",
    "2. Copy the `tensorboard` command shown below by running this cell.\n",
    "3. Paste and run the command in the Cloud Shell to launch TensorBoard.\n",
    "4. Once the command runs (You may have to click `Authorize` if prompted), click the link starting with `http://localhost`.\n",
    "\n",
    "Note: You may need to wait around 10 minutes after the job starts in order for the TensorBoard logs to be written to the GCS bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56ef1db-5921-4200-a2fb-e57013054f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Command to copy: tensorboard --logdir {base_output_dir}/logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60644e94-175a-4106-99cf-59bab0ecde59",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0872d7f9-3d16-469a-a615-014b7ed706a8",
   "metadata": {},
   "source": [
    "This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c67d8-95c7-42c8-8f4c-d92a54906d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_job.end_time is None:\n",
    "    print(\"Waiting for the training job to finish...\")\n",
    "    train_job.wait()\n",
    "    print(\"The training job has finished.\")\n",
    "\n",
    "print(\"Deploying models in:\", merged_model_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b3878-a1b5-4318-839b-3110b31568f5",
   "metadata": {},
   "source": [
    "The pre-built serving docker image for vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e663c2-409f-43e7-ab7f-f8ed3e126603",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240815_1634_RC00\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b35094-08f0-4a86-91c7-58f79620e74a",
   "metadata": {},
   "source": [
    "Accelerator type to use for serving. Find Vertex AI prediction supported accelerators and regions [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3fb26-1197-4792-8d53-2ec482864ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\"] {isTemplate: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcf5aba-7e6b-48c0-95b9-ae569e2003d9",
   "metadata": {},
   "source": [
    "Machine type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0c0a3b-30e2-4581-b31b-ea034f896893",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"2b\" in base_model_id:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        # Sets 1 L4 (24G) to deploy Gemma 2 2B models.\n",
    "        machine_type = \"g2-standard-12\"\n",
    "        accelerator_count = 1\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Recommended machine settings not found for accelerator type: %s\"\n",
    "            % accelerator_type\n",
    "        )\n",
    "elif \"9b\" in base_model_id:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        # Sets 2 L4 (24G) to deploy Gemma 2 9B models.\n",
    "        machine_type = \"g2-standard-24\"\n",
    "        accelerator_count = 2\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Recommended machine settings not found for accelerator type: %s\"\n",
    "            % accelerator_type\n",
    "        )\n",
    "elif \"27b\" in base_model_id:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        # Sets 4 L4 (24G) to deploy Gemma 2 27B models.\n",
    "        machine_type = \"g2-standard-48\"\n",
    "        accelerator_count = 4\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Recommended machine settings not found for accelerator type: %s\"\n",
    "            % accelerator_type\n",
    "        )\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Recommended machine settings not found for model: %s\" % base_model_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c3ce9b-a6be-41a2-a053-0f61c2630964",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e33bb9-417a-4651-b26d-fdad78d9c1e6",
   "metadata": {},
   "source": [
    "Set use_dedicated_endpoint to True if the endpoint is [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint) enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e51ba0-ecac-483e-bd65-96ebbb3bc512",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dedicated_endpoint = False  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964e4707-d776-40f0-a5ae-f62d53cbb08d",
   "metadata": {},
   "source": [
    "Accelerator parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1267581-1d72-4bf8-922f-55a5258999ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_memory_utilization = 0.85\n",
    "max_model_len = 4096  # Maximum context length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c392ba-1cca-4b85-a0c7-73c989bd063f",
   "metadata": {},
   "source": [
    "Deployment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795b9e9a-0fdf-4fc2-b052-39f8e663b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    base_model_id: str = None,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    gpu_memory_utilization: float = 0.9,\n",
    "    max_model_len: int = 4096,\n",
    "    dtype: str = \"auto\",\n",
    "    enable_trust_remote_code: bool = False,\n",
    "    enforce_eager: bool = False,\n",
    "    enable_lora: bool = False,\n",
    "    max_loras: int = 1,\n",
    "    max_cpu_loras: int = 8,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    "    max_num_seqs: int = 256,\n",
    "    model_type: str = None,\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name}-endpoint\",\n",
    "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "    )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    # See https://docs.vllm.ai/en/latest/models/engine_args.html for a list of possible arguments with descriptions.\n",
    "    vllm_args = [\n",
    "        \"python\",\n",
    "        \"-m\",\n",
    "        \"vllm.entrypoints.api_server\",\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=8080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        f\"--max-loras={max_loras}\",\n",
    "        f\"--max-cpu-loras={max_cpu_loras}\",\n",
    "        f\"--max-num-seqs={max_num_seqs}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    if enable_trust_remote_code:\n",
    "        vllm_args.append(\"--trust-remote-code\")\n",
    "\n",
    "    if enforce_eager:\n",
    "        vllm_args.append(\"--enforce-eager\")\n",
    "\n",
    "    if enable_lora:\n",
    "        vllm_args.append(\"--enable-lora\")\n",
    "\n",
    "    if model_type:\n",
    "        vllm_args.append(f\"--model-type={model_type}\")\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[8080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "    )\n",
    "    print(\n",
    "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    print(\"endpoint_name:\", endpoint.name)\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164299dc-b315-40f4-9940-e264f3d3e49d",
   "metadata": {},
   "source": [
    "Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c5dc1-2d97-42d4-9a5c-1f5c6c31cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"gemma2-vllm-serve-{now}\".replace(\"_\", \"-\")\n",
    "\n",
    "models[\"vllm_gpu\"], endpoints[\"vllm_gpu\"] = deploy_model_vllm(\n",
    "    model_name=model_name,\n",
    "    model_id=merged_model_output_dir,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    max_model_len=max_model_len,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f855b784-4862-45b4-b032-51adc6bcaaf5",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c373c9-efb8-429e-9f1d-9cc9a41eb11e",
   "metadata": {},
   "source": [
    "Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html).\n",
    "\n",
    "Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e232521-920e-46f5-957b-989d66e28763",
   "metadata": {},
   "source": [
    "**[Optional]** Loads an existing endpoint instance using the endpoint name:\n",
    "- Using `endpoint_name = endpoint.name` allows us to get the endpoint name of the endpoint `endpoint` created in the cells above.\n",
    "- Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load an existing endpoint with the ID 1234567890123456789.\n",
    "You may uncomment the code below to load an existing endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63faaa-9612-4d35-ae9b-008ec60a1212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoints[\"vllm_gpu\"] = aiplatform.Endpoint(aip_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1393b60f-7bb1-4f19-9aab-6131924b844a",
   "metadata": {},
   "source": [
    "Here we use an Example:\n",
    "\n",
    "```\n",
    "Prompt: What is a car?\n",
    "Output:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db50e2-4e77-4cc4-9d93-39813b45fe66",
   "metadata": {},
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4586672a-072a-498f-a59e-47fa7e188984",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is a car?\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8499626c-ca82-47a9-aaa9-5ecc0a898b0d",
   "metadata": {},
   "source": [
    "Generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842983c7-6bee-458f-bd09-8422a33da492",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 50  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "top_p = 1.0  # @param {type:\"number\"}\n",
    "top_k = 1  # @param {type:\"integer\"}\n",
    "raw_response = False  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa54c2b-42fb-4645-a256-f946fe6d39f5",
   "metadata": {},
   "source": [
    "Note: If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, by lowering `max_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b4a69-9e0f-43f0-9e75-16a523f28105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7315dc66-ca54-4275-bdcf-3d7eb53c8b11",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016fa2de-55a2-47c1-8627-fa7eadd6def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoints[\"vllm_gpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897536e-5c29-4aae-bebd-09b6ba64a286",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0989e3-629c-4586-b449-2d47da623fa9",
   "metadata": {},
   "source": [
    "Delete the experiment models and endpoints to recycle the resources and avoid unnecessary continuous charges that may incur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee5833e-6223-4dcd-912b-b819b89eeb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_resources = False # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b8fa06-5dac-4873-aefe-e56847f78b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if delete_resources:\n",
    "    # Delete the train job.\n",
    "    train_job.delete()\n",
    "    \n",
    "    # Undeploy model and delete endpoint.\n",
    "    for endpoint in endpoints.values():\n",
    "        endpoint.delete(force=True)\n",
    "    \n",
    "    # Delete models.\n",
    "    for model in models.values():\n",
    "        model.delete()\n",
    "    ! gsutil -m rm -r $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ce9927-d27e-45f2-8362-3d0aefb56fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
