{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc4905-50af-4b72-a030-497a8fb6b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d5b55-9d39-44c5-bb6a-07ced136e73b",
   "metadata": {},
   "source": [
    "# Gemma2 Deployment on Vertex AI with Hex-LLM and TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12621c-44d5-4d9c-9858-f22ef002539e",
   "metadata": {},
   "source": [
    "Based on [model_garden_gemma2_deployment_on_vertex.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma2_deployment_on_vertex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7653a270-a5d4-4ed6-baf8-4eaeb467f996",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-colab&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-colab&destination=evaluation_rag_use_cases-from_notebook-colab&url=https%3A%2F%2Fcolab.sandbox.google.com%2Fgithub%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_evaluation_services%2Fevaluation-rag-systems%2Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-colab_ent&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-colab_ent&destination=evaluation_rag_use_cases-from_notebook-colab_ent&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fcolab%2Fimport%2Fhttps%3A%252F%252Fraw.githubusercontent.com%252FGoogleCloudPlatform%252Fapplied-ai-engineering-samples%252Fmain%252Fgenai-on-vertex-ai%252Fvertex_evaluation_services%252Fevaluation-rag-systems%252Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-github&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-github&destination=evaluation_rag_use_cases-from_notebook-github&url=https%3A%2F%2Fgithub.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_evaluation_services%2Fevaluation-rag-systems%2Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://art-analytics.appspot.com/r.html?uaid=G-FHXEFWTT4E&utm_source=aRT-evaluation_rag_use_cases-from_notebook-vai_workbench&utm_medium=aRT-clicks&utm_campaign=evaluation_rag_use_cases-from_notebook-vai_workbench&destination=evaluation_rag_use_cases-from_notebook-vai_workbench&url=https%3A%2F%2Fconsole.cloud.google.com%2Fvertex-ai%2Fworkbench%2Fdeploy-notebook%3Fdownload_url%3Dhttps%3A%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fmain%2Fgenai-on-vertex-ai%2Fvertex_evaluation_services%2Fevaluation-rag-systems%2Fevaluation_rag_use_cases.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7f7200-359a-4c4c-962f-e97347025a98",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "    <td>Author(s)</td>\n",
    "    <td>Egon Soares</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ed964-587a-4542-b14d-ff487a7bc42e",
   "metadata": {},
   "source": [
    "![gemma2 deployment architecture](images/1.2-gemma2-deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a02d7e-a1d5-4b0f-afca-514d7c99810a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying Gemma 2 models\n",
    " * on TPU using **Hex-LLM**, a **H**igh-**E**fficiency **L**arge **L**anguage **M**odel serving solution built with **XLA** that is being developed by Google Cloud, and\n",
    " * on GPU using **TGI** ([text-generation-inference](https://github.com/huggingface/text-generation-inference)), the state-of-the-art open source LLM serving solution on GPU.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Deploy Gemma 2 with Hex-LLM on TPU\n",
    "- Deploy Gemma with [TGI](https://github.com/huggingface/text-generation-inference) on GPU\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55678d89-a4e4-49b4-a702-dd3deb8557d2",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96326f1-ba96-407c-a186-492565503c50",
   "metadata": {},
   "source": [
    "By default, the quota for TPU deployment `Custom model serving TPU v5e cores per region` is 4. TPU quota is only available in `us-west1`. You can request for higher TPU quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d322cb3-1631-4bb8-a013-48856eaf3a2b",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89be7e-ee1b-4fee-9cdd-73f03f34bb69",
   "metadata": {},
   "source": [
    "[Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919155e6-80e7-43a6-9ef8-36ded477651f",
   "metadata": {},
   "source": [
    "**[Optional]** Set project. If not set, the project will be set automatically according to the environment variable \"GOOGLE_CLOUD_PROJECT\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86352e46-1bc3-4304-8d57-b354dd65b734",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e143e-0621-4572-af1f-14771c0a1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68775d4f-4bb5-40d1-aedf-564848df6906",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not PROJECT_ID:\n",
    "    # Get the default cloud project id.\n",
    "    PROJECT_ID = os.environ.get(\"GOOGLE_CLOUD_PROJECT\", \"\")\n",
    "    assert PROJECT_ID, \"Provide a google cloud project id.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a19b09-6180-4c0b-8e0d-4900680df796",
   "metadata": {},
   "source": [
    "**[Optional]** Set region. If not set, the region will be set automatically according to the environment variable \"GOOGLE_CLOUD_REGION\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b49cbd-1500-47f4-99b9-ea5913cabc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a8fd9-518d-42e2-b7c0-62097755ad0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REGION:\n",
    "    # Get the default region for launching jobs.\n",
    "    REGION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"\")\n",
    "    assert REGION, \"Provide a google cloud region.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483c4dd-c1fe-43f9-aa4e-3dbc45369838",
   "metadata": {},
   "source": [
    "Upgrade Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea465408-2e6a-4846-97a9-d8654cca8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71721b23-39b9-4fc6-a2fa-f063b729967c",
   "metadata": {},
   "source": [
    "## Access Gemma 2 Models\n",
    "You must provide a Hugging Face User Access Token (read) to access the Gemma 2 models. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d115c485-068a-4f42-91be-a62477f712dd",
   "metadata": {},
   "source": [
    "**[Optional]** Set a Hugging Face read token. If not set, the token will be set automatically according to the environment variable \"HF_TOKEN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d7fa62-559c-45c8-b48f-2459392560c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36910054-b840-4bf6-a3ab-59f414180c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not HF_TOKEN:\n",
    "    # Get the HF token from the environment.\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\", \"\")\n",
    "    assert HF_TOKEN, \"Provide a read HF_TOKEN to load models from Hugging Face.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057d692d-c9dc-441a-b526-3241d3bc5c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_prefix = \"google/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea7038-ab18-487f-aa84-72bc00d74a2f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62cd6b-c4ad-4a64-abf5-0dd786c6f2bd",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2cca8-0fb8-4e10-a589-7070fdb82094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b907f-8e5d-46ce-92c7-7e75d2bcd072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "running_in_colab = \"google.colab\" in sys.modules\n",
    "\n",
    "if running_in_colab and os.environ.get(\"VERTEX_PRODUCT\", \"\") != \"COLAB_ENTERPRISE\":\n",
    "    from google.colab import auth as colab_auth\n",
    "    \n",
    "    colab_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d047056d-ba56-4fac-9a8e-d9dc466bcd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae73c7-a341-48c3-b14a-39ffa0bee1cc",
   "metadata": {},
   "source": [
    "Gets the default Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a437d59-066c-4587-9bff-13c70a589f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c6ca9-6b0d-444c-9909-465a816a400c",
   "metadata": {},
   "source": [
    "Initialize Vertex AI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2acc01-7e13-4341-815d-0e15e63b59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c103d-7027-4e3a-a21f-a0d1e4ebe90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, endpoints = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac453e05-ee49-449a-9d1d-3dcfed4d6138",
   "metadata": {},
   "source": [
    "## Deploy Gemma 2 models with Hex-LLM on TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db899e-747e-416b-b69e-8b76f42f2ff6",
   "metadata": {},
   "source": [
    "**Hex-LLM** is a **H**igh-**E**fficiency **L**arge **L**anguage **M**odel (LLM) TPU serving solution built with **XLA**, which is being developed by Google Cloud.\n",
    "\n",
    "Refer to the \"Request for TPU quota\" section for TPU quota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9b4cc-36dd-4235-baff-9d134d1a9cd8",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93674646-bda6-41a8-bcf9-7ba5969a73af",
   "metadata": {},
   "source": [
    "The pre-built serving docker images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ac1b0-59b5-4ced-949c-ef734bd84acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEXLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/hex-llm-serve:gemma2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb52e3-ff17-4b62-8203-716c96f26f5f",
   "metadata": {},
   "source": [
    "Set the model ID. Model weights can be loaded from HuggingFace or from a GCS bucket. Select one of the four model variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209fa0e-0f24-423b-bdc1-1f50e3c0dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemma-2-2b-it\"  # @param [\"gemma-2-2b\", \"gemma-2-2b-it\", \"gemma-2-9b\", \"gemma-2-9b-it\", \"gemma-2-27b\", \"gemma-2-27b-it\"] {allow-input: true, isTemplate: true}\n",
    "TPU_DEPLOYMENT_REGION = \"us-west1\"  # @param [\"us-west1\"] {isTemplate:true}\n",
    "model_id = os.path.join(model_path_prefix, MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b93038-be84-4acb-ae98-25d18fca27e8",
   "metadata": {},
   "source": [
    "Choose a machine type. You can find Vertex AI prediction TPUv5e machine types in https://cloud.google.com/vertex-ai/docs/predictions/use-tpu#deploy_a_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a165151-51e5-4de0-80ad-6341b438540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"2b\" in model_id:\n",
    "    # Sets ct5lp-hightpu-1t (1 TPU chip) to deploy Gemma 2 2B models.\n",
    "    machine_type = \"ct5lp-hightpu-1t\"\n",
    "    accelerator_type = \"TPU_V5e\"\n",
    "    # Note: 1 TPU V5 chip has only one core.\n",
    "    accelerator_count = 1\n",
    "elif \"9b\" in model_id:\n",
    "    # Sets ct5lp-hightpu-4t (4 TPU chips) to deploy Gemma 2 9B models.\n",
    "    machine_type = \"ct5lp-hightpu-4t\"\n",
    "    accelerator_type = \"TPU_V5e\"\n",
    "    # Note: 1 TPU V5 chip has only one core.\n",
    "    accelerator_count = 4\n",
    "else:\n",
    "    # Sets ct5lp-hightpu-8t (8 TPU chips) to deploy Gemma 2 27B models.\n",
    "    machine_type = \"ct5lp-hightpu-8t\"\n",
    "    accelerator_type = \"TPU_V5e\"\n",
    "    # Note: 1 TPU V5 chip has only one core.\n",
    "    accelerator_count = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3875c9-0645-4ba2-a824-208f2fb674df",
   "metadata": {},
   "source": [
    "(Optional) Check quota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13359fc-46dd-4d17-9f06-c4234e8967b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "\n",
    "def get_quota(project_id: str, region: str, resource_id: str) -> int:\n",
    "  \"\"\"Returns the quota for a resource in a region.\n",
    "\n",
    "  Args:\n",
    "    project_id: The project id.\n",
    "    region: The region.\n",
    "    resource_id: The resource id.\n",
    "\n",
    "  Returns:\n",
    "    The quota for the resource in the region. Returns -1 if can not figure out\n",
    "    the quota.\n",
    "\n",
    "  Raises:\n",
    "    RuntimeError: If the command to get quota fails.\n",
    "  \"\"\"\n",
    "  service_endpoint = \"aiplatform.googleapis.com\"\n",
    "\n",
    "  command = (\n",
    "      \"gcloud alpha services quota list\"\n",
    "      f\" --service={service_endpoint} --consumer=projects/{project_id}\"\n",
    "      f\" --filter='{service_endpoint}/{resource_id}' --format=json\"\n",
    "  )\n",
    "  process = subprocess.run(\n",
    "      command, shell=True, capture_output=True, text=True, check=True\n",
    "  )\n",
    "  if process.returncode == 0:\n",
    "    quota_data = json.loads(process.stdout)\n",
    "  else:\n",
    "    raise RuntimeError(f\"Error fetching quota data: {process.stderr}\")\n",
    "\n",
    "  if not quota_data or \"consumerQuotaLimits\" not in quota_data[0]:\n",
    "    return -1\n",
    "  if (\n",
    "      not quota_data[0][\"consumerQuotaLimits\"]\n",
    "      or \"quotaBuckets\" not in quota_data[0][\"consumerQuotaLimits\"][0]\n",
    "  ):\n",
    "    return -1\n",
    "  all_regions_data = quota_data[0][\"consumerQuotaLimits\"][0][\"quotaBuckets\"]\n",
    "  for region_data in all_regions_data:\n",
    "    if (\n",
    "        region_data.get(\"dimensions\")\n",
    "        and region_data[\"dimensions\"][\"region\"] == region\n",
    "    ):\n",
    "      if \"effectiveLimit\" in region_data:\n",
    "        return int(region_data[\"effectiveLimit\"])\n",
    "      else:\n",
    "        return 0\n",
    "  return -1\n",
    "\n",
    "def get_resource_id(\n",
    "    accelerator_type: str,\n",
    "    is_for_training: bool,\n",
    "    is_restricted_image: bool = False,\n",
    "    is_dynamic_workload_scheduler: bool = False,\n",
    ") -> str:\n",
    "  \"\"\"Returns the resource id for a given accelerator type and the use case.\n",
    "\n",
    "  Args:\n",
    "    accelerator_type: The accelerator type.\n",
    "    is_for_training: Whether the resource is used for training. Set false for\n",
    "      serving use case.\n",
    "    is_restricted_image: Whether the image is hosted in `vertex-ai-restricted`.\n",
    "    is_dynamic_workload_scheduler: Whether the resource is used with Dynamic\n",
    "      Workload Scheduler.\n",
    "\n",
    "  Returns:\n",
    "    The resource id.\n",
    "  \"\"\"\n",
    "  accelerator_suffix_map = {\n",
    "      \"NVIDIA_TESLA_V100\": \"nvidia_v100_gpus\",\n",
    "      \"NVIDIA_L4\": \"nvidia_l4_gpus\",\n",
    "      \"NVIDIA_TESLA_A100\": \"nvidia_a100_gpus\",\n",
    "      \"NVIDIA_A100_80GB\": \"nvidia_a100_80gb_gpus\",\n",
    "      \"NVIDIA_H100_80GB\": \"nvidia_h100_gpus\",\n",
    "      \"NVIDIA_TESLA_T4\": \"nvidia_t4_gpus\",\n",
    "      \"TPU_V5e\": \"tpu_v5e\",\n",
    "      \"TPU_V3\": \"tpu_v3\",\n",
    "  }\n",
    "  default_training_accelerator_map = {\n",
    "      key: f\"custom_model_training_{accelerator_suffix_map[key]}\"\n",
    "      for key in accelerator_suffix_map\n",
    "  }\n",
    "  dws_training_accelerator_map = {\n",
    "      key: f\"custom_model_training_preemptible_{accelerator_suffix_map[key]}\"\n",
    "      for key in accelerator_suffix_map\n",
    "  }\n",
    "  restricted_image_training_accelerator_map = {\n",
    "      \"NVIDIA_A100_80GB\": \"restricted_image_training_nvidia_a100_80gb_gpus\",\n",
    "  }\n",
    "  serving_accelerator_map = {\n",
    "      key: f\"custom_model_serving_{accelerator_suffix_map[key]}\"\n",
    "      for key in accelerator_suffix_map\n",
    "  }\n",
    "\n",
    "  if is_for_training:\n",
    "    if is_restricted_image and is_dynamic_workload_scheduler:\n",
    "      raise ValueError(\n",
    "          \"Dynamic Workload Scheduler does not work for restricted image\"\n",
    "          \" training.\"\n",
    "      )\n",
    "    training_accelerator_map = (\n",
    "        restricted_image_training_accelerator_map\n",
    "        if is_restricted_image\n",
    "        else default_training_accelerator_map\n",
    "    )\n",
    "    if accelerator_type in training_accelerator_map:\n",
    "      if is_dynamic_workload_scheduler:\n",
    "        return dws_training_accelerator_map[accelerator_type]\n",
    "      else:\n",
    "        return training_accelerator_map[accelerator_type]\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          f\"Could not find accelerator type: {accelerator_type} for training.\"\n",
    "      )\n",
    "  else:\n",
    "    if is_dynamic_workload_scheduler:\n",
    "      raise ValueError(\"Dynamic Workload Scheduler does not work for serving.\")\n",
    "    if accelerator_type in serving_accelerator_map:\n",
    "      return serving_accelerator_map[accelerator_type]\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          f\"Could not find accelerator type: {accelerator_type} for serving.\"\n",
    "      )\n",
    "\n",
    "def check_quota(\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    accelerator_type: str,\n",
    "    accelerator_count: int,\n",
    "    is_for_training: bool,\n",
    "    is_restricted_image: bool = False,\n",
    "    is_dynamic_workload_scheduler: bool = False,\n",
    "):\n",
    "  \"\"\"Checks if the project and the region has the required quota.\"\"\"\n",
    "  resource_id = get_resource_id(\n",
    "      accelerator_type,\n",
    "      is_for_training=is_for_training,\n",
    "      is_restricted_image=is_restricted_image,\n",
    "      is_dynamic_workload_scheduler=is_dynamic_workload_scheduler,\n",
    "  )\n",
    "  quota = get_quota(project_id, region, resource_id)\n",
    "  quota_request_instruction = (\n",
    "      \"Either use \"\n",
    "      \"a different region or request additional quota. Follow \"\n",
    "      \"instructions here \"\n",
    "      \"https://cloud.google.com/docs/quotas/view-manage#requesting_higher_quota\"\n",
    "      \" to check quota in a region or request additional quota for \"\n",
    "      \"your project.\"\n",
    "  )\n",
    "  if quota == -1:\n",
    "    raise ValueError(\n",
    "        f\"Quota not found for: {resource_id} in {region}.\"\n",
    "        f\" {quota_request_instruction}\"\n",
    "    )\n",
    "  if quota < accelerator_count:\n",
    "    raise ValueError(\n",
    "        f\"Quota not enough for {resource_id} in {region}: {quota} <\"\n",
    "        f\" {accelerator_count}. {quota_request_instruction}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c808fb5-c46c-45fe-a904-f8cf6e4d0863",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=TPU_DEPLOYMENT_REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee67aef-0425-4845-983e-4769763f4feb",
   "metadata": {},
   "source": [
    "Server parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b62c2-c304-41cc-ab70-4a5c081a4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_parallel_size = accelerator_count\n",
    "hbm_utilization_factor = 0.6  # Fraction of HBM memory allocated for KV cache after model loading. A larger value improves throughput but gives higher risk of TPU out-of-memory errors with long prompts.\n",
    "max_running_seqs = 256  # Maximum number of running sequences in a continuous batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9380113-9142-4227-a212-aef1174f0ff6",
   "metadata": {},
   "source": [
    "Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de220ba-59be-4b85-8384-2e08b1eec74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dedicated_endpoint = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e096e-7018-447f-84d4-6a0747b1eb64",
   "metadata": {},
   "source": [
    "Endpoint configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ea031-e5c2-4ed0-8618-2f7109f31037",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_replica_count = 1\n",
    "max_replica_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e00376-029e-4c59-a881-2e233eed0e15",
   "metadata": {},
   "source": [
    "Deployment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c330692-3e77-43e9-9e6c-b1ccf4254a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_hexllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    base_model_id: str = None,\n",
    "    tensor_parallel_size: int = 1,\n",
    "    machine_type: str = \"ct5lp-hightpu-1t\",\n",
    "    tpu_topology: str = \"1x1\",\n",
    "    hbm_utilization_factor: float = 0.6,\n",
    "    max_running_seqs: int = 256,\n",
    "    max_model_len: int = 4096,\n",
    "    endpoint_id: str = \"\",\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with Hex-LLM on TPU in Vertex AI.\"\"\"\n",
    "    if endpoint_id:\n",
    "        aip_endpoint_name = (\n",
    "            f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
    "        )\n",
    "        endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=f\"{model_name}-endpoint\",\n",
    "            location=TPU_DEPLOYMENT_REGION,\n",
    "            dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "        )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    if not tensor_parallel_size:\n",
    "        tensor_parallel_size = int(machine_type[-2])\n",
    "\n",
    "    num_hosts = int(tpu_topology.split(\"x\")[0])\n",
    "\n",
    "    hexllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor_parallel_size={tensor_parallel_size}\",\n",
    "        f\"--num_hosts={num_hosts}\",\n",
    "        f\"--hbm_utilization_factor={hbm_utilization_factor}\",\n",
    "        f\"--max_running_seqs={max_running_seqs}\",\n",
    "        f\"--max_model_len={max_model_len}\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"HEX_LLM_LOG_LEVEL\": \"info\",\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars.update({\"HF_TOKEN\": HF_TOKEN})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"hex_llm.server.api_server\"],\n",
    "        serving_container_args=hexllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        location=TPU_DEPLOYMENT_REGION,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        tpu_topology=tpu_topology if num_hosts > 1 else None,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646aba7-0214-4266-aaf7-413fcc9276b9",
   "metadata": {},
   "source": [
    "Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f5996-9031-42ce-8cde-70a3a732f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"{MODEL_ID}-{now}\".replace(\"_\", \"-\")\n",
    "\n",
    "models[\"hexllm_tpu\"], endpoints[\"hexllm_tpu\"] = deploy_model_hexllm(\n",
    "    model_name=model_name,\n",
    "    model_id=model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    hbm_utilization_factor=hbm_utilization_factor,\n",
    "    max_running_seqs=max_running_seqs,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328d89d-18af-4546-9c9f-51d15c3b0418",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1165f-f7d0-4145-b31a-86c60c6288ba",
   "metadata": {},
   "source": [
    "Once deployment succeeds, you can send requests to the endpoint with text prompts based on your `template`. Note that the first few prompts will take longer to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a86c5-2be4-48bf-b12f-f6c7ff532acd",
   "metadata": {},
   "source": [
    "**[Optional]** Loads an existing endpoint instance using the endpoint name:\n",
    "- Using `endpoint_name = endpoint.name` allows us to get the endpoint name of the endpoint `endpoint` created in the cell above.\n",
    "- Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load an existing endpoint with the ID 1234567890123456789."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac2c6d-3d7d-4ad0-b223-e3833efa7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment the code below to load an existing endpoint:\n",
    "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoints[\"hexllm_tpu\"] = aiplatform.Endpoint(aip_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ede3c-4bd7-492f-8260-08bb6a4c19b0",
   "metadata": {},
   "source": [
    "**[Optional]** Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12ec86-21cf-4fa9-9b0f-95943676f74d",
   "metadata": {},
   "source": [
    "Here we use an example:\n",
    "```\n",
    "Prompt: What is a car?\n",
    "Output: A car is a four-wheeled vehicle designed for the transportation of passengers and their belongings.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2be3cb-979d-41c6-b5c2-d1d9e6a92855",
   "metadata": {},
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd48ca8-2214-44e9-a548-4bd675009e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is a car?\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989cb4b-2bd9-4cec-b5aa-11474f7a0702",
   "metadata": {},
   "source": [
    "Generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7de6da-0d27-44a6-9d64-e1caaca3d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 50  # @param {type: \"integer\"}\n",
    "temperature = 1.0  # @param {type: \"number\"}\n",
    "top_p = 1.0  # @param {type: \"number\"}\n",
    "top_k = 1  # @param {type: \"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759dfae0-7832-4ddb-a8c5-d2bc2390bfe8",
   "metadata": {},
   "source": [
    "Note: If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fd67e-2435-4b54-b5dd-7e1a70de0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efda6ca-6a0b-4b77-9ee0-e6a5cd29d5d9",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f3216-0c1c-4390-9621-ebca1cefb4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoints[\"hexllm_tpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c335a-0d70-4f99-b5a9-ff81c125fc73",
   "metadata": {},
   "source": [
    "## Deploy Gemma models with TGI on GPU\n",
    "\n",
    "[TGI](https://github.com/huggingface/text-generation-inference) stands for Text Generation Inference. It's a powerful library designed specifically for running large language models on GPUs efficiently. TGI utilizes techniques like \"paged attention\" and \"continuous batching\" to improve the speed and throughput of LLMs.\n",
    "\n",
    "Currently, only L4 GPUs are demonstrated in this notebook. Functionality on other GPU types will be confirmed and added in the future.\n",
    "\n",
    "Gemma2 2B, 9B and 27B models require at least 1, 2, and 4 L4 GPUs respectively for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd37540-559c-401b-ac16-31e3954a06f2",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98d317-88ed-4817-bd1b-18ff51e2a2fc",
   "metadata": {},
   "source": [
    "The pre-built serving docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3683245-e015-4a5c-a273-a863d9a3f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "TGI_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-1.ubuntu2204.py310\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0825834-a383-44cd-8440-0fbd1daa72f8",
   "metadata": {},
   "source": [
    "Set the model ID. Model weights can be loaded from HuggingFace or from a GCS bucket. Select one of the four model variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f064a-70dd-44fc-9d5f-a2cc30961919",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemma-2-2b\"  # @param [\"gemma-2-2b\", \"gemma-2-2b-it\", \"gemma-2-9b\", \"gemma-2-9b-it\", \"gemma-2-27b\", \"gemma-2-27b-it\"] {allow-input: true, isTemplate: true}\n",
    "model_id = os.path.join(model_path_prefix, MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f404cb-622a-4641-8a9c-80ccffb84ceb",
   "metadata": {},
   "source": [
    "Choose a GPU.\n",
    "You can find Vertex AI prediction supported accelerators and regions in https://cloud.google.com/vertex-ai/docs/predictions/configure-compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81693d-b975-4c4d-b648-05d2e670a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\"] {isTemplate: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c626256-b62c-4063-ae65-ed110c5d5738",
   "metadata": {},
   "source": [
    "Choose a machine type. Example with NVIDIA_L4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180654c-9736-48ee-98ad-c68515cc98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"2b\" in MODEL_ID:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        # Sets 1 L4 (24G) to deploy Gemma 2 2B models.\n",
    "        machine_type = \"g2-standard-12\"\n",
    "        accelerator_count = 1\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Recommended machine settings not found for accelerator type: %s\"\n",
    "            % accelerator_type\n",
    "        )\n",
    "elif \"9b\" in MODEL_ID:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        # Sets 2 L4 (24G) to deploy Gemma 2 9B models.\n",
    "        machine_type = \"g2-standard-24\"\n",
    "        accelerator_count = 2\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Recommended machine settings not found for accelerator type: %s\"\n",
    "            % accelerator_type\n",
    "        )\n",
    "elif \"27b\" in MODEL_ID:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        # Sets 4 L4 (24G) to deploy Gemma 2 27B models.\n",
    "        machine_type = \"g2-standard-48\"\n",
    "        accelerator_count = 4\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Recommended machine settings not found for accelerator type: %s\"\n",
    "            % accelerator_type\n",
    "        )\n",
    "else:\n",
    "    raise ValueError(\"Recommended machine settings not found for model: %s\" % MODEL_ID)\n",
    "\n",
    "check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f4753-5a30-443e-aea8-b953c3591374",
   "metadata": {},
   "source": [
    "Deployment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48303ad8-9889-4453-a432-8222f33c01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_tgi(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_input_length: int = 2047,\n",
    "    max_total_tokens: int = 2048,\n",
    "    max_batch_prefill_tokens: int = 2048,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with TGI on GPU in Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name}-endpoint\",\n",
    "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "    )\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"NUM_SHARD\": f\"{accelerator_count}\",\n",
    "        \"MAX_INPUT_LENGTH\": f\"{max_input_length}\",\n",
    "        \"MAX_TOTAL_TOKENS\": f\"{max_total_tokens}\",\n",
    "        \"MAX_BATCH_PREFILL_TOKENS\": f\"{max_batch_prefill_tokens}\",\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=TGI_DOCKER_URI,\n",
    "        serving_container_ports=[8080],\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d904a8-5d16-435e-bad7-ff1ff4245fb0",
   "metadata": {},
   "source": [
    "Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2289e1-8249-4dea-8ee1-97042aae588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_name = f\"{MODEL_ID}-{now}\".replace(\"_\", \"-\")\n",
    "\n",
    "models[\"tgi\"], endpoints[\"tgi\"] = deploy_model_tgi(\n",
    "    model_name=model_name,\n",
    "    model_id=model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc61b6-3967-4558-9cd1-febd9934b025",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcacc6f-13f6-45fc-9281-fa2caf0802ec",
   "metadata": {},
   "source": [
    "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
    "\n",
    "Here we use an example from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) to show the finetuning outcome:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e29ea-36c4-47c4-beb6-0efb3290c530",
   "metadata": {},
   "source": [
    "```\n",
    "Prompt: How would the Future of AI in 10 Years look?\n",
    "Output: Predicting the future is always a challenging task, but here are some possible ways that AI could evolve over the next 10 years: Continued advancements in deep learning: Deep learning has been one of the main drivers of recent AI breakthroughs, and we can expect continued advancements in this area. This may include improvements to existing algorithms, as well as the development of new architectures that are better suited to specific types of data and tasks. Increased use of AI in healthcare: AI has the potential to revolutionize healthcare, by improving the accuracy of diagnoses, developing new treatments, and personalizing patient care. We can expect to see continued investment in this area, with more healthcare providers and researchers using AI to improve patient outcomes. Greater automation in the workplace: Automation is already transforming many industries, and AI is likely to play an increasingly important role in this process. We can expect to see more jobs being automated, as well as the development of new types of jobs that require a combination of human and machine skills. More natural and intuitive interactions with technology: As AI becomes more advanced, we can expect to see more natural and intuitive ways of interacting with technology. This may include voice and gesture recognition, as well as more sophisticated chatbots and virtual assistants. Increased focus on ethical considerations: As AI becomes more powerful, there will be a growing need to consider its ethical implications. This may include issues such as bias in AI algorithms, the impact of automation on employment, and the use of AI in surveillance and policing. Overall, the future of AI in 10 years is likely to be shaped by a combination of technological advancements, societal changes, and ethical considerations. While there are many exciting possibilities for AI in the future, it will be important to carefully consider its potential impact on society and to work towards ensuring that its benefits are shared fairly and equitably.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ce982-d9af-466a-ad9d-ff7b21d520b0",
   "metadata": {},
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa822e-b38d-4aa6-9002-dc10a222e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How would the Future of AI in 10 Years look?\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5884f4e-6439-4da1-bdbb-5695482ba021",
   "metadata": {},
   "source": [
    "Generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b575940-0fe7-4ad4-afd2-51e545ac221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 128  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "top_p = 0.9  # @param {type:\"number\"}\n",
    "top_k = 1  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a66bb-c54a-4487-98b8-46ee8277bdb1",
   "metadata": {},
   "source": [
    "Note: If you encounter the issue like ServiceUnavailable: 503 Took too long to respond when processing, you can reduce the maximum number of output tokens, such as set max_tokens as 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd92fd-b0e1-46e1-b816-18c978968b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overrides max_tokens and top_k parameters during inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"inputs\": f\"### Human: {prompt}### Assistant: \",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k,\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e513d-260c-44f8-b24e-f260d419c6c9",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b8908-beb6-4d94-879c-6033e2e69993",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoints[\"tgi\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c0b3b-81ac-48e9-9d02-16863c782bbc",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beafe57-485a-4d67-a032-48eaf3c54a7e",
   "metadata": {},
   "source": [
    "Delete the experiment models and endpoints to recycle the resources and avoid unnecessary continuous charges that may incur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b04c35-da1f-4ebc-a688-46bd5450796f",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_resources = False # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc857dcb-7ff0-477e-83ad-146d5f842269",
   "metadata": {},
   "outputs": [],
   "source": [
    "if delete_resources:\n",
    "    # Undeploy model and delete endpoint.\n",
    "    for endpoint in endpoints.values():\n",
    "        endpoint.delete(force=True)\n",
    "    \n",
    "    # Delete models.\n",
    "    for model in models.values():\n",
    "        model.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22f566b-0bf5-4fa5-bad3-1bca1b98549a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
