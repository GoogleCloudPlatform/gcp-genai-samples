{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ddc4905-50af-4b72-a030-497a8fb6b2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d5b55-9d39-44c5-bb6a-07ced136e73b",
   "metadata": {},
   "source": [
    "# Gemma2 Deployment on Vertex AI with Hex-LLM and TGI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12621c-44d5-4d9c-9858-f22ef002539e",
   "metadata": {},
   "source": [
    "Based on [model_garden_gemma2_deployment_on_vertex.ipynb](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/community/model_garden/model_garden_gemma2_deployment_on_vertex.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ed964-587a-4542-b14d-ff487a7bc42e",
   "metadata": {},
   "source": [
    "![gemma2 deployment architecture](images/1.2-gemma2-deployment.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a02d7e-a1d5-4b0f-afca-514d7c99810a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook demonstrates deploying Gemma 2 models\n",
    " * on TPU using **Hex-LLM**, a **H**igh-**E**fficiency **L**arge **L**anguage **M**odel serving solution built with **XLA** that is being developed by Google Cloud, and\n",
    " * on GPU using **TGI** ([text-generation-inference](https://github.com/huggingface/text-generation-inference)), the state-of-the-art open source LLM serving solution on GPU.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Deploy Gemma 2 with Hex-LLM on TPU\n",
    "- Deploy Gemma with [TGI](https://github.com/huggingface/text-generation-inference) on GPU\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55678d89-a4e4-49b4-a702-dd3deb8557d2",
   "metadata": {},
   "source": [
    "## Before you begin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96326f1-ba96-407c-a186-492565503c50",
   "metadata": {},
   "source": [
    "By default, the quota for TPU deployment `Custom model serving TPU v5e cores per region` is 4. TPU quota is only available in `us-west1`. You can request for higher TPU quota following the instructions at [\"Request a higher quota\"](https://cloud.google.com/docs/quota/view-manage#requesting_higher_quota)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d322cb3-1631-4bb8-a013-48856eaf3a2b",
   "metadata": {},
   "source": [
    "### Setup Google Cloud project "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c89be7e-ee1b-4fee-9cdd-73f03f34bb69",
   "metadata": {},
   "source": [
    "[Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83920bab-dca9-4898-88b8-4a27c1793bca",
   "metadata": {},
   "source": [
    "**[Optional]** [Create a Cloud Storage bucket](https://cloud.google.com/storage/docs/creating-buckets) for storing experiment outputs. Set the BUCKET_URI for the experiment environment. The specified Cloud Storage bucket (`BUCKET_URI`) should be located in the same region as where the notebook was launched. Note that a multi-region bucket (eg. \"us\") is not considered a match for a single region covered by the multi-region range (eg. \"us-central1\"). If not set, a unique GCS bucket will be created instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa0055-e50f-4a96-b3c5-62010808f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4357f32-bbfe-4be6-b195-c1904c7686cf",
   "metadata": {},
   "source": [
    "**[Optional]** Set region. If not set, the region will be set automatically according to Colab Enterprise environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36eb642-486e-47a5-be26-06aadb3a416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4483c4dd-c1fe-43f9-aa4e-3dbc45369838",
   "metadata": {},
   "source": [
    "Upgrade Vertex AI SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea465408-2e6a-4846-97a9-d8654cca8f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --quiet 'google-cloud-aiplatform>=1.64.0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4fc318-c476-44e2-8342-696242f0ac70",
   "metadata": {},
   "source": [
    "Clone samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6f169-9863-462f-b6a4-4969a31cb498",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/GoogleCloudPlatform/vertex-ai-samples.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71721b23-39b9-4fc6-a2fa-f063b729967c",
   "metadata": {},
   "source": [
    "## Access Gemma 2 Models\n",
    "You must provide a Hugging Face User Access Token (read) to access the Gemma 2 models. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37371b98-ee02-407e-85da-1882340279e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_TOKEN = \"\"  # @param {type:\"string\", isTemplate:true}\n",
    "assert (\n",
    "    HF_TOKEN\n",
    "), \"Provide a read HF_TOKEN to load models from Hugging Face, or select a different model source.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9805c5fa-20af-4d67-b618-5d8ac81b5b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_prefix = \"google/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaea7038-ab18-487f-aa84-72bc00d74a2f",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be62cd6b-c4ad-4a64-abf5-0dd786c6f2bd",
   "metadata": {},
   "source": [
    "Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d2cca8-0fb8-4e10-a589-7070fdb82094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import importlib\n",
    "import os\n",
    "import uuid\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "common_util = importlib.import_module(\n",
    "    \"vertex-ai-samples.community-content.vertex_model_garden.model_oss.notebook_util.common_util\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f67fbfa-d06d-4014-8927-bc4e42e3de72",
   "metadata": {},
   "source": [
    "Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30ca63f-93ac-4b1c-8e35-07199e03c87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the default cloud project id.\n",
    "PROJECT_ID = os.environ[\"GOOGLE_CLOUD_PROJECT\"]\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "if not REGION:\n",
    "    REGION = os.environ[\"GOOGLE_CLOUD_REGION\"]\n",
    "\n",
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}-{str(uuid.uuid4())[:4]}\"\n",
    "    BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"temporal\")\n",
    "MODEL_BUCKET = os.path.join(BUCKET_URI, \"gemma2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ae73c7-a341-48c3-b14a-39ffa0bee1cc",
   "metadata": {},
   "source": [
    "Gets the default Service Account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a437d59-066c-4587-9bff-13c70a589f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0827f-f68e-4f64-8158-0d6308c7273d",
   "metadata": {},
   "source": [
    "Provision permissions to the SERVICE_ACCOUNT with the GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2922d3-71ec-4da3-987b-60f64c4f99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/storage.admin\"\n",
    "! gcloud projects add-iam-policy-binding --no-user-output-enabled {PROJECT_ID} --member=serviceAccount:{SERVICE_ACCOUNT} --role=\"roles/aiplatform.user\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58c6ca9-6b0d-444c-9909-465a816a400c",
   "metadata": {},
   "source": [
    "Initialize Vertex AI API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2acc01-7e13-4341-815d-0e15e63b59d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85c103d-7027-4e3a-a21f-a0d1e4ebe90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models, endpoints = {}, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac453e05-ee49-449a-9d1d-3dcfed4d6138",
   "metadata": {},
   "source": [
    "## Deploy Gemma 2 models with Hex-LLM on TPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31db899e-747e-416b-b69e-8b76f42f2ff6",
   "metadata": {},
   "source": [
    "**Hex-LLM** is a **H**igh-**E**fficiency **L**arge **L**anguage **M**odel (LLM) TPU serving solution built with **XLA**, which is being developed by Google Cloud.\n",
    "\n",
    "Refer to the \"Request for TPU quota\" section for TPU quota."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b9b4cc-36dd-4235-baff-9d134d1a9cd8",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93674646-bda6-41a8-bcf9-7ba5969a73af",
   "metadata": {},
   "source": [
    "The pre-built serving docker images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77ac1b0-59b5-4ced-949c-ef734bd84acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEXLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai-restricted/vertex-vision-model-garden-dockers/hex-llm-serve:gemma2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bb52e3-ff17-4b62-8203-716c96f26f5f",
   "metadata": {},
   "source": [
    "Set the model ID. Model weights can be loaded from HuggingFace or from a GCS bucket. Select one of the four model variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209fa0e-0f24-423b-bdc1-1f50e3c0dea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemma-2-2b-it\"  # @param [\"gemma-2-2b\", \"gemma-2-2b-it\", \"gemma-2-9b\", \"gemma-2-9b-it\", \"gemma-2-27b\", \"gemma-2-27b-it\"] {allow-input: true, isTemplate: true}\n",
    "TPU_DEPLOYMENT_REGION = \"us-west1\"  # @param [\"us-west1\"] {isTemplate:true}\n",
    "model_id = os.path.join(model_path_prefix, MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b93038-be84-4acb-ae98-25d18fca27e8",
   "metadata": {},
   "source": [
    "Choose a machine type. You can find Vertex AI prediction TPUv5e machine types in https://cloud.google.com/vertex-ai/docs/predictions/use-tpu#deploy_a_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a165151-51e5-4de0-80ad-6341b438540c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"2b\" in model_id:\n",
    "    # Sets ct5lp-hightpu-1t (1 TPU chip) to deploy Gemma 2 2B models.\n",
    "    machine_type = \"ct5lp-hightpu-1t\"\n",
    "    accelerator_type = \"TPU_V5e\"\n",
    "    # Note: 1 TPU V5 chip has only one core.\n",
    "    accelerator_count = 1\n",
    "elif \"9b\" in model_id:\n",
    "    # Sets ct5lp-hightpu-4t (4 TPU chips) to deploy Gemma 2 9B models.\n",
    "    machine_type = \"ct5lp-hightpu-4t\"\n",
    "    accelerator_type = \"TPU_V5e\"\n",
    "    # Note: 1 TPU V5 chip has only one core.\n",
    "    accelerator_count = 4\n",
    "else:\n",
    "    # Sets ct5lp-hightpu-8t (8 TPU chips) to deploy Gemma 2 27B models.\n",
    "    machine_type = \"ct5lp-hightpu-8t\"\n",
    "    accelerator_type = \"TPU_V5e\"\n",
    "    # Note: 1 TPU V5 chip has only one core.\n",
    "    accelerator_count = 8\n",
    "\n",
    "common_util.check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=TPU_DEPLOYMENT_REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee67aef-0425-4845-983e-4769763f4feb",
   "metadata": {},
   "source": [
    "Server parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9b62c2-c304-41cc-ab70-4a5c081a4bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_parallel_size = accelerator_count\n",
    "hbm_utilization_factor = 0.6  # Fraction of HBM memory allocated for KV cache after model loading. A larger value improves throughput but gives higher risk of TPU out-of-memory errors with long prompts.\n",
    "max_running_seqs = 256  # Maximum number of running sequences in a continuous batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9380113-9142-4227-a212-aef1174f0ff6",
   "metadata": {},
   "source": [
    "Set use_dedicated_endpoint to False if you don't want to use [dedicated endpoint](https://cloud.google.com/vertex-ai/docs/general/deployment#create-dedicated-endpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de220ba-59be-4b85-8384-2e08b1eec74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dedicated_endpoint = True  # @param {type:\"boolean\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090e096e-7018-447f-84d4-6a0747b1eb64",
   "metadata": {},
   "source": [
    "Endpoint configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ea031-e5c2-4ed0-8618-2f7109f31037",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_replica_count = 1\n",
    "max_replica_count = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e00376-029e-4c59-a881-2e233eed0e15",
   "metadata": {},
   "source": [
    "Deployment function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c330692-3e77-43e9-9e6c-b1ccf4254a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_hexllm(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    base_model_id: str = None,\n",
    "    tensor_parallel_size: int = 1,\n",
    "    machine_type: str = \"ct5lp-hightpu-1t\",\n",
    "    tpu_topology: str = \"1x1\",\n",
    "    hbm_utilization_factor: float = 0.6,\n",
    "    max_running_seqs: int = 256,\n",
    "    max_model_len: int = 4096,\n",
    "    endpoint_id: str = \"\",\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with Hex-LLM on TPU in Vertex AI.\"\"\"\n",
    "    if endpoint_id:\n",
    "        aip_endpoint_name = (\n",
    "            f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_id}\"\n",
    "        )\n",
    "        endpoint = aiplatform.Endpoint(aip_endpoint_name)\n",
    "    else:\n",
    "        endpoint = aiplatform.Endpoint.create(\n",
    "            display_name=f\"{model_name}-endpoint\",\n",
    "            location=TPU_DEPLOYMENT_REGION,\n",
    "            dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "        )\n",
    "\n",
    "    if not base_model_id:\n",
    "        base_model_id = model_id\n",
    "\n",
    "    if not tensor_parallel_size:\n",
    "        tensor_parallel_size = int(machine_type[-2])\n",
    "\n",
    "    num_hosts = int(tpu_topology.split(\"x\")[0])\n",
    "\n",
    "    hexllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor_parallel_size={tensor_parallel_size}\",\n",
    "        f\"--num_hosts={num_hosts}\",\n",
    "        f\"--hbm_utilization_factor={hbm_utilization_factor}\",\n",
    "        f\"--max_running_seqs={max_running_seqs}\",\n",
    "        f\"--max_model_len={max_model_len}\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": base_model_id,\n",
    "        \"HEX_LLM_LOG_LEVEL\": \"info\",\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars.update({\"HF_TOKEN\": HF_TOKEN})\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=HEXLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"hex_llm.server.api_server\"],\n",
    "        serving_container_args=hexllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "        serving_container_deployment_timeout=7200,\n",
    "        location=TPU_DEPLOYMENT_REGION,\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        tpu_topology=tpu_topology if num_hosts > 1 else None,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6646aba7-0214-4266-aaf7-413fcc9276b9",
   "metadata": {},
   "source": [
    "Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49f5996-9031-42ce-8cde-70a3a732f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"hexllm_tpu\"], endpoints[\"hexllm_tpu\"] = deploy_model_hexllm(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=MODEL_ID),\n",
    "    model_id=model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    tensor_parallel_size=tensor_parallel_size,\n",
    "    hbm_utilization_factor=hbm_utilization_factor,\n",
    "    max_running_seqs=max_running_seqs,\n",
    "    min_replica_count=min_replica_count,\n",
    "    max_replica_count=max_replica_count,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8328d89d-18af-4546-9c9f-51d15c3b0418",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1165f-f7d0-4145-b31a-86c60c6288ba",
   "metadata": {},
   "source": [
    "Once deployment succeeds, you can send requests to the endpoint with text prompts based on your `template`. Note that the first few prompts will take longer to execute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a86c5-2be4-48bf-b12f-f6c7ff532acd",
   "metadata": {},
   "source": [
    "**[Optional]** Loads an existing endpoint instance using the endpoint name:\n",
    "- Using `endpoint_name = endpoint.name` allows us to get the endpoint name of the endpoint `endpoint` created in the cell above.\n",
    "- Alternatively, you can set `endpoint_name = \"1234567890123456789\"` to load an existing endpoint with the ID 1234567890123456789."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ac2c6d-3d7d-4ad0-b223-e3833efa7f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may uncomment the code below to load an existing endpoint:\n",
    "# endpoint_name = \"\"  # @param {type:\"string\"}\n",
    "# aip_endpoint_name = (\n",
    "#     f\"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{endpoint_name}\"\n",
    "# )\n",
    "# endpoints[\"hexllm_tpu\"] = aiplatform.Endpoint(aip_endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462ede3c-4bd7-492f-8260-08bb6a4c19b0",
   "metadata": {},
   "source": [
    "**[Optional]** Additionally, you can moderate the generated text with Vertex AI. See [Moderate text documentation](https://cloud.google.com/natural-language/docs/moderating-text) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12ec86-21cf-4fa9-9b0f-95943676f74d",
   "metadata": {},
   "source": [
    "Here we use an example:\n",
    "```\n",
    "Prompt: What is a car?\n",
    "Output: A car is a four-wheeled vehicle designed for the transportation of passengers and their belongings.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2be3cb-979d-41c6-b5c2-d1d9e6a92855",
   "metadata": {},
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd48ca8-2214-44e9-a548-4bd675009e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is a car?\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1989cb4b-2bd9-4cec-b5aa-11474f7a0702",
   "metadata": {},
   "source": [
    "Generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7de6da-0d27-44a6-9d64-e1caaca3d1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 50  # @param {type: \"integer\"}\n",
    "temperature = 1.0  # @param {type: \"number\"}\n",
    "top_p = 1.0  # @param {type: \"number\"}\n",
    "top_k = 1  # @param {type: \"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759dfae0-7832-4ddb-a8c5-d2bc2390bfe8",
   "metadata": {},
   "source": [
    "Note: If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`, you can reduce the maximum number of output tokens, such as set `max_tokens` as 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5fd67e-2435-4b54-b5dd-7e1a70de0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overrides parameters for inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efda6ca-6a0b-4b77-9ee0-e6a5cd29d5d9",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f3216-0c1c-4390-9621-ebca1cefb4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoints[\"hexllm_tpu\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29c335a-0d70-4f99-b5a9-ff81c125fc73",
   "metadata": {},
   "source": [
    "## Deploy Gemma models with TGI on GPU\n",
    "\n",
    "[TGI](https://github.com/huggingface/text-generation-inference) stands for Text Generation Inference. It's a powerful library designed specifically for running large language models on GPUs efficiently. TGI utilizes techniques like \"paged attention\" and \"continuous batching\" to improve the speed and throughput of LLMs.\n",
    "\n",
    "Currently, only L4 GPUs are demonstrated in this notebook. Functionality on other GPU types will be confirmed and added in the future.\n",
    "\n",
    "Gemma2 2B, 9B and 27B models require at least 1, 2, and 4 L4 GPUs respectively for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd37540-559c-401b-ac16-31e3954a06f2",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce98d317-88ed-4817-bd1b-18ff51e2a2fc",
   "metadata": {},
   "source": [
    "The pre-built serving docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3683245-e015-4a5c-a273-a863d9a3f951",
   "metadata": {},
   "outputs": [],
   "source": [
    "TGI_DOCKER_URI = \"us-docker.pkg.dev/deeplearning-platform-release/gcr.io/huggingface-text-generation-inference-cu121.2-1.ubuntu2204.py310\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0825834-a383-44cd-8440-0fbd1daa72f8",
   "metadata": {},
   "source": [
    "Set the model ID. Model weights can be loaded from HuggingFace or from a GCS bucket. Select one of the four model variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306f064a-70dd-44fc-9d5f-a2cc30961919",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemma-2-2b\"  # @param [\"gemma-2-2b\", \"gemma-2-2b-it\", \"gemma-2-9b\", \"gemma-2-9b-it\", \"gemma-2-27b\", \"gemma-2-27b-it\"] {allow-input: true, isTemplate: true}\n",
    "model_id = os.path.join(model_path_prefix, MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f404cb-622a-4641-8a9c-80ccffb84ceb",
   "metadata": {},
   "source": [
    "Choose a GPU.\n",
    "You can find Vertex AI prediction supported accelerators and regions in https://cloud.google.com/vertex-ai/docs/predictions/configure-compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a81693d-b975-4c4d-b648-05d2e670a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\"] {isTemplate: true}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c626256-b62c-4063-ae65-ed110c5d5738",
   "metadata": {},
   "source": [
    "Choose a machine type. Example with NVIDIA_L4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9180654c-9736-48ee-98ad-c68515cc98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"2b\" in MODEL_ID:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        # Sets 1 L4 (24G) to deploy Gemma 2 2B models.\n",
    "        machine_type = \"g2-standard-12\"\n",
    "        accelerator_count = 1\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Recommended machine settings not found for accelerator type: %s\"\n",
    "            % accelerator_type\n",
    "        )\n",
    "elif \"9b\" in MODEL_ID:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        # Sets 2 L4 (24G) to deploy Gemma 2 9B models.\n",
    "        machine_type = \"g2-standard-24\"\n",
    "        accelerator_count = 2\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Recommended machine settings not found for accelerator type: %s\"\n",
    "            % accelerator_type\n",
    "        )\n",
    "elif \"27b\" in MODEL_ID:\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        # Sets 4 L4 (24G) to deploy Gemma 2 27B models.\n",
    "        machine_type = \"g2-standard-48\"\n",
    "        accelerator_count = 4\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Recommended machine settings not found for accelerator type: %s\"\n",
    "            % accelerator_type\n",
    "        )\n",
    "else:\n",
    "    raise ValueError(\"Recommended machine settings not found for model: %s\" % MODEL_ID)\n",
    "\n",
    "common_util.check_quota(\n",
    "    project_id=PROJECT_ID,\n",
    "    region=REGION,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    is_for_training=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8f4753-5a30-443e-aea8-b953c3591374",
   "metadata": {},
   "source": [
    "Deployment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48303ad8-9889-4453-a432-8222f33c01b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model_tgi(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_input_length: int = 2047,\n",
    "    max_total_tokens: int = 2048,\n",
    "    max_batch_prefill_tokens: int = 2048,\n",
    "    use_dedicated_endpoint: bool = False,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys models with TGI on GPU in Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(\n",
    "        display_name=f\"{model_name}-endpoint\",\n",
    "        dedicated_endpoint_enabled=use_dedicated_endpoint,\n",
    "    )\n",
    "\n",
    "    env_vars = {\n",
    "        \"MODEL_ID\": model_id,\n",
    "        \"NUM_SHARD\": f\"{accelerator_count}\",\n",
    "        \"MAX_INPUT_LENGTH\": f\"{max_input_length}\",\n",
    "        \"MAX_TOTAL_TOKENS\": f\"{max_total_tokens}\",\n",
    "        \"MAX_BATCH_PREFILL_TOKENS\": f\"{max_batch_prefill_tokens}\",\n",
    "        \"DEPLOY_SOURCE\": \"notebook\",\n",
    "    }\n",
    "\n",
    "    # HF_TOKEN is not a compulsory field and may not be defined.\n",
    "    try:\n",
    "        if HF_TOKEN:\n",
    "            env_vars[\"HF_TOKEN\"] = HF_TOKEN\n",
    "    except NameError:\n",
    "        pass\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=TGI_DOCKER_URI,\n",
    "        serving_container_ports=[8080],\n",
    "        serving_container_environment_variables=env_vars,\n",
    "        serving_container_shared_memory_size_mb=(16 * 1024),  # 16 GB\n",
    "    )\n",
    "\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d904a8-5d16-435e-bad7-ff1ff4245fb0",
   "metadata": {},
   "source": [
    "Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2289e1-8249-4dea-8ee1-97042aae588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "models[\"tgi\"], endpoints[\"tgi\"] = deploy_model_tgi(\n",
    "    model_name=common_util.get_job_name_with_datetime(prefix=MODEL_ID),\n",
    "    model_id=model_id,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_input_length=max_input_length,\n",
    "    max_total_tokens=max_total_tokens,\n",
    "    max_batch_prefill_tokens=max_batch_prefill_tokens,\n",
    "    use_dedicated_endpoint=use_dedicated_endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddc61b6-3967-4558-9cd1-febd9934b025",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcacc6f-13f6-45fc-9281-fa2caf0802ec",
   "metadata": {},
   "source": [
    "Once deployment succeeds, you can send requests to the endpoint with text prompts.\n",
    "\n",
    "Here we use an example from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) to show the finetuning outcome:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15e29ea-36c4-47c4-beb6-0efb3290c530",
   "metadata": {},
   "source": [
    "```\n",
    "Prompt: How would the Future of AI in 10 Years look?\n",
    "Output: Predicting the future is always a challenging task, but here are some possible ways that AI could evolve over the next 10 years: Continued advancements in deep learning: Deep learning has been one of the main drivers of recent AI breakthroughs, and we can expect continued advancements in this area. This may include improvements to existing algorithms, as well as the development of new architectures that are better suited to specific types of data and tasks. Increased use of AI in healthcare: AI has the potential to revolutionize healthcare, by improving the accuracy of diagnoses, developing new treatments, and personalizing patient care. We can expect to see continued investment in this area, with more healthcare providers and researchers using AI to improve patient outcomes. Greater automation in the workplace: Automation is already transforming many industries, and AI is likely to play an increasingly important role in this process. We can expect to see more jobs being automated, as well as the development of new types of jobs that require a combination of human and machine skills. More natural and intuitive interactions with technology: As AI becomes more advanced, we can expect to see more natural and intuitive ways of interacting with technology. This may include voice and gesture recognition, as well as more sophisticated chatbots and virtual assistants. Increased focus on ethical considerations: As AI becomes more powerful, there will be a growing need to consider its ethical implications. This may include issues such as bias in AI algorithms, the impact of automation on employment, and the use of AI in surveillance and policing. Overall, the future of AI in 10 years is likely to be shaped by a combination of technological advancements, societal changes, and ethical considerations. While there are many exciting possibilities for AI in the future, it will be important to carefully consider its potential impact on society and to work towards ensuring that its benefits are shared fairly and equitably.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ce982-d9af-466a-ad9d-ff7b21d520b0",
   "metadata": {},
   "source": [
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7aa822e-b38d-4aa6-9002-dc10a222e7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How would the Future of AI in 10 Years look?\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5884f4e-6439-4da1-bdbb-5695482ba021",
   "metadata": {},
   "source": [
    "Generation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b575940-0fe7-4ad4-afd2-51e545ac221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_tokens = 128  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "top_p = 0.9  # @param {type:\"number\"}\n",
    "top_k = 1  # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98a66bb-c54a-4487-98b8-46ee8277bdb1",
   "metadata": {},
   "source": [
    "Note: If you encounter the issue like ServiceUnavailable: 503 Took too long to respond when processing, you can reduce the maximum number of output tokens, such as set max_tokens as 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd92fd-b0e1-46e1-b816-18c978968b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overrides max_tokens and top_k parameters during inferences.\n",
    "instances = [\n",
    "    {\n",
    "        \"inputs\": f\"### Human: {prompt}### Assistant: \",\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k,\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2e513d-260c-44f8-b24e-f260d419c6c9",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b8908-beb6-4d94-879c-6033e2e69993",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = endpoints[\"tgi\"].predict(\n",
    "    instances=instances, use_dedicated_endpoint=use_dedicated_endpoint\n",
    ")\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "642c0b3b-81ac-48e9-9d02-16863c782bbc",
   "metadata": {},
   "source": [
    "## Clean up resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beafe57-485a-4d67-a032-48eaf3c54a7e",
   "metadata": {},
   "source": [
    "Delete the experiment models and endpoints to recycle the resources and avoid unnecessary continuous charges that may incur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc857dcb-7ff0-477e-83ad-146d5f842269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Undeploy model and delete endpoint.\n",
    "for endpoint in endpoints.values():\n",
    "    endpoint.delete(force=True)\n",
    "\n",
    "# Delete models.\n",
    "for model in models.values():\n",
    "    model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8a1360-70a8-4663-9ced-294e39113656",
   "metadata": {},
   "source": [
    "To delete the bucket, change the variable to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49368a23-854f-4fdb-a887-da7e474e7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_bucket = False  # @param {type:\"boolean\"}\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
