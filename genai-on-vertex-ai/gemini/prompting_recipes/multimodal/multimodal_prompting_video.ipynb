{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hkZw98BK0AKv"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bb894cc4c54"
      },
      "source": [
        "# Multimodal Prompting with Gemini 1.5: Working with Videos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGhNH-y9z5EZ"
      },
      "source": [
        "<table align=\"left\">\n",
        "<td style=\"text-align: center\">\n",
        "<a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video.ipynb\">\n",
        "<img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "</a>\n",
        "</td>\n",
        "      <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fmain%2Fgenai-on-vertex-ai%2Fgemini%2Fprompting_recipes%2Fmultimodal%2Fmultimodal_prompting_video.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
        "    </a>\n",
        "  </td>\n",
        "<td style=\"text-align: center\">\n",
        "<a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/applied-ai-engineering-samples/main/genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video.ipynb\">\n",
        "<img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "</a>\n",
        "</td>    \n",
        "<td style=\"text-align: center\">\n",
        "<a href=\"https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/gemini/prompting_recipes/multimodal/multimodal_prompting_video.ipynb\">\n",
        "<img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "</a>\n",
        "</td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqS4jWxr0Eyz"
      },
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Michael Chertushkin](https://github.com/misha-chertushkin) |\n",
        "| Reviewer(s) | [Rajesh Thallam](https://github.com/rthallam), [Skander Hannachi](https://github.com/skanderhn)  |\n",
        "| Last updated | 2024-09-16 |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RS2kzIzdp-u"
      },
      "source": [
        "# Overview\n",
        "\n",
        "---\n",
        "\n",
        "Gemini 1.5 Pro and Flash models supports adding image, audio, video, and PDF files in text or chat prompts for a text or code response. Gemini 1.5 Pro supports up to 2 Million input tokens with up to 2 hours length of video per prompt. Gemini can analyze the audio embedded within a video as well. You can add videos to Gemini requests to perform [video analysis tasks](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/video-understanding) such as video summarization, video chapterization (or localization), key event detection, scene analysis, captioning and transcription and more. \n",
        "\n",
        "---\n",
        "\n",
        "In this notebook we cover prompting recipes and strategies for working with Gemini on videos and show some examples on the way. This notebook is organized as follows:\n",
        "\n",
        "- Video Understanding\n",
        "- Key event detection\n",
        "- Using System instruction\n",
        "- Analyzing videos with step-by-step reasoning\n",
        "- Generating structured output\n",
        "- Using context caching for repeated queries\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acd63312c2f4"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "The following steps are necessary to run this notebook, no matter what notebook environment you're using.\n",
        "\n",
        "If you're entirely new to Google Cloud, [get started here](https://cloud.google.com/docs/get-started)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13e6fde93ea3"
      },
      "source": [
        "## Google Cloud Project Setup\n",
        "\n",
        "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager). When you first create an account, you get a $300 free credit towards your compute/storage costs.\n",
        "1. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
        "1. [Enable the Service Usage API](https://console.cloud.google.com/apis/library/serviceusage.googleapis.com)\n",
        "1. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "1. [Enable the Cloud Storage API](https://console.cloud.google.com/flows/enableapi?apiid=storage.googleapis.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9b5ae4999b9"
      },
      "source": [
        "## Google Cloud Permissions\n",
        "\n",
        "**To run the complete Notebook, including the optional section, you will need to have the [Owner role](https://cloud.google.com/iam/docs/understanding-roles) for your project.**\n",
        "\n",
        "If you want to skip the optional section, you need at least the following [roles](https://cloud.google.com/iam/docs/granting-changing-revoking-access):\n",
        "* **`roles/serviceusage.serviceUsageAdmin`** to enable APIs\n",
        "* **`roles/iam.serviceAccountAdmin`** to modify service agent permissions\n",
        "* **`roles/aiplatform.user`** to use AI Platform components\n",
        "* **`roles/storage.objectAdmin`** to modify and delete GCS buckets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b203ddf1cdc"
      },
      "source": [
        "## Install Vertex AI SDK for Python and other dependencies (If Needed)\n",
        "\n",
        "The list `packages` contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user.## Install Vertex AI SDK for Python and other dependencies (If Needed)\n",
        "\n",
        "The list `packages` contains tuples of package import names and install names. If the import name is not found then the install name is used to install quitely for the current user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "514241a24fa4"
      },
      "outputs": [],
      "source": [
        "! pip install google-cloud-aiplatform --upgrade --quiet --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b187dc025e0"
      },
      "source": [
        "## Restart Runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b08062f2883"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6791e371ace9"
      },
      "source": [
        "## Authenticate\n",
        "\n",
        "If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud [project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects).\n",
        "\n",
        "If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into [Application Default Credentials for your local environment](https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev) and [initializing the Google Cloud CLI](https://cloud.google.com/docs/authentication/gcloud). In many cases, running `gcloud auth application-default login` in a shell on the machine running the notebook kernel is sufficient.\n",
        "\n",
        "More authentication options are discussed [here](https://cloud.google.com/docs/authentication)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "51cabca59af0"
      },
      "outputs": [],
      "source": [
        "# Colab authentication.\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()\n",
        "    print(\"Authenticated\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9e68b09a55c"
      },
      "source": [
        "## Set Google Cloud project information and Initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "Make sure to change `PROJECT_ID` in the next cell. You can leave the values for `REGION` unless you have a specific reason to change them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5256307afcd5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vertex AI SDK initialized.\n",
            "Vertex AI SDK version = 1.65.0\n"
          ]
        }
      ],
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "print(\"Vertex AI SDK initialized.\")\n",
        "print(f\"Vertex AI SDK version = {vertexai.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89c6c77513de"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2042cf8dce9f"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import (GenerationConfig, GenerativeModel,\n",
        "                                        HarmBlockThreshold, HarmCategory, Part)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9e80e805ceb"
      },
      "source": [
        "## Define Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c36297c5650f"
      },
      "outputs": [],
      "source": [
        "import http.client\n",
        "import textwrap\n",
        "import typing\n",
        "import urllib.request\n",
        "\n",
        "from google.cloud import storage\n",
        "from IPython import display\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "\n",
        "\n",
        "def wrap(string, max_width=80):\n",
        "    return textwrap.fill(string, max_width)\n",
        "\n",
        "\n",
        "def get_bytes_from_url(url: str) -> bytes:\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        response = typing.cast(http.client.HTTPResponse, response)\n",
        "        bytes = response.read()\n",
        "    return bytes\n",
        "\n",
        "\n",
        "def get_bytes_from_gcs(gcs_path: str):\n",
        "    bucket_name = gcs_path.split(\"/\")[2]\n",
        "    object_prefix = \"/\".join(gcs_path.split(\"/\")[3:])\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.get_bucket(bucket_name)\n",
        "    blob = bucket.get_blob(object_prefix)\n",
        "    return blob.download_as_bytes()\n",
        "\n",
        "\n",
        "def display_image(image_url: str, width: int = 300, height: int = 200):\n",
        "    if image_url.startswith(\"gs://\"):\n",
        "        image_bytes = get_bytes_from_gcs(image_url)\n",
        "    else:\n",
        "        image_bytes = get_bytes_from_url(image_url)\n",
        "    display.display(display.Image(data=image_bytes, width=width, height=height))\n",
        "\n",
        "\n",
        "def display_video(video_url: str, width: int = 300, height: int = 200):\n",
        "    if video_url.startswith(\"gs://\"):\n",
        "        video_bytes = get_bytes_from_gcs(video_url)\n",
        "    else:\n",
        "        video_bytes = get_bytes_from_url(video_url)\n",
        "    display.display(\n",
        "        display.Video(\n",
        "            data=video_bytes,\n",
        "            width=width,\n",
        "            height=height,\n",
        "            embed=True,\n",
        "            mimetype=\"video/mp4\",\n",
        "        )\n",
        "    )\n",
        "\n",
        "def display_audio(audio_url: str, width: int = 300, height: int = 200):\n",
        "    if audio_url.startswith(\"gs://\"):\n",
        "        audio_bytes = get_bytes_from_gcs(audio_url)\n",
        "    else:\n",
        "        audio_bytes = get_bytes_from_url(audio_url)\n",
        "    display.display(display.Audio(data=audio_bytes, embed=True))\n",
        "\n",
        "\n",
        "def print_prompt(contents: list[str | Part]):\n",
        "    for content in contents:\n",
        "        if isinstance(content, Part):\n",
        "            if content.mime_type.startswith(\"image\"):\n",
        "                display_image(image_url=content.file_data.file_uri)\n",
        "            elif content.mime_type.startswith(\"video\"):\n",
        "                display_video(video_url=content.file_data.file_uri)\n",
        "            elif content.mime_type.startswith(\"audio\"):\n",
        "                display_audio(audio_url=content.file_data.file_uri)\n",
        "            else:\n",
        "                print(content)\n",
        "        else:\n",
        "            print(content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mmDittp23Gp"
      },
      "source": [
        "## Initialize Gemini"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xOwys5I724od"
      },
      "outputs": [],
      "source": [
        "# Gemini Config\n",
        "GENERATION_CONFIG = {\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.95,\n",
        "}\n",
        "\n",
        "SAFETY_CONFIG = {\n",
        "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
        "}\n",
        "\n",
        "gemini_pro = GenerativeModel(model_name=\"gemini-1.5-pro-001\")\n",
        "gemini_flash = GenerativeModel(model_name=\"gemini-1.5-flash-001\")\n",
        "videos_path_prefix = (\n",
        "    \"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/videos\"\n",
        ")\n",
        "\n",
        "\n",
        "def generate(\n",
        "    model,\n",
        "    contents,\n",
        "    safety_settings=SAFETY_CONFIG,\n",
        "    generation_config=GENERATION_CONFIG,\n",
        "    as_markdown=False,\n",
        "):\n",
        "    responses = model.generate_content(\n",
        "        contents=contents,\n",
        "        generation_config=generation_config,\n",
        "        safety_settings=safety_settings,\n",
        "        stream=False,\n",
        "    )\n",
        "    if isinstance(responses, list):\n",
        "        for response in responses:\n",
        "            if as_markdown:\n",
        "                display.display(display.Markdown(response.text))\n",
        "            else:\n",
        "                print(wrap(response.text), end=\"\")\n",
        "    else:\n",
        "        if as_markdown:\n",
        "            display.display(display.Markdown(responses.text))\n",
        "        else:\n",
        "            print(wrap(responses.text), end=\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1ZnbbNo5DHS"
      },
      "outputs": [],
      "source": [
        "display_video(\n",
        "    video_url=\"gs://public-aaie-genai-samples/gemini/prompting_recipes/multimodal/videos/video_1.mp4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8s94ynm1vGt"
      },
      "source": [
        "# Prompt #1. Video Understanding\n",
        "\n",
        "This task requires the input to be presented in two different modalities: text and video. The example of the API call is below, however this is non-optimal prompt and we can make it better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5563489ed4f4"
      },
      "outputs": [],
      "source": [
        "video_path = f\"{videos_path_prefix}/video_1.mp4\"\n",
        "video_content = Part.from_uri(uri=video_path, mime_type=\"video/mp4\")\n",
        "prompt = \"\"\"Provide a description of the video. The description should also \n",
        "contain anything important which people say in the video.\"\"\"\n",
        "\n",
        "contents = [video_content, prompt]\n",
        "# print_prompt(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2bFaqufh5xIN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The video shows a hand holding a pink collapsible cup. The hand opens and closes\n",
            "the cup several times. There is no sound in the video."
          ]
        }
      ],
      "source": [
        "generate(gemini_pro, contents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QJnFXeqAvaT"
      },
      "source": [
        "As we see the model correctly picked what happens there, but it did not provide much details. Let's modify the prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecIw8YDWISQf"
      },
      "source": [
        "### Video Understanding. Advanced Prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MWnDgTHzAtqg"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The video showcases a person playfully tossing and catching a pink collapsible cup against a backdrop of pristine white curtains. \n",
              "\n",
              "**Detailed Breakdown:**\n",
              "\n",
              "* **00:00:** The video begins with the person tossing the cup upwards. The cup is partially collapsed, showcasing its flexibility.\n",
              "* **00:01:** The person catches the cup effortlessly, demonstrating its lightweight and easy-to-handle design.\n",
              "* **00:02 - 00:10:** This sequence repeats the tossing and catching action, emphasizing the cup's portability and fun aspect. The repetitive motion suggests a sense of enjoyment and leisure.\n",
              "\n",
              "**Entities and Relationships:**\n",
              "\n",
              "* **Person:** The video focuses on the hand and arm of a person, suggesting their interaction with the cup.\n",
              "* **Collapsible Cup:** The central object is a bright pink collapsible cup, highlighting its vibrant color and unique feature.\n",
              "* **White Curtains:** The plain white curtains serve as a neutral background, drawing attention solely to the cup and its movement.\n",
              "\n",
              "**Central Theme:**\n",
              "\n",
              "The video aims to showcase the collapsible cup's practicality and playful nature. The bright color, combined with the tossing action, suggests a product designed for an active, on-the-go lifestyle. The white background further emphasizes the cup's aesthetic appeal and versatility. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video \n",
        "and produce the detailed description about what happens on the video.\n",
        "\n",
        "Key Points:\n",
        "- Use timestamps (in MM:SS format) to output key events from the video.\n",
        "- Add information about what happens at each timestamp.\n",
        "- Add information about entities in the video and capture the relationship between them.\n",
        "- Highlight the central theme or focus of the video.\n",
        "\n",
        "Remember:\n",
        "- Try to recover hidden meaning from the scene. For example, some hidden humor \n",
        "  or some hidden context.\n",
        "\"\"\"\n",
        "\n",
        "contents = [video_content, prompt]\n",
        "generate(gemini_pro, contents, as_markdown=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAzxT1LNB-Sj"
      },
      "source": [
        "The response with the updated prompt captures much more details. Although this prompt is rather generic and can be used for other videos, let's add specifics to the prompt. For example, if we want to capture at which time certain event happened."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NACDJqhQIYK3"
      },
      "source": [
        "# Prompt #2. Video Understanding: Key events detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JE5P7Hf-4rhl"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The video showcases a hand playfully tossing and catching a pink collapsible cup against a backdrop of pristine white curtains. \n",
              "\n",
              "Here's a breakdown:\n",
              "\n",
              "- **00:00** The video begins with the hand already in motion, tossing the cup upwards.\n",
              "- **00:01** The hand deftly catches the cup as it descends, momentarily pausing before sending it airborne again.\n",
              "- **00:02** This marks the second throw of the cup, demonstrating the ease with which it can be caught and tossed due to its lightweight and collapsible design.\n",
              "\n",
              "The video's central theme revolves around the portability and fun aspect of the collapsible cup. The simple act of tossing and catching emphasizes its lightweight nature, while the vibrant pink color adds a playful touch. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video \n",
        "and produce the detailed description about what happens on the video.\n",
        "\n",
        "Key Points:\n",
        "- Use timestamps (in MM:SS format) to output key events from the video.\n",
        "- Add information about what happens at each timestamp.\n",
        "- Add information about entities in the video and capture the relationship between them.\n",
        "- Highlight the central theme or focus of the video.\n",
        "\n",
        "Remember:\n",
        "- Try to recover hidden meaning from the scene. For example, some hidden humor \n",
        "  or some hidden context.\n",
        "\n",
        "At which moment the cup was thrown for the second time?\n",
        "\"\"\"\n",
        "\n",
        "contents = [video_content, prompt]\n",
        "generate(gemini_pro, contents, as_markdown=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OxLf3GkLJS6u"
      },
      "source": [
        "# Prompt #3. Video Understanding: Using System instruction\n",
        "\n",
        "System Instruction (SI) is an effective way to steer Gemini's behavior and shape \n",
        "how the model responds to your prompt. SI can be used to describe model behavior \n",
        "such as persona, goal, tasks to perform, output format / tone / style, any constraints etc. \n",
        "\n",
        "SI behaves more \"sticky\" (or consistent) during multi-turn behavior. For example, \n",
        "if you want to achieve a behavior that the model will consistently follow, then \n",
        "system instruction is the best way to put this instruction.\n",
        "\n",
        "In this example, we will move the task rules to system instruction and the \n",
        "question on a specific event in the user prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "qPZurMKjJpqG"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video \n",
        "and produce the detailed description about what happens on the video.\n",
        "\n",
        "Key Points:\n",
        "- Use timestamps (in MM:SS format) to output key events from the video.\n",
        "- Add information about what happens at each timestamp.\n",
        "- Add information about entities in the video and capture the relationship between them.\n",
        "- Highlight the central theme or focus of the video.\n",
        "\n",
        "Remember:\n",
        "- Try to recover hidden meaning from the scene. For example, some hidden humor \n",
        "  or some hidden context.\n",
        "\"\"\"\n",
        "\n",
        "prompt = \"At which moment the cup was thrown for the second time?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2fbb0fd520d1"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The video showcases a hand playfully tossing and catching a collapsible pink cup against a backdrop of pristine white curtains. The cup's flexibility and the hand's dexterity are emphasized throughout the short clip. \n",
              "\n",
              "Here's a breakdown:\n",
              "\n",
              "- **0:00:** The video begins with the hand launching the cup upwards.\n",
              "- **0:01:** The hand deftly catches the cup as it descends.\n",
              "- **0:02:**  The cup is thrown for the second time. The toss is gentle, almost like a light bounce. \n",
              "\n",
              "The video doesn't explicitly convey a deeper narrative or humor. It seems to focus on the simple satisfaction of effortless tossing and catching, highlighting the object's properties. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "gemini_pro_si = GenerativeModel(\n",
        "    model_name=\"gemini-1.5-pro-001\", system_instruction=system_prompt\n",
        ")\n",
        "\n",
        "contents = [video_content, prompt]\n",
        "generate(gemini_pro_si, contents, as_markdown=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzejnMTf7yxb"
      },
      "source": [
        "# Prompt #4. Video Understanding: Step-by-step reasoning\n",
        "\n",
        "We see that actually a mistake happened in analyzing the video. The model does not show all the timestamps where the cup is thrown. Let's fix it with \"step-by-step reasoning\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "iZajqS827x8I"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "The video showcases a person playfully tossing a pink collapsible cup against a white curtain backdrop. The cup's flexibility is evident as it expands and collapses with each toss. \n",
              "\n",
              "Here's a breakdown of the key moments:\n",
              "\n",
              "- **0:00:** The video begins with the person tossing the cup upwards.\n",
              "- **0:01:** The person catches the cup with their right hand.\n",
              "- **0:02:** The cup is thrown again.\n",
              "- **0:03:** The person catches the cup again.\n",
              "\n",
              "The cup is thrown for the second time at the timestamp **0:02**.\n",
              "\n",
              "The video highlights the functionality and portability of the collapsible cup, subtly emphasizing its convenience for those constantly on the move. The playful tossing adds a touch of lightheartedness, suggesting the product is not just practical but also fun to use. \n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "step_by_step_prompt = \"\"\"Describe the video. Analyze the video step-by-step. \n",
        "Output all times when the cup is thrown with timestamps. \n",
        "After that output the timestamp, when the cup is thrown for the second time.\n",
        "\"\"\"\n",
        "\n",
        "contents = [video_content, step_by_step_prompt]\n",
        "generate(gemini_pro_si, contents, as_markdown=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86NmGY798oMC"
      },
      "source": [
        "# Prompt #5. Video Understanding: Get structured outputs\n",
        "\n",
        "Gemini 1.5 Pro and Flash models can generate structured outputs such as JSON, providing a blueprint for the model's output. This feature is also referred to as [controlled generation](https://developers.googleblog.com/en/mastering-controlled-generation-with-gemini-15-schema-adherence/). \n",
        "\n",
        "In this example, we demonstrate Gemini to return structured output (JSON) from a video analysis. One of the ways to achieve better understanding of video (or any multimodal) content is to prompt the model to explain its \"reasoning\" about the response. This has proven to be very effective method, however it can increase the latency. \n",
        "\n",
        "[Vertex AI Gemini API](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) makes it easy to return JSON output by configuring response MIME type as `application/json`. Optionally, you can also configure `response_schema` with the JSON schema for the model to generate output as per the schema."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "d8d4675dc101"
      },
      "outputs": [],
      "source": [
        "response_schema = {\n",
        "    \"type\": \"ARRAY\",\n",
        "    \"items\": {\n",
        "        \"type\": \"OBJECT\",\n",
        "        \"properties\": {\n",
        "            \"harmfulness_reasoning\": {\n",
        "                \"type\": \"STRING\",\n",
        "                \"description\": \"Step-by-step detailed reasoning about how harmful is the video\",\n",
        "            },\n",
        "            \"harmfulness_score\": {\n",
        "                \"type\": \"INTEGER\",\n",
        "                \"description\": \"Number between 0 and 5 indicating how harmful is the video\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"harmfulness_reasoning\", \"harmfulness_score\"],\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "87b34d3255b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{\"harmfulness_reasoning\": \"The video features a person playing with a\n",
            "collapsible cup. There are no elements of violence, sexual content, drugs, or\n",
            "harmful activities. The person handles the cup gently.\", \"harmfulness_score\":\n",
            "0}]"
          ]
        }
      ],
      "source": [
        "structured_prompt = \"\"\"You are an expert video analyzer. You task is to analyze the video \n",
        "and produce a harmfulness score - how harmful this video can be for kids.\"\"\"\n",
        "\n",
        "contents = [video_content, structured_prompt]\n",
        "\n",
        "generate(\n",
        "    gemini_pro,\n",
        "    contents,\n",
        "    generation_config=GenerationConfig(\n",
        "        response_mime_type=\"application/json\", response_schema=response_schema\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJf-Iq8TOxKo"
      },
      "source": [
        "The model returned the correct score for the video by asking the model to output \"reasoning\" along with the score. Adding \"reasoning\" field before the \"score\" gives a consistent and correct score. The intuition is  that LLM can generate \"reasoning\" first and rely on the thoughts to properly produce the score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33SnNRcvLg73"
      },
      "source": [
        "# Prompt #6. Video Understanding: Context Caching\n",
        "\n",
        "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview?hl=en) is a method to reduce the cost of requests that contain repeated content with high input token count. It can potentially reduce the latency at the cost of storing the objects in the cache. The user can specify cache expiration time for which the object is saved in cache.\n",
        "\n",
        "Context caching helps a lot when we want:\n",
        "- to repeatedly ask questions about the long video\n",
        "- to reduce costs and save latency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "b74299377f9f"
      },
      "outputs": [],
      "source": [
        "long_video_path = f\"{videos_path_prefix}/long_video_1.mp4\"\n",
        "long_video_content = Part.from_uri(uri=long_video_path, mime_type=\"video/mp4\")\n",
        "\n",
        "prompt = \"\"\"Describe what happens in the beginning, in the middle and in the \n",
        "end of the video. Also, list the name of the main character and any problems \n",
        "they face.\"\"\"\n",
        "\n",
        "contents = [long_video_content, prompt]\n",
        "# print_prompt(contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "IzJU9CoiMoBj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The video is a silent film called \"Sherlock Jr.\" starring Buster Keaton.  In the\n",
            "beginning, Buster is a movie projectionist who is studying to be a detective. He\n",
            "is in love with a girl, but her father doesn't approve of him. Buster is framed\n",
            "for stealing the girl's father's watch, and he is kicked out of the house.  In\n",
            "the middle, Buster falls asleep while projecting a movie and dreams that he is a\n",
            "detective investigating the theft of a pearl necklace. He uses his detective\n",
            "skills to solve the case, but he is constantly thwarted by the villain.  In the\n",
            "end, Buster wakes up from his dream and realizes that he has been framed for\n",
            "stealing the watch. He goes to the pawn shop where the watch was pawned and\n",
            "finds the real thief. He clears his name and wins the girl's heart.  The main\n",
            "character is Buster Keaton, and he faces the problems of being framed for\n",
            "stealing a watch, being kicked out of the house, and trying to win the girl's\n",
            "heart.\n",
            "Time elapsed: 65.75050516799092 seconds\n"
          ]
        }
      ],
      "source": [
        "# Time the call without context caching\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "start = timer()\n",
        "generate(gemini_pro, contents)\n",
        "end = timer()\n",
        "\n",
        "print(f\"\\nTime elapsed: {end - start} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "K27kb9ofVD-L"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "\n",
        "from vertexai.preview import caching\n",
        "from vertexai.preview.generative_models import GenerativeModel\n",
        "\n",
        "cached_content = caching.CachedContent.create(\n",
        "    model_name=\"gemini-1.5-pro-001\",\n",
        "    contents=[long_video_content],\n",
        "    ttl=datetime.timedelta(hours=1),\n",
        "    display_name=\"long video cache\",\n",
        ")\n",
        "\n",
        "model_cached = GenerativeModel.from_cached_content(cached_content=cached_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8sf4bx6NOSP2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The video is a silent film called \"Sherlock Jr.\" starring Buster Keaton.   In\n",
            "the beginning, Buster is a movie projectionist who is studying to be a\n",
            "detective. He is in love with a girl, but her father doesn't approve of him. A\n",
            "rival for the girl's affections frames Buster for stealing her father's watch.\n",
            "In the middle, Buster is kicked out of the girl's house and tries to follow his\n",
            "rival to prove his innocence. He gets into a series of misadventures, including\n",
            "being chased by a train and falling into a river.  In the end, Buster returns to\n",
            "the movie theater and falls asleep while watching a movie. He dreams that he is\n",
            "a detective in the movie and solves the case. He wakes up and realizes that he\n",
            "has solved the case in real life as well. He is reunited with the girl and her\n",
            "father, and his rival is arrested.  The main character is Buster Keaton. He\n",
            "faces the problems of being framed for a crime he didn't commit, being kicked\n",
            "out of the girl's house, and being chased by a train. He also has to deal with a\n",
            "series of misadventures that happen to him while he is trying to prove his\n",
            "innocence.\n",
            "Time elapsed: 60.3449609875679 seconds\n"
          ]
        }
      ],
      "source": [
        "# Call with context caching\n",
        "start = timer()\n",
        "responses = model_cached.generate_content(\n",
        "    prompt,\n",
        "    generation_config=GENERATION_CONFIG,\n",
        "    safety_settings=SAFETY_CONFIG,\n",
        "    stream=False,\n",
        ")\n",
        "end = timer()\n",
        "\n",
        "print(wrap(responses.text), end=\"\")\n",
        "\n",
        "print(f\"\\nTime elapsed: {end - start} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zO4-G73j6waM"
      },
      "source": [
        "As we see the result with context caching was relatively faster than without context caching. Not only that, the cost of the request is lower as we did not need to send the video again during the prompt for analysis.\n",
        "\n",
        "Context caching therefore is ideal for the repeated questions against the same long file: video, document, audio."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSNrNDh2Ev0G"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "This demonstrated various examples of working with Gemini using videos. Following are general prompting strategies when working with Gemini on multimodal prompts, that can help achieve better performance from Gemini:\n",
        "\n",
        "1. Craft clear and concise instructions.\n",
        "1. Add your video or any media first for single-media prompts.\n",
        "1. Add few-shot examples to the prompt to show the model how you want the task done and the expected output.\n",
        "1. Break down the task step-by-step.\n",
        "1. Specify the output format.\n",
        "1. Ask Gemini to include reasoning in its response along with decision or scores\n",
        "1. Use context caching for repeated queries.\n",
        "\n",
        "Specifically, when working with videos following may help:\n",
        "\n",
        "1. Specify timestamp format when localizing videos.\n",
        "1. Ask Gemini to focus on visual content for well-known video clips.\n",
        "1. Process long videos in segments for dense outputs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebce3772f858"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "multimodal_prompting_video.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "vertex-llm",
      "language": "python",
      "name": "vertex-llm"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
