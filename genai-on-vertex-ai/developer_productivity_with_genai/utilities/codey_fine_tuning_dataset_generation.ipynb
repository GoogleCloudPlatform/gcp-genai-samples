{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ],
   "metadata": {
    "id": "5VJkrztfOyf2"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "| | |\n",
    "|----------|-------------|\n",
    "| Author(s)   | Lei Pan |\n",
    "| Last updated | 01/22/2024 |"
   ],
   "metadata": {
    "id": "QbEcuBwWO0gf"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generate Fine-tuning Dataset\n",
    "\n",
    "Codey models are text-to-code models from Google AI, trained on a massive code related dataset. You can generate code related responses for different scenarios such as writing functions, unit tests, debugging, explaining code etc. Here is [the overview](https://cloud.google.com/vertex-ai/docs/generative-ai/code/code-models-overview) of all the Codey APIs.\n",
    "\n",
    "For some scenarios, fine-tuned codey models work better such as generating code using custom libraries it has never been trained before. In those use cases, you will need to create training dataset to be able to do fine-tuning. Here is [the overview](https://cloud.google.com/vertex-ai/docs/generative-ai/models/tune-code-models) of codey fine-tuning.\n",
    "\n",
    "In this notebook, we will show you how to generate fine-tuning dataset to tune codey models.\n",
    "\n",
    "- Step 1: Set up basic input and output text\n",
    "- Step 2: Simulate more examples based on the input texts\n",
    "- Step 3: Automatically store json data to a JSONL File\n",
    "- Step 4: Automatically upload JSONL to the GCS bucket\n",
    "\n",
    "\n",
    "Caveat: this is done as an example only. In the real world practice, you want to generate more examples for different aspects of using APIs and twist around it to find optimal training datasets."
   ],
   "metadata": {
    "id": "R94Z3P_HO9D1"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prep Work\n",
    "\n",
    "If you don't have a GCP project set up and Vertex AI enabled, please follow [the doc](https://cloud.google.com/vertex-ai/docs/start/cloud-environment#set_up_a_project) to set them up before you proceed.\n",
    "\n"
   ],
   "metadata": {
    "id": "OIV1WfCQQCBA"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Install pre-requisites\n",
    "\n",
    "If running in Colab install the pre-requisites into the runtime. Otherwise it is assumed that the notebook is running in Vertex Workbench. In that case it is recommended to install the pre-requistes from a terminal using the `--user` option."
   ],
   "metadata": {
    "id": "KOGt-e5RhYUb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmD8SLl_D4kS"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    ! pip install google-cloud-aiplatform\n",
    "    ! pip install jsonlines\n",
    "    from google.colab import auth as google_auth\n",
    "    google_auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "39VesLHDQDmT"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import jsonlines\n",
    "import vertexai\n",
    "from vertexai.language_models import TextGenerationModel\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize Vertex AI\n",
    "\n",
    "Please set VERTEX_API_PROJECT and VERTEX_API_LOCATION below with your project id and location for Vertex AI. This should be the project in which you enabled Vertex AI."
   ],
   "metadata": {
    "id": "CwgpnjvQP-55"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "vertexai.init(project=\"your project\", location=\"your location\")"
   ],
   "metadata": {
    "id": "PDHjWMBuNilD"
   },
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Set Up Text Generation Function and GCS Bucket Upload Function\n",
    "\n",
    "- We need text generation to simulate input training data\n",
    "- We need GCS upload function to automatically upload generated training dataset to your GCS bucket"
   ],
   "metadata": {
    "id": "S_nIY39hQJ5R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def paraphrase_input_text(input_text: str) -> str:\n",
    "  parameters = {\n",
    "      \"temperature\": 0.2,\n",
    "      \"max_output_tokens\": 256,\n",
    "      \"top_p\": 0.8,\n",
    "      \"top_k\": 40 ,\n",
    "  }\n",
    "\n",
    "  model = TextGenerationModel.from_pretrained(\"text-bison\")\n",
    "  response = model.predict(\n",
    "      f\"Paraphrase this sentence: {input_text}\",\n",
    "      **parameters,\n",
    "  )\n",
    "  print(f\"Response from Model: {response.text}\")\n",
    "  return response.text\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    generation_match_precondition = None\n",
    "\n",
    "    blob.upload_from_filename(source_file_name, if_generation_match=generation_match_precondition)\n",
    "\n",
    "    print(\n",
    "        f\"File {source_file_name} uploaded to {destination_blob_name}.\"\n",
    "    )\n"
   ],
   "metadata": {
    "id": "QeysHVPQObLu"
   },
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 1: Set Up Basic Input and Output Text\n",
    "\n",
    "- Training datasets for fine-tuning codey models should be in a jsonline file and this is the format [ {input_text:\"xxx\",output_text:\"xxx\"}\n",
    "  {input_text:\"xxx\",output_text:\"xxx\"}\n",
    "  .....\n",
    "]\n",
    "- Input_text means the prompts that you want the model to understand, output_text means the results/coding blocks you want the model to produce\n",
    "- In this example, since we want codey model to know how to use vertex AI search API do to 1) basic search request and 2) more advanced function in the response - converting protobuf to dictionary, we will use the corresponding input and output text for model to learn how to use the API\n",
    "\n"
   ],
   "metadata": {
    "id": "HUhN4XeeQfYC"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Refer to this link to check out the full function of search_sample:\n",
    "https://github.com/GoogleCloudPlatform/python-docs-samples/blob/HEAD/discoveryengine/search_sample.py\n",
    "\n",
    "We only use a few lines from that example as training dataset."
   ],
   "metadata": {
    "id": "vigyryLbXg-K"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "basic_input_text = \"Create a function to perform search requests to the Vertex AI Search and Conversation API and return the search results.\"\n",
    "basic_output_text = \"\"\"def search_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    search_engine_id: str,\n",
    "    serving_config_id: str,\n",
    "    search_query: str,\n",
    ") -> List[discoveryengine.SearchResponse.SearchResult]:\n",
    "    client = discoveryengine.SearchServiceClient()\n",
    "    serving_config = client.serving_config_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        data_store=search_engine_id,\n",
    "        serving_config=serving_config_id,\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.SearchRequest(\n",
    "        serving_config=serving_config,\n",
    "        query=search_query,\n",
    "    )\n",
    "    response = client.search(request)\n",
    "\n",
    "    return response\"\"\"\n",
    "\n",
    "advance_input_text = \"Create a function to send search requests to Vertex AI Search API, convert the protobuf search response to a dictionary, and return the dictionary result.\"\n",
    "advance_output_text = \"\"\"\n",
    "def search_sample(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    search_engine_id: str,\n",
    "    serving_config_id: str,\n",
    "    search_query: str,\n",
    ") -> List[discoveryengine.SearchResponse.SearchResult]:\n",
    "    client = discoveryengine.SearchServiceClient()\n",
    "    serving_config = client.serving_config_path(\n",
    "        project=project_id,\n",
    "        location=location,\n",
    "        data_store=search_engine_id,\n",
    "        serving_config=serving_config_id,\n",
    "    )\n",
    "\n",
    "    request = discoveryengine.SearchRequest(\n",
    "        serving_config=serving_config,\n",
    "        query=search_query,\n",
    "    )\n",
    "    response = client.search(request)\n",
    "    results = [MessageToDict(result.document._pb) for result in response.results]\n",
    "\n",
    "    return results\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "apEegmkUG-hq"
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 2: Simulate More Examples Based on the Input Texts\n",
    "\n",
    "- We got 2 examples above. That's not enough to fine-tune codey models. We're going to use text-bison model to simulate 8 more input_text variations to map to the same output/results we mentioned above.\n",
    "- With 10 examples, models should be able to learn for each category of prompt, what output we are looking for.\n"
   ],
   "metadata": {
    "id": "rwzWndEzUbRH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Call text model to simulate more input text as examples"
   ],
   "metadata": {
    "id": "bWK4v4vxVnrs"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "json_data =[{\"input_text\": basic_input_text,\"output_text\": basic_output_text},\n",
    "            {\"input_text\": advance_input_text,\"output_text\": advance_output_text}\n",
    "            ]\n",
    "\n",
    "def simulate_input_text_add_jsondata(temp_input,temp_output,json_data):\n",
    "  for i in range(4):\n",
    "    new_input_text = paraphrase_input_text(temp_input)\n",
    "    line_json = {\"input_text\": new_input_text,\"output_text\": temp_output}\n",
    "    json_data.append(line_json)\n",
    "    temp_input = new_input_text\n",
    "  return json_data\n",
    "\n",
    "json_data = simulate_input_text_add_jsondata(basic_input_text,basic_output_text,json_data)\n",
    "json_data = simulate_input_text_add_jsondata(advance_input_text,advance_output_text,json_data)"
   ],
   "metadata": {
    "id": "_B-mTNNOIDb3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 3: Automatically Store Json Data to a JSONL File"
   ],
   "metadata": {
    "id": "D3OnFD7IV5yN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "with open('output.jsonl', 'w') as outfile:\n",
    "    for entry in json_data:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n",
    "\n",
    "jsonl_file = open(\"output.jsonl\", \"r\")\n",
    "print(jsonl_file.read())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74ic9S85Ne48",
    "outputId": "f7b5dafa-1d9f-458f-c5e3-3bd6339ed1f1"
   },
   "execution_count": 51,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\"input_text\": \"Create a function to perform search requests to the Vertex AI Search and Conversation API and return the search results.\", \"output_text\": \"def search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n\\n    return response\"}\n",
      "{\"input_text\": \"Create a function to send search requests to Vertex AI Search API, convert the protobuf search response to a dictionary, and return the dictionary result.\", \"output_text\": \"\\ndef search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n    results = [MessageToDict(result.document._pb) for result in response.results]\\n\\n    return results\\n\"}\n",
      "{\"input_text\": \" Develop a function that can send search queries to the Vertex AI Search and Conversation API and provide the search results.\", \"output_text\": \"def search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n\\n    return response\"}\n",
      "{\"input_text\": \" Create a function that can send search queries to the Vertex AI Search and Conversation API and return the search results.\", \"output_text\": \"def search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n\\n    return response\"}\n",
      "{\"input_text\": \" Write a function that can send search queries to the Vertex AI Search and Conversation API and then return the search results.\", \"output_text\": \"def search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n\\n    return response\"}\n",
      "{\"input_text\": \" Create a function that can send search queries to the Vertex AI Search and Conversation API and then return the search results.\", \"output_text\": \"def search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n\\n    return response\"}\n",
      "{\"input_text\": \" Write a function that sends search requests to the Vertex AI Search API, converts the protobuf search response into a dictionary, and returns the dictionary result.\", \"output_text\": \"\\ndef search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n    results = [MessageToDict(result.document._pb) for result in response.results]\\n\\n    return results\\n\"}\n",
      "{\"input_text\": \" Create a function that sends search requests to the Vertex AI Search API, converts the protobuf search response into a Python dictionary, and returns the dictionary result.\", \"output_text\": \"\\ndef search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n    results = [MessageToDict(result.document._pb) for result in response.results]\\n\\n    return results\\n\"}\n",
      "{\"input_text\": \" Write a function that:\\n- Sends search requests to the Vertex AI Search API.\\n- Converts the protobuf search response into a Python dictionary.\\n- Returns the dictionary result.\", \"output_text\": \"\\ndef search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n    results = [MessageToDict(result.document._pb) for result in response.results]\\n\\n    return results\\n\"}\n",
      "{\"input_text\": \" Create a function that:\\n- Makes search requests to the Vertex AI Search API.\\n- Changes the protobuf search response into a Python dictionary.\\n- Gives back the dictionary result.\", \"output_text\": \"\\ndef search_sample(\\n    project_id: str,\\n    location: str,\\n    search_engine_id: str,\\n    serving_config_id: str,\\n    search_query: str,\\n) -> List[discoveryengine.SearchResponse.SearchResult]:\\n    client = discoveryengine.SearchServiceClient()\\n    serving_config = client.serving_config_path(\\n        project=project_id,\\n        location=location,\\n        data_store=search_engine_id,\\n        serving_config=serving_config_id,\\n    )\\n\\n    request = discoveryengine.SearchRequest(\\n        serving_config=serving_config,\\n        query=search_query,\\n    )\\n    response = client.search(request)\\n    results = [MessageToDict(result.document._pb) for result in response.results]\\n\\n    return results\\n\"}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Step 4: Automatically Upload JSONL to the GCS Bucket\n",
    "\n",
    "Replace \"your bucket name\" with your GCS bucket for fine-tuning code model"
   ],
   "metadata": {
    "id": "lFe8-27QWLUv"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "upload_blob(\"your bucket name\", \"output.jsonl\", \"output.jsonl\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69AsyYssOWzB",
    "outputId": "6584d212-e193-4e6d-9faa-3ad41647cf0d"
   },
   "execution_count": 55,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "File output.jsonl uploaded to output.jsonl.\n"
     ]
    }
   ]
  }
 ]
}
