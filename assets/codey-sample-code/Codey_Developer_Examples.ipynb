{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n",
        "\n",
        "Codey models are text-to-code models from Google AI, trained on a massive code related dataset. You can generate code related responses for different scenarios such as writing functions, unit tests, debugging, autocompleting and etc. This notebook is to show you how to use Code-bison and Codechat-bison to generat unit tests and debug code.\n",
        "\n",
        "The notebook is structured as follows:\n",
        "\n",
        "1. We will explain how to Code-bison Python SDK to write unit tests for functions.\n",
        "2. We will explain how to Codechat-bison Python SDK to debug code.\n",
        "3. We will explain how to Code-bison Python SDK to generate documents/comments for code blocks."
      ],
      "metadata": {
        "id": "qhqOkOsjgJ1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Author: [leip@](https://moma.corp.google.com/person/leip)\n",
        "* Date: 09/22/23"
      ],
      "metadata": {
        "id": "MrWty706_Lvi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prep Work"
      ],
      "metadata": {
        "id": "Zw9NzqCg6Wff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install Libraries\n",
        "import sys\n",
        "\n",
        "if 'google.colab' in sys.modules:\n",
        "    ! pip install google-cloud-aiplatform\n",
        "    ! pip install google-cloud-discoveryengine\n",
        "    from google.colab import auth as google_auth\n",
        "    google_auth.authenticate_user()"
      ],
      "metadata": {
        "id": "LWo-Dxmx6F26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize Vertex AI\n",
        "\n",
        "import vertexai\n",
        "from vertexai.language_models import CodeGenerationModel\n",
        "\n",
        "VERTEX_API_PROJECT = 'certain-haiku-391918'\n",
        "VERTEX_API_LOCATION = 'us-central1'\n",
        "\n",
        "vertexai.init(project=VERTEX_API_PROJECT, location=VERTEX_API_LOCATION)\n",
        "code_generation_model = CodeGenerationModel.from_pretrained(\"code-bison@001\")"
      ],
      "metadata": {
        "id": "PKzpbtZ4EFqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unit Test with Code-bison\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mUZEf28uEKz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set Up Unit Test Prompt and Invoke Code-bison with Parameters and Prompt\n",
        "\n",
        "\n",
        "prefix = \"\"\"You're an expert Python programmer and great at writing unit tests in Python. please write unit test for the code below:\n",
        "\n",
        "class Rectangle:\n",
        "    def __init__(self, width, height):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "\n",
        "    def get_area(self):\n",
        "        return self.width * self.height\n",
        "\n",
        "    def set_width(self, width):\n",
        "        self.width = width\n",
        "\n",
        "    def set_height(self, height):\n",
        "        self.height = height\n",
        "\n",
        "\"\"\"\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 512\n",
        "}\n",
        "\n",
        "response = code_generation_model.predict(\n",
        "        prefix=prefix, **parameters\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlIrFS2BEJNA",
        "outputId": "7945b7ec-ec8c-4fec-d26b-6a00f804e141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```python\n",
            "import unittest\n",
            "\n",
            "from rectangle import Rectangle\n",
            "\n",
            "\n",
            "class TestRectangle(unittest.TestCase):\n",
            "\n",
            "    def test_get_area(self):\n",
            "        \"\"\"Test that the get_area() method returns the correct area.\"\"\"\n",
            "        rectangle = Rectangle(10, 20)\n",
            "        self.assertEqual(rectangle.get_area(), 200)\n",
            "\n",
            "    def test_set_width(self):\n",
            "        \"\"\"Test that the set_width() method sets the width correctly.\"\"\"\n",
            "        rectangle = Rectangle(10, 20)\n",
            "        rectangle.set_width(30)\n",
            "        self.assertEqual(rectangle.width, 30)\n",
            "\n",
            "    def test_set_height(self):\n",
            "        \"\"\"Test that the set_height() method sets the height correctly.\"\"\"\n",
            "        rectangle = Rectangle(10, 20)\n",
            "        rectangle.set_height(40)\n",
            "        self.assertEqual(rectangle.height, 40)\n",
            "\n",
            "\n",
            "if __name__ == '__main__':\n",
            "    unittest.main()\n",
            "```\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Debug Code with Code-chat"
      ],
      "metadata": {
        "id": "ilZYru0RKMSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Initialize Codechat-bison\n",
        "\n",
        "from vertexai.language_models import CodeChatModel\n",
        "\n",
        "code_chat_model = CodeChatModel.from_pretrained(\"codechat-bison@001\")\n",
        "chat = code_chat_model.start_chat()"
      ],
      "metadata": {
        "id": "5eD9AAY2Kdi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set Up Debugging Prompt\n",
        "\n",
        "message_1 = \"\"\"You're an expert Pytorch programmer and great at debugging issues in Pytorch. Tell me how to fix this error - 'ValueError: expected sequence of length 43 at dim 1 (got 37)' I got from the code below:\n",
        "\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        ")\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mnli\")\n",
        "\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples[\"premise\"], examples[\"hypothesis\"], truncation=True)\n",
        "\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"distilbert-finetuned-mnli\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        ")\n",
        "\n",
        "metric = evaluate.load(\"glue\", \"mnli\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation_matched\"],\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "trainer.train()\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mKpfjZ7PIaR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Invoke Codechat-bison with Parameters and Prompt\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 2048\n",
        "}\n",
        "\n",
        "response_1 = chat.send_message(message_1)\n",
        "\n",
        "print(response_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJvw7wL5IlmC",
        "outputId": "4aedc1ae-0c62-465b-b0e9-519f2b746b09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The error message is telling you that the input to the model is not the correct size. The model expects an input of shape (batch_size, sequence_length), but the input you are providing is of shape (batch_size, 37).\n",
            "\n",
            "To fix this, you need to make sure that the input to the model is the correct size. In this case, you can do this by changing the `truncation=True` parameter in the `preprocess_function` to `truncation=False`. This will tell the tokenizer to not truncate the input, even if it is longer than the maximum sequence length.\n",
            "\n",
            "Once you have made this change, you should be able to run your code without any errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set Up Follow-Up Debugging Question Prompt\n",
        "\n",
        "message_2 = \"\"\" Ok. I fixed that error. I am getting this new error - \"RuntimeError: CUDA error: CUBLAS_STATUS_ALLOC_FAILED when calling `cublasCreate(handle)`\n",
        "\". How to fix this error?\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HPm6Q_P5LcqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Invoke Codechat-bison with Parameters and Follow-Up Debugging Question Prompt\n",
        "\n",
        "response_2 = chat.send_message(message_2)\n",
        "\n",
        "print(response_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKvC4mi5Mcqw",
        "outputId": "bcce515b-b9f9-436f-8eee-e31d9bf5ef6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The error message is telling you that there is a problem with the CUDA memory allocation. This could be caused by a number of things, such as not having enough free memory on your GPU, or a problem with the CUDA driver.\n",
            "\n",
            "To fix this error, you can try the following:\n",
            "\n",
            "* Make sure that you have enough free memory on your GPU.\n",
            "* Try restarting your computer.\n",
            "* Try updating your CUDA driver.\n",
            "* Try reinstalling PyTorch.\n",
            "\n",
            "If you are still having problems, you can contact the PyTorch support team for help.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code Document Generation"
      ],
      "metadata": {
        "id": "EQ3c90vib2ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Set Up Document Generation Prompt and Invoke Code-bison with Parameters and Prompt\n",
        "\n",
        "\n",
        "prefix = \"\"\"You're great at writing documents for python code. Write comments for this block of python code line by line: [\n",
        "\n",
        "import cmath\n",
        "\n",
        "a = 1\n",
        "b = 5\n",
        "c = 6\n",
        "\n",
        "d = (b**2) - (4*a*c)\n",
        "\n",
        "sol1 = (-b-cmath.sqrt(d))/(2*a)\n",
        "sol2 = (-b+cmath.sqrt(d))/(2*a)\n",
        "\n",
        "print('The solution are {0} and {1}'.format(sol1,sol2))\n",
        "]\n",
        "\n",
        "\"\"\"\n",
        "parameters = {\n",
        "    \"temperature\": 0.2,\n",
        "    \"max_output_tokens\": 512\n",
        "}\n",
        "\n",
        "response = code_generation_model.predict(\n",
        "        prefix=prefix, **parameters\n",
        ")\n",
        "\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Fuy5l_Ub1ev",
        "outputId": "4eaa7c38-f525-4c7c-d999-5b5bee15b951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "* Import the `cmath` module.\n",
            "* Define the variables `a`, `b`, and `c`.\n",
            "* Calculate the discriminant `d`.\n",
            "* Calculate the solutions `sol1` and `sol2`.\n",
            "* Print the solutions to the console.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XiJEI2ekcuEi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}