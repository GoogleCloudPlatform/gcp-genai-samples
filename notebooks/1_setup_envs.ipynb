{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"display: flex; align-items: left;\">\n",
        "    <a href=\"https://sites.google.com/corp/google.com/genai-solutions/home?authuser=0\">\n",
        "        <img src=\"https://storage.googleapis.com/miscfilespublic/Linkedin%20Banner%20%E2%80%93%202.png\" style=\"margin-right\">\n",
        "    </a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRyGcAepAPJ5"
      },
      "source": [
        "\n",
        "# **Talk2Data: Initial Environment Setup**\n",
        "\n",
        "---\n",
        "\n",
        "This notebook walks through the initial environment setup needed for running the Talk2Data solution. \n",
        "As the solution is build on modular components, you can skip to the sections that are relevant for your use case. \n",
        "\n",
        "Currently supported Source DBs are: \n",
        "- PostgreSQL on Google Cloud SQL \n",
        "- BigQuery\n",
        "\n",
        "Furthermore, the following vector stores are supported \n",
        "- pgvector on PostgreSQL \n",
        "- BigQuery vector\n",
        "\n",
        "\n",
        "The notebook covers the following steps: \n",
        "> 1. Setting up PostgreSQL instance on Google Cloud SQL (for using both or either of PostgreSQL & pgvector)\n",
        "\n",
        "> 2. Migrating BigQuery public data to the PostgreSQL instance (for using PostgreSQL as the Source DB)\n",
        "\n",
        "> 3. Setting up BigQuery environment (for using both or either of BigQuery Source DB & Vector Store)\n",
        "\n",
        "> 4. Populating the vector store with the 'known good' question-SQL pairs (BigQuery vector DB & pgvector)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsWGZW_fUJjN"
      },
      "source": [
        "### üìí Using this interactive notebook\n",
        "\n",
        "If you have not used this IDE with jupyter notebooks it will ask for installing Python + Jupyter extensions. Please go ahead install them\n",
        "\n",
        "Click the **run** icons ‚ñ∂Ô∏è  of each section within this notebook.\n",
        "\n",
        "> üí° Alternatively, you can run the currently selected cell with `Ctrl + Enter` (or `‚åò + Enter` on a Mac).\n",
        "\n",
        "> ‚ö†Ô∏è **To avoid any errors**, wait for each section to finish in their order before clicking the next ‚Äúrun‚Äù icon.\n",
        "\n",
        "This sample must be connected to a **Google Cloud project**, but nothing else is needed other than your Google Cloud project.\n",
        "\n",
        "You can use an existing project. Alternatively, you can create a new Cloud project [with cloud credits for free.](https://cloud.google.com/free/docs/gcp-free-tier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RicDCkdI-hmp"
      },
      "source": [
        "## üöß **0. Getting Started**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CqBnkLLCDFz"
      },
      "source": [
        "### üíª **Install Code Dependencies**\n",
        "If you didn't already, install the dependencies by running either poetry install \n",
        "\n",
        "##### Don't forget to switch your notebook kernel to the newly generated .venv environment after running the poetry command. \n",
        "\n",
        "If you cannot find it manually select the Python Interpreter path that you see when you run poetry shell (e.g. ~/.cache/talk2data-Fajjajah-py3.9/bin/python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34mInstalling dependencies from lock file\u001b[39m\n",
            "\n",
            "No dependencies to install or update\n",
            "\n",
            "\u001b[39;1mInstalling\u001b[39;22m the current project: \u001b[36mtalktodata\u001b[39m (\u001b[39;1m0.1.0\u001b[39;22m)\u001b[1G\u001b[2K\u001b[39;1mInstalling\u001b[39;22m the current project: \u001b[36mtalktodata\u001b[39m (\u001b[32m0.1.0\u001b[39m)\n"
          ]
        }
      ],
      "source": [
        "#ignore this if you already using the shell \n",
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(os.path.join('..')))\n",
        "\n",
        "#install poetry and run below\n",
        "!poetry lock\n",
        "!poetry install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yygMe6rPWxHS"
      },
      "source": [
        "### üîê Authenticate to Google Cloud \n",
        "Authenticate to Google Cloud as the IAM user logged into this notebook in order to access your Google Cloud Project.\n",
        "\n",
        "You can do this within Google Colab or using the Application Default Credentials in the Google Cloud CLI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PTXN1_DSXj2b"
      },
      "outputs": [],
      "source": [
        "\"\"\"Colab Auth\"\"\" \n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "\n",
        "\"\"\"Google CLI Auth\"\"\"\n",
        "# !gcloud auth application-default login\n",
        "\n",
        "\n",
        "import google.auth\n",
        "credentials, project_id = google.auth.default()\n",
        "# credentials = google.auth.credentials.with_scopes_if_required(credentials)\n",
        "# authed_http = google.auth.transport.requests.AuthorizedSession(credentials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4W6FPnrYEE8"
      },
      "source": [
        "### üîó Connect Your Google Cloud Project\n",
        "Time to connect your Google Cloud Project to this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "fVz5zhvZ1mM3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'talk2data-genai-sa'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#@markdown Please fill in the value below with your GCP project ID and then run the cell.\n",
        "# PROJECT_ID = \"talk2data-genai-sa\" #@param {type:\"string\"}\n",
        "PROJECT_ID = input(\"Please enter the Project ID. This project will be considered to create source data. \")\n",
        "\n",
        "# Quick input validations.\n",
        "assert PROJECT_ID, \"‚ö†Ô∏è Please provide your Google Cloud Project ID\"\n",
        "\n",
        "# Configure gcloud.\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "PROJECT_ID\n",
        "# !gcloud auth application-default set-quota-project {PROJECT_ID}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enable Required APIs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "!gcloud services enable sqladmin.googleapis.com # Enable Cloud SQL Admin API\n",
        "!gcloud services enable aiplatform.googleapis.com # Enable AI Platform API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **PostgreSQL Source Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noWgbDQQO7mr"
      },
      "source": [
        "## ‚òÅÔ∏è **1. Setting up Cloud SQL PostgreSQL** \n",
        "A **Postgres** Cloud SQL instance is required for the following stages of this notebook.\n",
        "\n",
        "To connect and access our Postgres Cloud SQL database instance(s) we will leverage the [Cloud SQL Python Connector](https://github.com/GoogleCloudPlatform/cloud-sql-python-connector).\n",
        "\n",
        "The Cloud SQL Python Connector is a library that can be used alongside a database driver to allow users to easily connect to a Cloud SQL database without having to manually allowlist IP or manage SSL certificates. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypjpse8yBRdI"
      },
      "source": [
        "üíΩ **Create a Postgres Instance**\n",
        "\n",
        "Running the below cell will verify the existence of a Cloud SQL instance or create a new one if one does not exist.\n",
        "\n",
        "> ‚è≥ - Creating a Cloud SQL instance may take a few minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "_vIX7rNtVLhn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing Postgres Cloud SQL Instance!\n"
          ]
        }
      ],
      "source": [
        "#@markdown Please fill in the both the Google Cloud region and name of your Cloud SQL instance. Once filled in, run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "PG_REGION = \"us-central1\" #@param {type:\"string\"}\n",
        "PG_INSTANCE = \"domingo\"\n",
        "PG_PASSWORD = \"vector123\"\n",
        "\n",
        "# Quick input validations.\n",
        "assert PG_REGION, \"‚ö†Ô∏è Please provide a Google Cloud region\"\n",
        "assert PG_INSTANCE, \"‚ö†Ô∏è Please provide the name of your instance\"\n",
        "\n",
        "# check if Cloud SQL instance exists in the provided region\n",
        "database_version = !gcloud sql instances describe {PG_INSTANCE} --format=\"value(databaseVersion)\"\n",
        "if database_version[0].startswith(\"POSTGRES\"):\n",
        "  print(\"Found existing Postgres Cloud SQL Instance!\")\n",
        "else:\n",
        "  print(\"Creating new Cloud SQL instance...\")\n",
        "  !gcloud sql instances create {PG_INSTANCE} --database-version=POSTGRES_15 \\\n",
        "    --region={PG_REGION} --cpu=1 --memory=4GB --root-password={PG_PASSWORD} \\\n",
        "    --database-flags=cloudsql.iam_authentication=On"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzb0dFO6C4h6"
      },
      "source": [
        "## ‚û°Ô∏è **2. Migrate a public BigQuery database to our PostgreSQL instance**\n",
        "Let's migrate a public BigQuery dataset over to our newly created PostgreSQL instance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A) Set up a Google Cloud Storage Bucket \n",
        "This bucket will be used to store the export of our BigQuery public dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0q5uFF0sJnWK"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bucket Created : talk2data-genai-sa-talk2data\n"
          ]
        }
      ],
      "source": [
        "#@markdown Please fill in the both the Google Cloud region and name of your Cloud SQL instance. Once filled in, run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "BUCKET_NAME = str(PROJECT_ID+'-talk2data') #@param {type:\"string\"}\n",
        "print(\"Bucket Created : \"+ BUCKET_NAME)\n",
        "# Quick input validations.\n",
        "assert BUCKET_NAME, \"‚ö†Ô∏è Please provide a unique name for your bucket\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bucket talk2data-genai-sa-talk2data created\n"
          ]
        }
      ],
      "source": [
        "from google.cloud import storage\n",
        "from urllib.error import HTTPError\n",
        "\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "try: \n",
        "    bucket = storage_client.bucket(BUCKET_NAME)\n",
        "\n",
        "    if bucket.exists(): \n",
        "        print(\"This bucket already exists.\")\n",
        "\n",
        "    else:\n",
        "        bucket = storage_client.create_bucket(BUCKET_NAME)\n",
        "        print(f\"Bucket {bucket.name} created\")\n",
        "\n",
        "except:\n",
        "        print(\"‚ö†Ô∏è This bucket already exists in another project. Make sure to give your bucket a unique name.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B) Export BigQuery Dataset to the Bucket\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@markdown Please fill in which BigQuery dataset to export. You can leave the default values. Once filled in, run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "BQ_PROJECT = \"bigquery-public-data\"\n",
        "BQ_DATABASE = \"google_dei\"\n",
        "BQ_TABLE = \"dar_intersectional_hiring\"\n",
        "\n",
        "\n",
        "# Quick input validations.\n",
        "assert BQ_PROJECT, \"‚ö†Ô∏è Please specify the BigQuery Project\"\n",
        "assert BQ_DATABASE, \"‚ö†Ô∏è Please specify the BigQuery Database\"\n",
        "assert BQ_TABLE, \"‚ö†Ô∏è Please specify the BigQuery Table\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Exported bigquery-public-data:google_dei.dar_intersectional_hiring to gs://talk2data-genai-sa-talk2data/export*.csv\n"
          ]
        }
      ],
      "source": [
        "BUCKET_FILENAME = \"export*.csv\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "destination_uri = \"gs://{}/{}\".format(BUCKET_NAME, BUCKET_FILENAME)\n",
        "dataset_ref = bigquery.DatasetReference(BQ_PROJECT, BQ_DATABASE)\n",
        "table_ref = dataset_ref.table(BQ_TABLE)\n",
        "\n",
        "extract_job = client.extract_table(\n",
        "    table_ref,\n",
        "    destination_uri,\n",
        "    # Location must match that of the source table.\n",
        "    location=\"US\",\n",
        ")  # API request\n",
        "extract_job.result()  # Waits for job to complete.\n",
        "\n",
        "print(\n",
        "    f\"Exported {BQ_PROJECT}:{BQ_DATABASE}.{BQ_TABLE} to {destination_uri}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C) Retrieve Data Types and Formats \n",
        "To migrate our exported .csv files to PostgreSQL, we need to fetch the Data Types and Format from our table in the .csv export. \n",
        "This needs to be done as we're setting up the PostgreSQL table and columns first (and need to provide the columns in the setup).\n",
        "We will load the .csv content into the table afterwards. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.cloud import storage\n",
        "import pandas as pd \n",
        "from google.cloud.sql.connector import Connector\n",
        "\n",
        "storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "blobs = bucket.list_blobs()\n",
        "\n",
        "for idx,blob in enumerate(blobs):\n",
        "    if idx == 0: \n",
        "        URI = \"gs://{}\".format(blob.id).split('.csv', 1)[0]+'.csv'\n",
        "        df = pd.read_csv(URI)\n",
        "\n",
        "        field_names = df.columns\n",
        "        field_types = df.dtypes\n",
        "\n",
        "    else: \n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "field_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "field_types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### D) Build the SQL Query for Table Creation \n",
        "Every database is different. To acommodate for different table structures depending on which BigQuery dataset is being loaded in, we will build the SQL query for creating the required PostgreSQL table dynamically. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "OPTIONAL: \n",
        "If you want to specify specific values for the schema, database, and user, please modify the cell below. \n",
        "You can also keep the default values.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.sql.databases.create) HTTPError 400: Invalid request: failed to create database vectordb. Detail: pq: database \"vectordb\" already exists.\n",
            "Creating Cloud SQL user...done.                                                \n",
            "Created user [vector_user].\n"
          ]
        }
      ],
      "source": [
        "#@markdown Please specify the PGSchema or leave it as default (public)\n",
        "PG_SCHEMA = 'dei'   #default: 'public'\n",
        "PG_DATABASE = 'vectordb'    #default: 'postgres'\n",
        "PG_USER = 'vector_user'    #default: 'postgres'\n",
        "\n",
        "!gcloud sql databases create  {PG_DATABASE} --instance={PG_INSTANCE}\n",
        "\n",
        "!gcloud sql users create {PG_USER} \\\n",
        "--instance={PG_INSTANCE} \\\n",
        "--password={PG_PASSWORD}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8961/3709309006.py:6: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  cols += str(field_names[i]) +\" \"+ str(field_types[i])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'CREATE TABLE dei.dar_intersectional_hiring(workforce object, report_year int64, gender_us object, race_asian float64, race_black float64, race_hispanic_latinx float64, race_native_american float64, race_white float64)'"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_sql(PG_SCHEMA, BQ_TABLE, field_names, field_types): \n",
        "\n",
        "    cols = \"\" \n",
        "\n",
        "    for i in range(len(field_names)): \n",
        "        cols += str(field_names[i]) +\" \"+ str(field_types[i])\n",
        "        if i < (len(field_names)-1): \n",
        "            cols += \", \"\n",
        "\n",
        "\n",
        "    sql = f\"\"\"CREATE TABLE {PG_SCHEMA}.{BQ_TABLE}({cols})\"\"\"\n",
        "\n",
        "    return sql\n",
        "\n",
        "sql = get_sql(PG_SCHEMA, BQ_TABLE, field_names, field_types)\n",
        "sql"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### E) Create the PostgreSQL Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio \n",
        "\n",
        "async def create_pg_table(PROJECT_ID,\n",
        "                          PG_REGION,\n",
        "                          PG_INSTANCE,\n",
        "                          PG_PASSWORD,\n",
        "                          BQ_TABLE, \n",
        "                          PG_DATABASE, \n",
        "                          SQL,\n",
        "                          PG_USER): \n",
        "    \"\"\"Create PG Table from BQ Schema\"\"\"\n",
        "    import asyncio\n",
        "    import asyncpg\n",
        "    from google.cloud.sql.connector import Connector\n",
        "\n",
        "    # Replace the Data Types to work with PostgreSQL supported ones \n",
        "    SQL = SQL.replace(\"object\", \"TEXT\").replace(\"int64\", \"INTEGER\").replace(\"float64\", \"DOUBLE PRECISION\")\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    async with Connector(loop=loop) as connector:\n",
        "        # Create connection to Cloud SQL database\n",
        "        conn: asyncpg.Connection = await connector.connect_async(\n",
        "            f\"{PROJECT_ID}:{PG_REGION}:{PG_INSTANCE}\",  # Cloud SQL instance connection name\n",
        "            \"asyncpg\",\n",
        "            user=f\"{PG_USER}\",\n",
        "            db=f\"{PG_DATABASE}\",\n",
        "            password=f\"{PG_PASSWORD}\"\n",
        "        )\n",
        "\n",
        "        await conn.execute(f\"DROP SCHEMA IF EXISTS {PG_SCHEMA} CASCADE\")        \n",
        "\n",
        "        await conn.execute(f\"CREATE SCHEMA {PG_SCHEMA}\")        \n",
        "\n",
        "        await conn.execute(f\"DROP TABLE IF EXISTS {BQ_TABLE} CASCADE\")\n",
        "        \n",
        "        # Create the table.\n",
        "        await conn.execute(SQL)\n",
        "\n",
        "        await conn.close()\n",
        "\n",
        "\n",
        "# # Create PG Table \n",
        "await(create_pg_table(PROJECT_ID, PG_REGION, PG_INSTANCE, PG_PASSWORD, BQ_TABLE, PG_DATABASE, sql,PG_USER))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### F) Import Data to PostgreSQL Table\n",
        "The below cell will iterate through each export file on our Google Cloud Storage Bucket and load it to the PostgreSQL instance. \n",
        "This may take a while, depending on the size of the BigQuery public dataset. You can optionally set the LIMIT parameter to limit how many export files will be loaded in. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 102 entries, 3 to 107\n",
            "Data columns (total 8 columns):\n",
            " #   Column                Non-Null Count  Dtype  \n",
            "---  ------                --------------  -----  \n",
            " 0   workforce             102 non-null    object \n",
            " 1   report_year           102 non-null    int64  \n",
            " 2   gender_us             102 non-null    object \n",
            " 3   race_asian            102 non-null    float64\n",
            " 4   race_black            102 non-null    float64\n",
            " 5   race_hispanic_latinx  102 non-null    float64\n",
            " 6   race_native_american  102 non-null    float64\n",
            " 7   race_white            102 non-null    float64\n",
            "dtypes: float64(5), int64(1), object(2)\n",
            "memory usage: 7.2+ KB\n"
          ]
        }
      ],
      "source": [
        "async def import_to_pg(PROJECT_ID,\n",
        "                          PG_REGION,\n",
        "                          PG_INSTANCE,\n",
        "                          PG_USER,\n",
        "                          PG_PASSWORD,\n",
        "                          PG_DATABASE,\n",
        "                          BQ_TABLE, \n",
        "                          BUCKET_NAME,\n",
        "                          field_types): \n",
        "    from google.cloud import storage\n",
        "    import pandas as pd \n",
        "    import asyncio\n",
        "    import asyncpg\n",
        "    from google.cloud.sql.connector import Connector\n",
        "\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "\n",
        "    bucket = storage_client.get_bucket(BUCKET_NAME)\n",
        "    blobs = bucket.list_blobs()\n",
        "\n",
        "    LIMIT = 3 \n",
        "\n",
        "    PG_SCHEMA = 'dei'\n",
        "\n",
        "    loop = asyncio.get_running_loop()\n",
        "    async with Connector(loop=loop) as connector:\n",
        "        # Create connection to Cloud SQL database\n",
        "        conn: asyncpg.Connection = await connector.connect_async(\n",
        "            f\"{PROJECT_ID}:{PG_REGION}:{PG_INSTANCE}\",  # Cloud SQL instance connection name\n",
        "            \"asyncpg\",\n",
        "            user=f\"{PG_USER}\",\n",
        "            password=f\"{PG_PASSWORD}\",\n",
        "            db=f\"{PG_DATABASE}\",\n",
        "        )\n",
        "\n",
        "        for idx,blob in enumerate(blobs):\n",
        "            if idx < LIMIT:     # Comment this out if you don't want to use a limit \n",
        "                URI = \"gs://{}\".format(blob.id).split('.csv', 1)[0]+'.csv'\n",
        "                df = pd.read_csv(URI)\n",
        "\n",
        "                df = df.dropna()\n",
        "\n",
        "                df.info()   \n",
        "\n",
        "                # Copy the dataframe to the table.\n",
        "                tuples = list(df.itertuples(index=False))\n",
        "\n",
        "                r = tuples \n",
        "                c = list(df) \n",
        "\n",
        "                await conn.copy_records_to_table(\n",
        "                    BQ_TABLE, records=tuples, columns=list(df), schema_name=PG_SCHEMA, timeout=3600\n",
        "                )\n",
        "        await conn.close()\n",
        "\n",
        "    # # Load Data into PG Table \n",
        "await(import_to_pg(PROJECT_ID, PG_REGION, PG_INSTANCE, PG_USER, PG_PASSWORD, PG_DATABASE, BQ_TABLE, BUCKET_NAME, field_types))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üóÑÔ∏è **4. Load Known Good SQL into Vector Store**\n",
        "The following cell will load the Known Good Question-SQL pairs into our Vector Store. \n",
        "For this, it is fetching the contents of the file `known_good_sql.csv`. \n",
        "If you have your own Question-SQL examples, put them in the .csv file before running the cell below. \n",
        "\n",
        "Note: For pgvector, the pairs will be loaded in the 'public' schema. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO: Use PGConnector \n",
        "async def cache_known_sql(PROJECT_ID,\n",
        "                           PG_REGION,\n",
        "                           PG_INSTANCE,\n",
        "                           PG_USER,\n",
        "                           PG_PASSWORD,\n",
        "                           PG_DATABASE,\n",
        "                           VECTOR_STORE):\n",
        "    from utilities import root_dir \n",
        "    import pandas as pd\n",
        "    import os\n",
        "    import asyncio\n",
        "    import asyncpg\n",
        "    from google.cloud.sql.connector import Connector\n",
        "\n",
        "    df = pd.read_csv(root_dir+\"/scripts/known_good_sql.csv\")\n",
        "    df = df.loc[:, [\"prompt\", \"sql\", \"database_name\"]]\n",
        "    df = df.dropna()\n",
        "\n",
        "    if VECTOR_STORE == \"cloudsql-pgvector\": \n",
        "        loop = asyncio.get_running_loop()\n",
        "        async with Connector(loop=loop) as connector:\n",
        "            # Create connection to Cloud SQL database\n",
        "            conn: asyncpg.Connection = await connector.connect_async(\n",
        "                f\"{PROJECT_ID}:{PG_REGION}:{PG_INSTANCE}\",  # Cloud SQL instance connection name\n",
        "                \"asyncpg\",\n",
        "                user=f\"{PG_USER}\",\n",
        "                password=f\"{PG_PASSWORD}\",\n",
        "                db=f\"{PG_DATABASE}\",\n",
        "            )\n",
        "\n",
        "\n",
        "            await conn.execute(\"DROP TABLE IF EXISTS query_example_embeddings CASCADE\")\n",
        "\n",
        "            # Create the `query_example_embeddings` table.\n",
        "            await conn.execute(\n",
        "                \"\"\"CREATE TABLE query_example_embeddings(\n",
        "                                    prompt TEXT,\n",
        "                                    sql TEXT,\n",
        "                                    database_name TEXT)\"\"\"\n",
        "            )\n",
        "\n",
        "            # Copy the dataframe to the 'query_example_embeddings' table.\n",
        "            tuples = list(df.itertuples(index=False))\n",
        "            await conn.copy_records_to_table(\n",
        "                \"query_example_embeddings\", records=tuples, columns=list(df), timeout=3600\n",
        "            )\n",
        "            await conn.close()\n",
        "\n",
        "    elif VECTOR_STORE == \"bigquery\": \n",
        "        \"\"\"TODO\"\"\"\n",
        "        print(\"\")\n",
        "\n",
        "    else: raise ValueError(\"Not a valid parameter for a vector store.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "await(cache_known_sql(PROJECT_ID,\n",
        "                           PG_REGION,\n",
        "                           PG_INSTANCE,\n",
        "                           PG_USER,\n",
        "                           PG_PASSWORD,\n",
        "                           PG_DATABASE,\n",
        "                           VECTOR_STORE=\"cloudsql-pgvector\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### BigQuery Dataset Setup for Processing (Logging)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Destination Dataset exists\n",
            "talk2data-genai-sa.talk2data is created\n"
          ]
        }
      ],
      "source": [
        "# @markdown Please fill in the both the Google Cloud region and name of your Dataset. Once filled in, run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "BQ_TALK2DATA_DATASET_NAME = \"talk2data\" #@param {type:\"string\"}\n",
        "BQ_DATASET_REGION = \"us-central1\" #@param {type:\"string\"}\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import google.api_core \n",
        "\n",
        "client=bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "dataset_ref = f\"{PROJECT_ID}.{BQ_TALK2DATA_DATASET_NAME}\"\n",
        "\n",
        "try:\n",
        "    client.get_dataset(dataset_ref)\n",
        "    print(\"Destination Dataset exists\")\n",
        "except google.api_core.exceptions.NotFound:\n",
        "    print(\"Cannot find the dataset hence creating.......\")\n",
        "    dataset=bigquery.Dataset(dataset_ref)\n",
        "    dataset.location=BQ_DATASET_REGION\n",
        "    client.create_dataset(dataset)\n",
        "\n",
        "print(str(dataset_ref)+\" is created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enter your vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "VECTOR_STORE=input(\"Enter which vector you would like to us \\\"cloudsql-pgvector\\\"  or \\\"bigquery-vector\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2giQFUUCttsK"
      },
      "source": [
        "## üíæ Save your Configuration \n",
        "We will save the configurations set in this notebook to avoid redundant parameter settings in the following notebooks. \n",
        "The information will be stored in the file `config.ini` in the root directory of this repository. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "\n",
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(module_path+'/config.ini')\n",
        "\n",
        "config['GCP']['PROJECT_ID'] = PROJECT_ID\n",
        "config['CONFIG']['DATA_SOURCE'] = 'cloudsql-pg'\n",
        "config['CONFIG']['VECTOR_STORE'] = VECTOR_STORE\n",
        "config['PGCLOUDSQL']['PG_SCHEMA'] = PG_SCHEMA\n",
        "config['PGCLOUDSQL']['PG_DATABASE'] = PG_DATABASE\n",
        "config['PGCLOUDSQL']['PG_USER'] = PG_USER\n",
        "config['PGCLOUDSQL']['PG_REGION'] = PG_REGION\n",
        "config['PGCLOUDSQL']['PG_INSTANCE'] = PG_INSTANCE\n",
        "config['PGCLOUDSQL']['PG_PASSWORD'] = PG_PASSWORD\n",
        "\n",
        "config['BIGQUERY']['BQ_DATASET_REGION'] = BQ_DATASET_REGION\n",
        "config['BIGQUERY']['BQ_TALK2DATA_DATASET_NAME'] = BQ_TALK2DATA_DATASET_NAME\n",
        "\n",
        "\n",
        "with open(module_path+'/config.ini', 'w') as configfile:    # save\n",
        "    config.write(configfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **BigQuery Source Setup**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚òÅÔ∏è **1. Source from BigQuery Public Dataset**\n",
        "\n",
        "Conside the table from the public dataset to ask questions against. Copy that the needed table to local dataset so that.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Please fill in these values.\n",
        "BQ_SRC_PROJECT = \"bigquery-public-data\"\n",
        "BQ_SRC_DATASET = \"fda_food\"\n",
        "BQ_SRC_TABLE = \"food_enforcement\"\n",
        "BQ_SRC_REGION= \"us\"\n",
        "\n",
        "BQ_DST_PROJECT = PROJECT_ID\n",
        "BQ_DST_DATASET = \"fda_food\"\n",
        "BQ_DST_TABLE = \"food_enforcement\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def createBQDataset(bq_project_id, dataset_name,dataset_region):\n",
        "    from google.cloud import bigquery\n",
        "    import google.api_core \n",
        "\n",
        "    client=bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "    dataset_ref = f\"{bq_project_id}.{dataset_name}\"\n",
        "\n",
        "    try:\n",
        "        client.get_dataset(dataset_ref)\n",
        "        print(\"Destination Dataset exists\")\n",
        "    except google.api_core.exceptions.NotFound:\n",
        "        print(\"Cannot find the dataset hence creating.......\")\n",
        "        dataset=bigquery.Dataset(dataset_ref)\n",
        "        dataset.location=dataset_region\n",
        "        client.create_dataset(dataset)\n",
        "        \n",
        "    return dataset_ref\n",
        "\n",
        "def createBQTable(bq_project_id,dataset_name, table_name):\n",
        "        from google.cloud import bigquery\n",
        "        import google.api_core \n",
        "\n",
        "        client=bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "        table_ref = client.dataset(dataset_name, project=bq_project_id).table(table_name)\n",
        "\n",
        "        try:\n",
        "            client.get_table(table_ref)\n",
        "            print(\"Destination Table exists\")\n",
        "            \n",
        "        except google.api_core.exceptions.NotFound:\n",
        "            print(\"Cannot find the table hence creating.......\")\n",
        "            table = bigquery.Table(table_ref)\n",
        "            client.create_table(table)\n",
        "\n",
        "        return table_ref\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Destination Dataset exists\n",
            "Destination Table exists\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CopyJob<project=talk2data-genai-sa, location=US, id=b6f2b6de-066e-46db-a72a-d0bd4316e9b2>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Create destination able and copy the table data\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client=bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "dst_dataset_ref=createBQDataset(BQ_DST_PROJECT,BQ_DST_DATASET,BQ_SRC_REGION)\n",
        "\n",
        "dst_table_ref=createBQTable(BQ_DST_PROJECT,BQ_DST_DATASET,BQ_DST_TABLE)\n",
        "\n",
        "src_table_ref = client.dataset(BQ_SRC_DATASET, project=BQ_SRC_PROJECT).table(BQ_SRC_TABLE)\n",
        "\n",
        "job_config = bigquery.CopyJobConfig(write_disposition=\"WRITE_TRUNCATE\")\n",
        "\n",
        "copy_job = client.copy_table(src_table_ref, dst_table_ref, job_config=job_config)\n",
        "\n",
        "\n",
        "\n",
        "# Wait for the job to complete and check for errors\n",
        "copy_job.result()  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  **2. BigQuery Dataset Setup for Processing (Logs and Vector Embedding (bigquery vector store)**\n",
        "\n",
        "In the above step we setup our source data now we need to setup bigquery for processing and operational work such as storing embedding or storing logs etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Destination Dataset exists\n",
            "talk2data-genai-sa.talk2data is created\n"
          ]
        }
      ],
      "source": [
        "# @markdown Please fill in the both the Google Cloud region and name of your Dataset. Once filled in, run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "BQ_TALK2DATA_DATASET_NAME = \"talk2data\" #@param {type:\"string\"}\n",
        "BQ_DATASET_REGION = \"us-central1\" #@param {type:\"string\"}\n",
        "\n",
        "from google.cloud import bigquery\n",
        "import google.api_core \n",
        "\n",
        "client=bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "dataset_ref=createBQDataset(PROJECT_ID,BQ_TALK2DATA_DATASET_NAME,BQ_DATASET_REGION)\n",
        "\n",
        "print(str(dataset_ref)+\" is created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  **Create PostgreSQL Instance on CloudSQL if you storing embeddings on pgvector store**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing Postgres Cloud SQL Instance!\n"
          ]
        }
      ],
      "source": [
        "#@markdown Please fill in the both the Google Cloud region and name of your Cloud SQL instance. Once filled in, run the cell.\n",
        "\n",
        "# Please fill in these values.\n",
        "PG_REGION = \"us-central1\" #@param {type:\"string\"}\n",
        "PG_INSTANCE = \"domingo\"\n",
        "PG_PASSWORD = \"vector123\"\n",
        "\n",
        "\n",
        "# Quick input validations.\n",
        "assert PG_REGION, \"‚ö†Ô∏è Please provide a Google Cloud region\"\n",
        "assert PG_INSTANCE, \"‚ö†Ô∏è Please provide the name of your instance\"\n",
        "\n",
        "# check if Cloud SQL instance exists in the provided region\n",
        "database_version = !gcloud sql instances describe {PG_INSTANCE} --format=\"value(databaseVersion)\"\n",
        "if database_version[0].startswith(\"POSTGRES\"):\n",
        "  print(\"Found existing Postgres Cloud SQL Instance!\")\n",
        "else:\n",
        "  print(\"Creating new Cloud SQL instance...\")\n",
        "  !gcloud sql instances create {PG_INSTANCE} --database-version=POSTGRES_15 \\\n",
        "    --region={PG_REGION} --cpu=1 --memory=4GB --root-password={PG_PASSWORD} \\\n",
        "    --database-flags=cloudsql.iam_authentication=On"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;31mERROR:\u001b[0m (gcloud.sql.databases.create) HTTPError 400: Invalid request: failed to create database talk2data. Detail: pq: database \"talk2data\" already exists.\n",
            "Creating Cloud SQL user...done.                                                \n",
            "Created user [vector_user].\n"
          ]
        }
      ],
      "source": [
        "#Customize below values if needed or they will default to default values\n",
        "\n",
        "PG_SCHEMA = 'talk2data'   #default: 'public'\n",
        "PG_DATABASE = 'talk2data'    #default: 'postgres'\n",
        "PG_USER = 'vector_user'    #default: 'postgres'\n",
        "\n",
        "\n",
        "!gcloud sql databases create  {PG_DATABASE} --instance={PG_INSTANCE}\n",
        "\n",
        "!gcloud sql users create {PG_USER} \\\n",
        "--instance={PG_INSTANCE} \\\n",
        "--password={PG_PASSWORD}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enter your vector store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "VECTOR_STORE=input(\"Enter which vector store you would like to use \\\"cloudsql-pgvector\\\"  or \\\"bigquery-vector\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üíæ Save your Configuration \n",
        "We will save the configurations set in this notebook to avoid redundant parameter settings in the following notebooks. \n",
        "The information will be stored in the file `config.ini` in the root directory of this repository. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "\n",
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(module_path+'/config.ini')\n",
        "\n",
        "config['GCP']['PROJECT_ID'] = PROJECT_ID\n",
        "config['CONFIG']['DATA_SOURCE'] = 'bigquery'\n",
        "config['BIGQUERY']['BQ_DATASET_REGION'] = BQ_DATASET_REGION\n",
        "config['BIGQUERY']['BQ_DATASET_NAME'] = BQ_DST_DATASET\n",
        "config['BIGQUERY']['BQ_TALK2DATA_DATASET_NAME'] = BQ_TALK2DATA_DATASET_NAME\n",
        "\n",
        "#ignore below you do not intend to use pgvector store\n",
        "config['PGCLOUDSQL']['PG_SCHEMA'] = PG_SCHEMA\n",
        "config['PGCLOUDSQL']['PG_DATABASE'] = PG_DATABASE\n",
        "config['PGCLOUDSQL']['PG_USER'] = PG_USER\n",
        "config['PGCLOUDSQL']['PG_REGION'] = PG_REGION\n",
        "config['PGCLOUDSQL']['PG_INSTANCE'] = PG_INSTANCE\n",
        "config['PGCLOUDSQL']['PG_PASSWORD'] = PG_PASSWORD\n",
        "\n",
        "\n",
        "with open(module_path+'/config.ini', 'w') as configfile:    # save\n",
        "    config.write(configfile)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
