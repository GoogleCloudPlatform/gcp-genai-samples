{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Colab Auth\"\"\" \n",
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "\n",
    "\"\"\"Google CLI Auth\"\"\"\n",
    "# !gcloud auth application-default login\n",
    "\n",
    "\n",
    "import google.auth\n",
    "credentials, project_id = google.auth.default()\n",
    "\n",
    "#@markdown Please fill in the value below with your GCP project ID and then run the cell.\n",
    "# PROJECT_ID = \"talk2data-genai-sa\" #@param {type:\"string\"}\n",
    "PROJECT_ID = input(\"Please enter the Google Cloud Project ID \")\n",
    "\n",
    "# Quick input validations.\n",
    "assert PROJECT_ID, \"⚠️ Please provide your Google Cloud Project ID\"\n",
    "\n",
    "# Configure gcloud.\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "PROJECT_ID\n",
    "# !gcloud auth application-default set-quota-project {PROJECT_ID}\n",
    "\n",
    "import os\n",
    "os.environ['GOOGLE_CLOUD_QUOTA_PROJECT']=PROJECT_ID\n",
    "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "import asyncpg\n",
    "from google.cloud.sql.connector import Connector\n",
    "from google.cloud import bigquery\n",
    "\n",
    "module_prefixes = [\"Talk2Data\", \"open-data-qna\", \"applied-ai-engineering-samples\"]\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "while current_dir != os.path.dirname(current_dir):  # Loop until root dir\n",
    "    if any(prefix in current_dir for prefix in module_prefixes):\n",
    "        config_path = os.path.join(current_dir, 'config.ini')\n",
    "        if os.path.exists(config_path):\n",
    "            config.read(config_path)\n",
    "            root_dir = current_dir\n",
    "            break  # Stop searching once found\n",
    "    current_dir = os.path.dirname(current_dir)  # Move to parent dir\n",
    "\n",
    "if not 'root_dir' in locals():  # If not found in any parent dir\n",
    "    raise FileNotFoundError(\"config.ini not found in current or parent directories.\")\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read(root_dir+'/config.ini')\n",
    "\n",
    "PROJECT_ID = config['GCP']['PROJECT_ID']\n",
    "DATA_SOURCE = config['CONFIG']['DATA_SOURCE']\n",
    "VECTOR_STORE = config['CONFIG']['VECTOR_STORE']\n",
    "PG_SCHEMA = config['PGCLOUDSQL']['PG_SCHEMA']\n",
    "PG_DATABASE = config['PGCLOUDSQL']['PG_DATABASE']\n",
    "PG_USER = config['PGCLOUDSQL']['PG_USER']\n",
    "PG_REGION = config['PGCLOUDSQL']['PG_REGION'] \n",
    "PG_INSTANCE = config['PGCLOUDSQL']['PG_INSTANCE'] \n",
    "PG_PASSWORD = config['PGCLOUDSQL']['PG_PASSWORD']\n",
    "BQ_OPENDATAQNA_DATASET_NAME = config['BIGQUERY']['BQ_OPENDATAQNA_DATASET_NAME']\n",
    "BQ_LOG_TABLE_NAME = config['BIGQUERY']['BQ_LOG_TABLE_NAME'] \n",
    "BQ_DATASET_REGION = config['BIGQUERY']['BQ_DATASET_REGION']\n",
    "BQ_DATASET_NAME = config['BIGQUERY']['BQ_DATASET_NAME']\n",
    "BQ_TABLE_LIST = config['BIGQUERY']['BQ_TABLE_LIST']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Known Good SQL Found at Path :: /home/mokshazna/Talk2Data/scripts/known_good_sql.csv\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "root_dir = os.path.expanduser('~')  # Start at the user's home directory\n",
    "\n",
    "while current_dir != root_dir:\n",
    "    for dirpath, dirnames, filenames in os.walk(current_dir):\n",
    "        config_path = os.path.join(dirpath, 'known_good_sql.csv')\n",
    "        if os.path.exists(config_path):\n",
    "            file_path = config_path  # Update root_dir to the found directory\n",
    "            break  # Stop outer loop once found\n",
    "\n",
    "    current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "print(\"Known Good SQL Found at Path :: \"+file_path)\n",
    "\n",
    "# Load the file\n",
    "df_kgq = pd.read_csv(file_path)\n",
    "df_kgq = df_kgq.loc[:, [\"prompt\", \"sql\", \"database_name\"]]\n",
    "df_kgq = df_kgq.dropna()\n",
    "\n",
    "print('Known Good SQLs Loaded into a Dataframe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Specify mode for loading the known good sql\n",
    "\n",
    "The known good sql can loaded in two modes:\n",
    "* Append mode: Apended to the existing KGQ in the vector store \n",
    "* Refresh mode: Delete the existing KGQ and create of fresh copy from KGQ in known_good_sql.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loading_mode = 'append' # Options 'append' or 'referesh'\n",
    "\n",
    "assert loading_mode in {'append', 'referesh'}, \"⚠️ Invalid loading_mode. Must be 'append' and 'referesh'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good SQL Examples have been loaded to BigQuery Vector\n"
     ]
    }
   ],
   "source": [
    "# If you have Known Good Queries, load them to known_good_sql.csv file; \n",
    "# These will be used as few shot examples for query generation. \n",
    "if loading_mode == 'refresh':\n",
    "\n",
    "    from embeddings.kgq_embeddings import setup_kgq_table\n",
    "    # Delete any old tables and create a new table to KGQ embeddings\n",
    "    if VECTOR_STORE=='bigquery-vector':\n",
    "        await(setup_kgq_table(project_id=PROJECT_ID,\n",
    "                            instance_name=None,\n",
    "                            database_name=None,\n",
    "                            schema=BQ_OPENDATAQNA_DATASET_NAME,\n",
    "                            database_user=None,\n",
    "                            database_password=None,\n",
    "                            region=BQ_DATASET_REGION,\n",
    "                            VECTOR_STORE = VECTOR_STORE\n",
    "                            ))\n",
    "\n",
    "    elif VECTOR_STORE=='cloudsql-pgvector':\n",
    "        await(setup_kgq_table(project_id=PROJECT_ID,\n",
    "                            instance_name=PG_INSTANCE,\n",
    "                            database_name=PG_DATABASE,\n",
    "                            schema=None,\n",
    "                            database_user=PG_USER,\n",
    "                            database_password=PG_PASSWORD,\n",
    "                            region=PG_REGION,\n",
    "                            VECTOR_STORE = VECTOR_STORE\n",
    "                            ))\n",
    "\n",
    "\n",
    "print(\"Adding Known Good Queries to the Vector store.....\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "current_dir = os.getcwd()\n",
    "root_dir = os.path.expanduser('~')  # Start at the user's home directory\n",
    "\n",
    "while current_dir != root_dir:\n",
    "    for dirpath, dirnames, filenames in os.walk(current_dir):\n",
    "        config_path = os.path.join(dirpath, 'known_good_sql.csv')\n",
    "        if os.path.exists(config_path):\n",
    "            file_path = config_path  # Update root_dir to the found directory\n",
    "            break  # Stop outer loop once found\n",
    "\n",
    "    current_dir = os.path.dirname(current_dir)\n",
    "\n",
    "print(\"Known Good SQL Found at Path :: \"+file_path)\n",
    "\n",
    "# Load the file\n",
    "df_kgq = pd.read_csv(file_path)\n",
    "df_kgq = df_kgq.loc[:, [\"prompt\", \"sql\", \"database_name\"]]\n",
    "df_kgq = df_kgq.dropna()\n",
    "\n",
    "from embeddings.kgq_embeddings import store_kgq_embeddings\n",
    "# Add KGQ to the vector store\n",
    "if VECTOR_STORE=='bigquery-vector':\n",
    "    await(store_kgq_embeddings(df_kgq,\n",
    "                                project_id=PROJECT_ID,\n",
    "                                instance_name=None,\n",
    "                                database_name=BQ_OPENDATAQNA_DATASET_NAME,\n",
    "                                schema=BQ_DATASET_NAME,\n",
    "                                database_user=None,\n",
    "                                database_password=None,\n",
    "                                region=BQ_DATASET_REGION,\n",
    "                                VECTOR_STORE = VECTOR_STORE\n",
    "                                ))\n",
    "\n",
    "elif VECTOR_STORE=='cloudsql-pgvector':\n",
    "    await(store_kgq_embeddings(df_kgq,\n",
    "                                project_id=PROJECT_ID,\n",
    "                                instance_name=PG_INSTANCE,\n",
    "                                database_name=PG_DATABASE,\n",
    "                                schema=None,\n",
    "                                database_user=PG_USER,\n",
    "                                database_password=PG_PASSWORD,\n",
    "                                region=PG_REGION,\n",
    "                                VECTOR_STORE = VECTOR_STORE\n",
    "                                ))\n",
    "print('Done!!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talktodata-Fy2pM2BF-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
