{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "raEIA83EG6ahzfMOjKPk4leE"
      },
      "source": [
        "# @title Install Necessary Packages\n",
        "first = True\n",
        "if first:\n",
        "  ! pip install google-cloud-aiplatform --upgrade --quiet\n",
        "  ! pip install shapely==1.8.5 --quiet\n",
        "  ! pip install sqlalchemy --upgrade --quiet\n",
        "  ! pip install asyncio asyncpg cloud-sql-python-connector[\"asyncpg\"] --quiet\n",
        "  ! pip install numpy pandas --quiet\n",
        "  ! pip install pgvector --quiet\n",
        "  ! pip install pg8000 --quiet\n",
        "  ! pip install gradio --quiet"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "raEIA83EG6ahzfMOjKPk4leE"
    },
    {
      "cell_type": "code",
      "source": [
        "# get_ipython().kernel.do_shutdown(True)"
      ],
      "metadata": {
        "id": "7Hp8gUgeMrUB"
      },
      "execution_count": null,
      "outputs": [],
      "id": "7Hp8gUgeMrUB"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "8313wfm1MzjX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709107474809,
          "user_tz": -330,
          "elapsed": 182,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 1,
      "outputs": [],
      "id": "8313wfm1MzjX"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "auth_user=!gcloud config get-value account\n",
        "auth_user=auth_user[0]\n",
        "print('Authenticated User: ' + str(auth_user))\n"
      ],
      "metadata": {
        "id": "wz70u8PPM1Ka",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709107476478,
          "user_tz": -330,
          "elapsed": 1102,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "2b28f5b1-6ee0-4bd2-d622-ee55635a6783"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated User: moksh.google@shoppersstop.com\n"
          ]
        }
      ],
      "id": "wz70u8PPM1Ka"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Assignment of Variables\n",
        "source_type='BigQuery'\n",
        "\n",
        "\n",
        "# @markdown Provide the below details to start using the notebook\n",
        "PROJECT_ID='ss-genai-npd-svc-prj-01' # @param {type:\"string\"}\n",
        "LLM_ENDPOINT_REGION = 'asia-southeast1' # @param {type:\"string\"}\n",
        "DATAPROJECT_ID='ss-genai-npd-svc-prj-01'  # This needs to be adjusted when using the bq public dataset\n",
        "\n",
        "#set and show gcp project\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "!gcloud config get-value project\n",
        "#!bash gcloud auth application-default login\n",
        "\n",
        "\n",
        "# BQ Schema (DATASET) where tables leave\n",
        "\n",
        "schema=\"NA\" # @param {type:\"string\"}.  ### DDL extraction performed at this level, for the entire schema\n",
        "USER_DATASET= DATAPROJECT_ID + '.' + schema\n",
        "\n",
        "table_id_list=\"ss-jarvis-npd-svc-prj-01.Customer.FC_Customer_Master, ss-jarvis-npd-svc-prj-01.Customer.Persona,ss-jarvis-npd-svc-prj-01.Product.Product_Master, ss-jarvis-npd-svc-prj-01.Sales.storesales,ss-jarvis-npd-svc-prj-01.Sales.ecommdenormdata,ss-jarvis-npd-svc-prj-01.Store.dim_location_full\" # @param {type:\"string\"}\n",
        "\n",
        "\n",
        "# BQ Schema (DATASET) where tables leave\n",
        "\n",
        "# Execution Parameters\n",
        "SQL_VALIDATION='ALL'\n",
        "INJECT_ONE_ERROR=False\n",
        "EXECUTE_FINAL_SQL=True\n",
        "SQL_MAX_FIX_RETRY=3\n",
        "AUTO_ADD_KNOWNGOOD_SQL=True\n",
        "\n",
        "# Analytics Warehouse\n",
        "ENABLE_ANALYTICS=True\n",
        "DATASET_NAME='nl2sql'\n",
        "DATASET_LOCATION='asia-south1'\n",
        "LOG_TABLE_NAME='query_logs'\n",
        "FULL_LOG_TEXT=''\n",
        "\n",
        "\n",
        "# Palm Models to use\n",
        "model_id='gemini-pro' # @param {type:\"string\"}\n",
        "chat_model_id='codechat-bison-32k' # @param {type:\"string\"}\n",
        "embeddings_model='textembedding-gecko@001'\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nd1vzO_0eup2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108024659,
          "user_tz": -330,
          "elapsed": 4023,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "d60b52c7-229b-47d2-faf8-b58737870eba"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated property [core/project].\n",
            "ss-genai-npd-svc-prj-01\n"
          ]
        }
      ],
      "id": "nd1vzO_0eup2"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Common Imports\n",
        "import time\n",
        "import datetime\n",
        "from datetime import datetime, timezone\n",
        "import hashlib\n",
        "import vertexai\n",
        "import pandas\n",
        "import pandas_gbq\n",
        "import matplotlib.pyplot as plt\n",
        "from sqlalchemy import create_engine\n",
        "from sqlalchemy import text\n",
        "import pandas as pd\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "import json\n",
        "from google.cloud import bigquery\n",
        "from google.cloud.exceptions import NotFound\n",
        "from logging import exception\n",
        "import asyncio\n",
        "import asyncpg\n",
        "from google.cloud.sql.connector import Connector\n",
        "import numpy as np\n",
        "from pgvector.asyncpg import register_vector\n",
        "from google.cloud import aiplatform\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "tEiQd5f9e0Wv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108036630,
          "user_tz": -330,
          "elapsed": 11974,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 4,
      "outputs": [],
      "id": "tEiQd5f9e0Wv"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Model Endpoint Creation\n",
        "def createModel(PROJECT_ID, LLM_ENDPOINT_REGION, model_id):\n",
        "  from vertexai.preview.language_models import TextGenerationModel\n",
        "  from vertexai.preview.language_models import CodeGenerationModel\n",
        "  from vertexai.preview.language_models import CodeChatModel\n",
        "  from vertexai.preview.generative_models import GenerativeModel\n",
        "\n",
        "  if model_id == 'code-bison-32k':\n",
        "    model = CodeGenerationModel.from_pretrained('code-bison-32k')\n",
        "  elif model_id == 'text-bison-32k':\n",
        "    model = TextGenerationModel.from_pretrained('text-bison-32k')\n",
        "  elif model_id == 'codechat-bison-32k':\n",
        "    model = CodeChatModel.from_pretrained(\"codechat-bison-32k\")\n",
        "  elif model_id == 'gemini-pro':\n",
        "    model = GenerativeModel(\"gemini-pro\")\n",
        "  else:\n",
        "    raise ValueError\n",
        "  return model\n",
        "\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=LLM_ENDPOINT_REGION)\n",
        "model=createModel(PROJECT_ID, LLM_ENDPOINT_REGION,model_id)\n",
        "chat_model=createModel(PROJECT_ID, LLM_ENDPOINT_REGION,chat_model_id)"
      ],
      "metadata": {
        "id": "gT9bFkCc6D4H",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108037229,
          "user_tz": -330,
          "elapsed": 603,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 5,
      "outputs": [],
      "id": "gT9bFkCc6D4H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Utility Functions"
      ],
      "metadata": {
        "id": "cRdOA57vv8QY"
      },
      "id": "cRdOA57vv8QY"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_sql(result):\n",
        "  result = result.replace(\"```sql\", \"\").replace(\"```\", \"\")\n",
        "  return result\n",
        "\n",
        "def clean_json(result):\n",
        "  result = result.replace(\"```json\", \"\").replace(\"```\", \"\").replace(\"json\", \"\")\n",
        "  return result\n",
        "\n",
        "def execute_final_sql(generated_sql):\n",
        "  df = pandas_gbq.read_gbq(generated_sql, project_id=PROJECT_ID)\n",
        "  return df"
      ],
      "metadata": {
        "id": "SX5WjVR_46Rs",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108037229,
          "user_tz": -330,
          "elapsed": 6,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "SX5WjVR_46Rs",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_results( question, generated_sql,final_exec_result_df):\n",
        "  context_prompt=f''' You are expert in summarizing results in natural language to a user's question in natural language.\n",
        "    User has asked the question : \"{question}\" and the SQL generated for the question is : \"{generated_sql}\"\n",
        "\n",
        "    By running the SQL the results are as below\n",
        "\n",
        "    {final_exec_result_df.to_markdown(index=False)}\n",
        "\n",
        "    Us the SQL results above and answer the question in natural language.\n",
        "    '''\n",
        "  context_query = model.generate_content(context_prompt, stream=False)\n",
        "  return context_query.candidates[0].text\n"
      ],
      "metadata": {
        "id": "JFzYjo8GVvz-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108037229,
          "user_tz": -330,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "JFzYjo8GVvz-",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Query Build"
      ],
      "metadata": {
        "id": "FcgtGsAcwacm"
      },
      "id": "FcgtGsAcwacm"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title RAG Based SQL Generator\n",
        "def gen_dyn_rag_sql(question,table_result_joined,column_result_joined,similar_questions):\n",
        "  global FULL_LOG_TEXT\n",
        "  not_related_msg='select \\'Question is not related to the dataset\\' as unrelated_answer from dual;'\n",
        "  context_prompt = f\"\"\"\n",
        "\n",
        "      You are a BigQuery SQL guru. Write a SQL comformant query for Bigquery that answers the following question while using the provided context to correctly refer to the BigQuery tables and the needed column names.\n",
        "\n",
        "      Guidelines:\n",
        "      - Only answer questions relevant to the tables listed in the table schema. If a non-related question comes, answer exactly: {not_related_msg}\n",
        "      - Join as minimal tables as possible.\n",
        "      - When joining tables ensure all join columns are the same data_type.\n",
        "      - Analyze the database and the table schema provided as parameters and undestand the relations (column and table relations).\n",
        "      - Consider alternative options to CAST function. If performing a CAST, use only Bigquery supported datatypes.\n",
        "      - Don't include any comments in code.\n",
        "      - Remove ```sql and ``` from the output and generate the SQL in single line.\n",
        "      - Tables should be refered to using a fully qualified name (project_id.owner.table_name).\n",
        "      - Use all the non-aggregated columns from the \"SELECT\" statement while framing \"GROUP BY\" block.\n",
        "      - Return syntactically and symantically correct SQL for BigQuery with proper relation mapping i.e project_id, owner, table and column relation.\n",
        "      - Use ONLY the column names (column_name) mentioned in Table Schema. DO NOT USE any other column names outside of this.\n",
        "      - Associate column_name mentioned in Table Schema only to the table_name specified under Table Schema.\n",
        "      - Use SQL 'AS' statement to assign a new name temporarily to a table column or even a table wherever needed.\n",
        "      - Table names are case sensitive. DO NOT uppercase or lowercase the table names.\n",
        "\n",
        "    Table Schema:\n",
        "    {table_result_joined}\n",
        "\n",
        "    Column Description:\n",
        "    {column_result_joined}\n",
        "\n",
        "    Question/SQL Generated Examples:\n",
        "\n",
        "    {similar_questions}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    SQL Generated:\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "  print('\\n LLM GEN SQL Prompt: \\n' + context_prompt)\n",
        "  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n LLM GEN SQL Prompt:  ... \\n'\n",
        "  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + context_prompt + '\\n'\n",
        "\n",
        "\n",
        "  if model_id =='code-bison-32k':\n",
        "      context_query = model.predict(context_prompt, max_output_tokens = 1024, temperature=0)\n",
        "  elif model_id =='gemini-pro':\n",
        "      context_query = model.generate_content(context_prompt, stream=False)\n",
        "  else:\n",
        "    raise ValueError('model_id not found')\n",
        "\n",
        "  resp_return = clean_sql(str(context_query.candidates[0].text))\n",
        "\n",
        "  print(clean_sql(str(context_query.candidates[0].text)))\n",
        "\n",
        "  return resp_return\n",
        "\n"
      ],
      "metadata": {
        "id": "aSQzpVcCWKxd",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108037229,
          "user_tz": -330,
          "elapsed": 5,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "cellView": "form"
      },
      "id": "aSQzpVcCWKxd",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Validate Syntax of SQL\n",
        "def validate_sql_syntax(question, generated_sql, table_result_joined,column_result_joined,similar_questions):\n",
        "\n",
        "  #print(columns_df.to_markdown(index = False))\n",
        "  s = 'error'\n",
        "\n",
        "  context_prompt = f\"\"\"\n",
        "\n",
        "    Classify the SQL query (generated sql) as valid or invalid.\n",
        "\n",
        "    Instructions:\n",
        "    - Use ONLY the column names (column_name) mentioned in Table Schema.\n",
        "    - DO NOT USE any other column names outside the schema in this context.\n",
        "    - To be considered valid, a SQL must be semantically correct, use correct ANSI SQL syntax and answer the question below.\n",
        "    - Respond using a valid JSON format with only two elements: valid and errors. Remove ```json and ``` from the output\n",
        "    - JSON must be valid. In the JSON data format, the keys must be enclosed in double quotes. Document must start with LEFT CURLY BRACKET character and end with the RIGHT CURLY BRACKET character\n",
        "\n",
        "    Table Schema:\n",
        "    {table_result_joined}\n",
        "\n",
        "    Columns Description:\n",
        "    {column_result_joined}\n",
        "\n",
        "    Examples:\n",
        "    {similar_questions}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    SQL Query:\n",
        "    {generated_sql}\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # print(context_prompt)\n",
        "  if model_id =='code-bison-32k':\n",
        "      context_query = model.predict(context_prompt, max_output_tokens = 1024, temperature=0)\n",
        "  elif model_id =='gemini-pro':\n",
        "      context_query = model.generate_content(context_prompt, stream=False)\n",
        "  else:\n",
        "    raise ValueError('model_id not found')\n",
        "\n",
        "  resp_return = clean_json(clean_sql(str(context_query.candidates[0].text)))\n",
        "\n",
        "  # print(clean_json(clean_sql(str(context_query.candidates[0].text))))\n",
        "\n",
        "  return resp_return\n"
      ],
      "metadata": {
        "id": "t3Ptnl9UWHa8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108037230,
          "user_tz": -330,
          "elapsed": 6,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "cellView": "form"
      },
      "id": "t3Ptnl9UWHa8",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Test SQL Plan\n",
        "def test_sql_plan_execution(generated_sql):\n",
        "  from google.cloud import bigquery\n",
        "  try:\n",
        "\n",
        "    run_dataset=PROJECT_ID + '.' + DATASET_NAME\n",
        "    df=pd.DataFrame()\n",
        "\n",
        "    # Construct a BigQuery client object.\n",
        "    client = bigquery.Client(project=PROJECT_ID)\n",
        "\n",
        "    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)\n",
        "\n",
        "    # Start the query, passing in the extra configuration.\n",
        "    query_job = client.query(\n",
        "        (generated_sql),\n",
        "        job_config=job_config,\n",
        "    )  # Make an API request.\n",
        "\n",
        "    # A dry run query completes immediately.\n",
        "    print(\"This query will process {} bytes.\".format(query_job.total_bytes_processed))\n",
        "    return 'Execution Plan OK'\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    msg='Error. Message: '+ str(e)\n",
        "    return msg\n",
        "\n"
      ],
      "metadata": {
        "id": "Jo7S6zYJWAcc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108037418,
          "user_tz": -330,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "cellView": "form"
      },
      "id": "Jo7S6zYJWAcc",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Chat Model INIT for retrys\n",
        "def init_chat():\n",
        "  global FULL_LOG_TEXT\n",
        "  not_related_msg='select \\'Question is not related to the dataset\\' as unrelated_answer from dual;'\n",
        "\n",
        "  context_prompt = f\"\"\"\n",
        "\n",
        "    You are a BigQuery SQL guru. This session is trying to troubleshoot a Google BigQuery SQL query.\n",
        "    As the user provides versions of the query and the errors returned by BigQuery,\n",
        "    return a never seen alternative SQL query that fixes the errors.\n",
        "    It is important that the query still answer the original question.\n",
        "\n",
        "      Guidelines:\n",
        "      - Only answer questions relevant to the tables listed in the table schema. If a non-related question comes, answer exactly: {not_related_msg}\n",
        "      - Join as minimal tables as possible.\n",
        "      - When joining tables ensure all join columns are the same data_type.\n",
        "      - Analyze the database and the table schema provided as parameters and undestand the relations (column and table relations).\n",
        "      - Consider alternative options to CAST function. If performing a CAST, use only Bigquery supported datatypes.\n",
        "      - Don't include any comments in code.\n",
        "      - Remove ```sql and ``` from the output and generate the SQL in single line.\n",
        "      - Tables should be refered to using a fully qualified name (project_id.owner.table_name).\n",
        "      - Use all the non-aggregated columns from the \"SELECT\" statement while framing \"GROUP BY\" block.\n",
        "      - Return syntactically and symantically correct SQL for BigQuery with proper relation mapping i.e project_id, owner, table and column relation.\n",
        "      - Use ONLY the column names (column_name) mentioned in Table Schema. DO NOT USE any other column names outside of this.\n",
        "      - Associate column_name mentioned in Table Schema only to the table_name specified under Table Schema.\n",
        "      - Use SQL 'AS' statement to assign a new name temporarily to a table column or even a table wherever needed.\n",
        "      - Table names are case sensitive. DO NOT uppercase or lowercase the table names.\n",
        "\n",
        "  \"\"\"\n",
        "  print('\\n Initializing code chat model ...')\n",
        "  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Initializing code chat model ... \\n'\n",
        "  chat_session = chat_model.start_chat(context=context_prompt)\n",
        "  #chat_session = chat_model.start_chat(context=\"\")\n",
        "  # context_prompt\n",
        "  return chat_session\n"
      ],
      "metadata": {
        "id": "7mWsuOfHV61h",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108039116,
          "user_tz": -330,
          "elapsed": 6,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "cellView": "form"
      },
      "id": "7mWsuOfHV61h",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Vector Loop Up for RAG\n",
        "def get_tables_columns_bqvector(question):\n",
        "  table_sql=f\"\"\"  SELECT base.idx, base.detailed_description,distance\n",
        "FROM VECTOR_SEARCH(\n",
        "  TABLE `ss-genai-npd-svc-prj-01.nl2sql.table_comments_embeddings`, \"ml_generate_embedding_result\",\n",
        "  (\n",
        "  SELECT ml_generate_embedding_result, content AS query\n",
        "  FROM ML.GENERATE_EMBEDDING(\n",
        "  MODEL `ss-genai-npd-svc-prj-01.nl2sql.embedding_model`,\n",
        "  (SELECT \"{question}\" AS content))\n",
        "  ),\n",
        "  top_k => 5,distance_type=>\"COSINE\" )\"\"\"\n",
        "\n",
        "  column_sql=f\"\"\"  SELECT base.idx, base.detailed_description,distance\n",
        "FROM VECTOR_SEARCH(\n",
        "  TABLE `ss-genai-npd-svc-prj-01.nl2sql.column_comments_embeddings`,\n",
        "  \"ml_generate_embedding_result\",\n",
        "  (\n",
        "  SELECT ml_generate_embedding_result, content AS query\n",
        "  FROM ML.GENERATE_EMBEDDING(\n",
        "  MODEL `ss-genai-npd-svc-prj-01.nl2sql.embedding_model`,\n",
        "  (SELECT \"{question}\" AS content))\n",
        "  ),\n",
        "  top_k => 5,distance_type=>\"COSINE\" )\"\"\"\n",
        "\n",
        "\n",
        "  table_results_joined=\"\"\n",
        "  column_results_joined=\"\"\n",
        "\n",
        "  tables_df=pandas_gbq.read_gbq(table_sql,project_id=PROJECT_ID)\n",
        "  columns_df=pandas_gbq.read_gbq(column_sql,project_id=PROJECT_ID)\n",
        "\n",
        "  for index, row in tables_df.iterrows():\n",
        "    table_results_joined = table_results_joined + row[\"detailed_description\"] + ' \\n'\n",
        "\n",
        "  for index, row in columns_df.iterrows():\n",
        "    column_results_joined = column_results_joined + row[\"detailed_description\"] + '\\n'\n",
        "\n",
        "  return table_results_joined,column_results_joined"
      ],
      "metadata": {
        "id": "YtlVmKI2207c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108040956,
          "user_tz": -330,
          "elapsed": 3,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "YtlVmKI2207c",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exact same question match search\n",
        "def search_sql_vector_by_id(question):\n",
        "    global FULL_LOG_TEXT\n",
        "    msg=''\n",
        "    sql=f\"\"\"select generated_sql from `ss-genai-npd-svc-prj-01.nl2sql.sql_embeddings` where\n",
        "    idx=to_hex(md5(\"{question}\"))\"\"\"\n",
        "    results=pandas_gbq.read_gbq(sql,project_id=PROJECT_ID)\n",
        "    # print(results)\n",
        "    if results.empty:\n",
        "        FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n SQL Not Found in Vector DB. \\n'\n",
        "        msg='SQL Not Found in Vector DB'\n",
        "\n",
        "    for index,row in results.iterrows():\n",
        "        print('\\n Record found in Vector DB. Parameters: \\n')\n",
        "        FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Record found in Vector DB. Parameters: \\n'\n",
        "        #print(r[0])\n",
        "        msg=str(row[\"generated_sql\"])\n",
        "    return msg"
      ],
      "metadata": {
        "id": "vAEZczRW6w0o",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108043076,
          "user_tz": -330,
          "elapsed": 197,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "vAEZczRW6w0o",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Similar Question search\n",
        "def search_sql_nearest_vector(question,):\n",
        "    global FULL_LOG_TEXT\n",
        "\n",
        "    msg=''\n",
        "    sql=f\"\"\" SELECT base.question, base.generated_sql\n",
        "FROM VECTOR_SEARCH(\n",
        "  TABLE `ss-genai-npd-svc-prj-01.nl2sql.sql_embeddings`, \"ml_generate_embedding_result\",\n",
        "  (\n",
        "  SELECT ml_generate_embedding_result, content AS query\n",
        "  FROM ML.GENERATE_EMBEDDING(\n",
        "  MODEL `ss-genai-npd-svc-prj-01.nl2sql.embedding_model`,\n",
        "  (SELECT \"{question}\" AS content))\n",
        "  ),\n",
        "  top_k => 3,distance_type=>\"COSINE\" )\n",
        "\n",
        "    \"\"\"\n",
        "    results=pandas_gbq.read_gbq(sql,project_id=PROJECT_ID)\n",
        "    if results.empty:\n",
        "     msg='SQL Nearest Not Found in Vector DB'\n",
        "     print('\\n No record near the query was found in the Vector DB. \\n')\n",
        "     FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n No record near the query was found in the Vector DB. \\n'\n",
        "\n",
        "    for index,row in results.iterrows():\n",
        "      msg=msg + '\\nQuestion:' + str(row[\"question\"]) + '\\n' + 'Generated SQL:' + str(row[\"generated_sql\"]) + '\\n'\n",
        "\n",
        "    return msg"
      ],
      "metadata": {
        "id": "_9mkyY78-OEZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108044444,
          "user_tz": -330,
          "elapsed": 4,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "_9mkyY78-OEZ",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Retry SQL generation with Chat Model\n",
        "def rewrite_sql_chat(chat_session, question, generated_sql, table_result_joined , column_result_joined, error_msg, similar_questions):\n",
        "\n",
        "  context_prompt = f\"\"\"\n",
        "    What is an alternative SQL statement to address the error mentioned below?\n",
        "    Present a different SQL from previous ones. It is important that the query still answer the original question.\n",
        "    Do not repeat suggestions.\n",
        "\n",
        "  Question:\n",
        "  \"{question}\"\n",
        "\n",
        "  Previously Generated (bad) SQL Query:\n",
        " \" {generated_sql}\"\n",
        "\n",
        "  Error Message:\n",
        "  {error_msg}\n",
        "\n",
        "  Table Schema:\n",
        "  {table_result_joined}\n",
        "\n",
        "  Column Description:\n",
        "  {column_result_joined}\n",
        "\n",
        "  Good SQL Examples:\n",
        "  {similar_questions}\n",
        "  \"\"\"\n",
        "\n",
        "  #Column Descriptions:\n",
        "  #{column_result_joined}\n",
        "\n",
        "\n",
        "  if chat_model_id =='codechat-bison-32k':\n",
        "      response = chat_session.send_message(context_prompt)\n",
        "      resp_return = clean_sql(str(response.candidates[0]))\n",
        "  elif chat_model_id =='gemini-pro':\n",
        "      response = chat_session.send_message(context_prompt, stream=False)\n",
        "      resp_return = clean_sql(str(response.text))\n",
        "\n",
        "  else:\n",
        "    raise ValueError('model_id not found')\n",
        "\n",
        "  print(resp_return)\n",
        "\n",
        "  return resp_return\n"
      ],
      "metadata": {
        "id": "mOgE24HIXQUM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108046578,
          "user_tz": -330,
          "elapsed": 239,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "mOgE24HIXQUM",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Adding Good SQLs to Vector Tables\n",
        "def add_vector_sql_collection(question, final_sql):\n",
        "\n",
        "  global FULL_LOG_TEXT\n",
        "\n",
        "  sql=f'''\n",
        "      insert into `ss-genai-npd-svc-prj-01.nl2sql.sql_embeddings`\n",
        "      SELECT * FROM ML.GENERATE_EMBEDDING(\n",
        " MODEL `ss-genai-npd-svc-prj-01.nl2sql.embedding_model`,\n",
        "   (\n",
        "   select to_hex(md5(concat(question))) as idx,\n",
        "       \"{question}\" as question ,final_sql,current_datetime() as epoch_time,\n",
        "       \"{question}\" as content  from ss-genai-npd-svc-prj-01.nl2sql.sql\n",
        "  )\n",
        ");\n",
        "    '''\n",
        "\n",
        "  print(sql)\n",
        "  pandas_gbq.read_gbq(sql,project=PROJECT_ID)\n",
        "  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Record added to Vector DB. Parameters: \\n'\n",
        "  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + str(question) + '\\n'\n",
        "\n",
        "  return 'Question ' + str(question) + ' added to the Vector DB'\n",
        "\n"
      ],
      "metadata": {
        "id": "mRKtwqrMw2oA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108048843,
          "user_tz": -330,
          "elapsed": 196,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "execution_count": 16,
      "outputs": [],
      "id": "mRKtwqrMw2oA"
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Log the process on to BQ\n",
        "def append_2_bq(model, question, generated_sql, found_in_vector, need_rewrite, failure_step, error_msg):\n",
        "  global FULL_LOG_TEXT\n",
        "\n",
        "  if ENABLE_ANALYTICS is True:\n",
        "      print('\\nInside the Append to BQ block\\n')\n",
        "      table_id=PROJECT_ID + '.' + DATASET_NAME + '.' + LOG_TABLE_NAME\n",
        "      now = datetime.now()\n",
        "\n",
        "      table_exists=False\n",
        "      client = bigquery.Client()\n",
        "\n",
        "      df1 = pd.DataFrame(columns=[\n",
        "          'source_type',\n",
        "          'project_id',\n",
        "          'user',\n",
        "          'schema',\n",
        "          'model_used',\n",
        "          'question',\n",
        "          'generated_sql',\n",
        "          'found_in_vector',\n",
        "          'need_rewrite',\n",
        "          'failure_step',\n",
        "          'error_msg',\n",
        "          'execution_time',\n",
        "          'full_log'\n",
        "          ])\n",
        "\n",
        "      new_row = {\n",
        "          \"source_type\":source_type,\n",
        "          \"project_id\":str(PROJECT_ID),\n",
        "          \"user\":str(auth_user),\n",
        "          \"schema\": schema,\n",
        "          \"model_used\": model,\n",
        "          \"question\": question,\n",
        "          \"generated_sql\": generated_sql,\n",
        "          \"found_in_vector\":found_in_vector,\n",
        "          \"need_rewrite\":need_rewrite,\n",
        "          \"failure_step\":failure_step,\n",
        "          \"error_msg\":error_msg,\n",
        "          \"execution_time\": now,\n",
        "          \"full_log\": FULL_LOG_TEXT\n",
        "        }\n",
        "\n",
        "      df1.loc[len(df1)] = new_row\n",
        "\n",
        "      db_schema=[\n",
        "            # Specify the type of columns whose type cannot be auto-detected. For\n",
        "            # example the \"title\" column uses pandas dtype \"object\", so its\n",
        "            # data type is ambiguous.\n",
        "            bigquery.SchemaField(\"source_type\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"project_id\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"user\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"schema\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"model_used\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"question\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"generated_sql\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"found_in_vector\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"need_rewrite\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"failure_step\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"error_msg\", bigquery.enums.SqlTypeNames.STRING),\n",
        "            bigquery.SchemaField(\"execution_time\", bigquery.enums.SqlTypeNames.TIMESTAMP),\n",
        "            bigquery.SchemaField(\"full_log\", bigquery.enums.SqlTypeNames.STRING),\n",
        "          ]\n",
        "\n",
        "      try:\n",
        "        client.get_table(table_id)  # Make an API request.\n",
        "        #print(\"Table {} already exists.\".format(table_id))\n",
        "        table_exists=True\n",
        "      except NotFound:\n",
        "          print(\"Table {} is not found.\".format(table_id))\n",
        "          table_exists=False\n",
        "\n",
        "      if table_exists is True:\n",
        "          print('Performing streaming insert')\n",
        "          errors = client.insert_rows_from_dataframe(table=table_id, dataframe=df1, selected_fields=db_schema)  # Make an API request.\n",
        "          #if errors == []:\n",
        "          #    print(\"New rows have been added.\")\n",
        "          #else:\n",
        "          #    print(\"Encountered errors while inserting rows: {}\".format(errors))\n",
        "      else:\n",
        "          pandas_gbq.to_gbq(df1, table_id, project_id=PROJECT_ID)  # replace to replace table; append to append to a table\n",
        "\n",
        "\n",
        "      # df1.loc[len(df1)] = new_row\n",
        "      # pandas_gbq.to_gbq(df1, table_id, project_id=PROJECT_ID, if_exists='append')  # replace to replace table; append to append to a table\n",
        "      print('\\n Query added to BQ log table \\n')\n",
        "      FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Query added to BQ log table \\n'\n",
        "      return 'Row added'\n",
        "  else:\n",
        "    print('\\n BQ Analytics is disabled so query was not added to BQ log table \\n')\n",
        "    FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n BQ Analytics is disabled so query was not added to BQ log table \\n'\n",
        "\n",
        "    return 'BQ Analytics is disabled'\n"
      ],
      "metadata": {
        "id": "CyMbaNZkBbMb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108051045,
          "user_tz": -330,
          "elapsed": 181,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "CyMbaNZkBbMb",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Query call function\n",
        "\n",
        "def call_gen_sql(question):\n",
        "\n",
        "  # Reset question log variable\n",
        "  global FULL_LOG_TEXT\n",
        "  FULL_LOG_TEXT=''\n",
        "\n",
        "  # Overwriting for testing purposes\n",
        "  #INJECT_ONE_ERROR = True\n",
        "\n",
        "\n",
        "  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n User Question: \\n'\n",
        "  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + str(question) + '\\n'\n",
        "\n",
        "  # Will look into the Vector DB first and see if there is a hash match.\n",
        "  # If yes, return the known good SQL.\n",
        "  # If not, return 3 good examples to be used by the LLM\n",
        "  search_sql_vector_by_id_return=search_sql_vector_by_id(question)\n",
        "  print(\"search_sql_vector_by_id_return :\" + search_sql_vector_by_id_return)\n",
        "\n",
        "  if search_sql_vector_by_id_return == 'SQL Not Found in Vector DB':   ### Only go thru the loop if hash of the question is not found in Vector.\n",
        "\n",
        "        # Look into Vector for similar queries. Similar queries will be added to the LLM prompt (few shot examples)\n",
        "        similar_questions_return = search_sql_nearest_vector(question)\n",
        "        # print('Found Similar Questions \\n')\n",
        "        #print(search_sql_vector_by_id_return)\n",
        "        FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Found Similar Questions ... \\n'\n",
        "        FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + str(similar_questions_return) + '\\n'\n",
        "\n",
        "\n",
        "        unrelated_question=False\n",
        "        stop_loop = False\n",
        "        retry_max_count= SQL_MAX_FIX_RETRY\n",
        "        retry_count=0\n",
        "        chat_session=init_chat()\n",
        "        table_result_joined,column_result_joined=get_tables_columns_bqvector(question)\n",
        "\n",
        "        if len(table_result_joined) > 0 :\n",
        "            #print('tables from vector:' + table_result_joined + ' : ' + str(len(table_result_joined)))\n",
        "            generated_sql=gen_dyn_rag_sql(question,table_result_joined,column_result_joined, similar_questions_return)\n",
        "            print('Generated SQL:\\n' )\n",
        "            print(generated_sql)\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Generated SQL: ... \\n'\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + generated_sql + '\\n'\n",
        "            if 'unrelated_answer' in generated_sql :\n",
        "              stop_loop=True\n",
        "              #print('Inside if statement to check for unrelated question ...')\n",
        "              unrelated_question=True\n",
        "        else:\n",
        "            stop_loop=True\n",
        "            unrelated_question=True\n",
        "            print('No ANN tables found in Vector ...')\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n No ANN tables found in Vector ... \\n'\n",
        "\n",
        "\n",
        "\n",
        "        while (stop_loop is False):\n",
        "\n",
        "            ### Syntax validation via LLM block\n",
        "            print('\\n Will call PALM next to validate the generated SQL ... \\n')\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Will call PALM next to validate the generated SQL ... \\n'\n",
        "            #FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + generated_sql + '\\n'\n",
        "            valid_sql_return=validate_sql_syntax(question, generated_sql, table_result_joined , column_result_joined,similar_questions_return)\n",
        "            print('Return JSON from validation: ' + valid_sql_return)\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n  Return JSON from validation \\n'\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + valid_sql_return + '\\n'\n",
        "\n",
        "            json_syntax_result=json.loads(valid_sql_return)\n",
        "            print('\\n SQL Syntax Validity:' + str(json_syntax_result['valid']))\n",
        "            print('\\n SQL Syntax Error Description:' +str(json_syntax_result['errors']) + '\\n')\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n SQL Syntax Validity: \\n'\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + str(json_syntax_result['valid']) + '\\n'\n",
        "\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n SQL Syntax Error Description: \\n'\n",
        "            FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + str(json_syntax_result['errors']) + '\\n'\n",
        "\n",
        "            if json_syntax_result['valid'] is True:   # LLM indicated the syntax is valid\n",
        "\n",
        "              print('\\n Testing code execution by performing explain plan on SQL ... \\n')\n",
        "              FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Testing code execution by performing explain plan on SQL ...: \\n'\n",
        "              #FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + str(json_syntax_result['errors']) + '\\n'\n",
        "\n",
        "              if INJECT_ONE_ERROR is True:\n",
        "                if retry_count < 1:\n",
        "                  print('\\n Injecting error on purpose to test code ... Adding ROWID at the end of the string\\n')\n",
        "                  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Injecting error on purpose to test code ... Adding ROWID at the end of the string \\n'\n",
        "                  generated_sql=generated_sql + ' ROWID'\n",
        "\n",
        "              sql_plan_test_result=test_sql_plan_execution(generated_sql) # Calling explain plan\n",
        "              FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Calling explain plan on SQL ...: \\n'\n",
        "              FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + str(sql_plan_test_result) + '\\n'\n",
        "\n",
        "\n",
        "              if sql_plan_test_result == 'Execution Plan OK':  # Explain plan is OK\n",
        "\n",
        "                stop_loop = True\n",
        "\n",
        "                FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Execution plan came back OK ...: \\n'\n",
        "\n",
        "                if EXECUTE_FINAL_SQL is True:\n",
        "                  final_exec_result_df=execute_final_sql(generated_sql)\n",
        "                  print('\\n Question: ' + question + '\\n')\n",
        "                  print('\\n Final SQL Execution Result: \\n')\n",
        "                  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Final SQL Execution Result ... Question: \\n'\n",
        "                  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + question + '\\n'\n",
        "                  print(final_exec_result_df)\n",
        "                  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + str(final_exec_result_df) + '\\n'\n",
        "                  if AUTO_ADD_KNOWNGOOD_SQL is True:  #### Adding to the Known Good SQL Vector DB\n",
        "                    if len(final_exec_result_df) >= 1:\n",
        "                      if not \"ORA-\" in str(final_exec_result_df.iloc[0,0]):\n",
        "                          print('\\n Adding Known Good SQL to Vector DB ... \\n')\n",
        "                          FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Adding Known Good SQL to Vector DB ... \\n'\n",
        "                          # add_vector_sql_collection_return=add_vector_sql_collection( question, generated_sql)\n",
        "                      else:\n",
        "                          ### Need to call retry\n",
        "                          stop_loop = False\n",
        "                          rewrite_result=rewrite_sql_chat(chat_session, question, generated_sql, table_result_joined , column_result_joined, str(final_exec_result_df.iloc[0,0]) ,similar_questions_return)\n",
        "                          print('\\n Rewritten SQL:')\n",
        "                          print(rewrite_result)\n",
        "                          FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Rewritten SQL: \\n'\n",
        "                          FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + rewrite_result + '\\n'\n",
        "                          generated_sql=rewrite_result\n",
        "                          retry_count+=1\n",
        "\n",
        "\n",
        "                else:  # Do not execute final SQL\n",
        "                  print(\"Not executing final SQL since EXECUTE_FINAL_SQL variable is False\\n \")\n",
        "                  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Not executing final SQL since EXECUTE_FINAL_SQL variable is False \\n'\n",
        "\n",
        "\n",
        "                appen_2_bq_result=append_2_bq(model_id, question, generated_sql, 'N', 'N', '', '')\n",
        "\n",
        "              else:  # Failure on explain plan execution\n",
        "                  print(\"Error on explain plan execution: \\n \" + sql_plan_test_result)\n",
        "                  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Error on explain plan execution \\n'\n",
        "                  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + sql_plan_test_result + '\\n'\n",
        "\n",
        "                  append_2_bq_result=append_2_bq(model_id, question, generated_sql, 'N', 'Y', 'explain_plan_validation', sql_plan_test_result )\n",
        "                  ### Need to call retry\n",
        "                  rewrite_result=rewrite_sql_chat(chat_session, question, generated_sql, table_result_joined , column_result_joined, sql_plan_test_result,similar_questions_return)\n",
        "                  print('\\n Rewritten SQL:')\n",
        "                  print(rewrite_result)\n",
        "                  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Rewritten SQL: \\n'\n",
        "                  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + rewrite_result + '\\n'\n",
        "                  generated_sql=rewrite_result\n",
        "                  retry_count+=1\n",
        "\n",
        "            else:  # syntax validation returned False\n",
        "              print('Syntax Error')\n",
        "              FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Syntax Error found ... \\n'\n",
        "              append_2_bq_result=append_2_bq(model_id, question, generated_sql, 'N', 'Y', 'syntax_validation', str(json_syntax_result['errors']))\n",
        "              ### Need to call retry\n",
        "              rewrite_result=rewrite_sql_chat(chat_session, question, generated_sql, table_result_joined , column_result_joined, str(json_syntax_result['errors']) , similar_questions_return)\n",
        "              print('\\n Rewritten SQL:')\n",
        "              print(rewrite_result)\n",
        "              FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Rewritten SQL: \\n'\n",
        "              FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + rewrite_result + '\\n'\n",
        "              generated_sql=rewrite_result\n",
        "              retry_count+=1\n",
        "\n",
        "            if retry_count > retry_max_count:\n",
        "              stop_loop = True\n",
        "\n",
        "        # After the while is completed\n",
        "        if retry_count > retry_max_count:\n",
        "          print('\\n Oopss!!! Could not find a good SQL. This is the best I came up with !!!!! \\n')\n",
        "          print(generated_sql)\n",
        "          FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Oopss!!! Could not find a good SQL. This is the best I came up with !!!!! \\n'\n",
        "          FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + generated_sql + '\\n'\n",
        "\n",
        "\n",
        "        # If query is unrelated to the dataset\n",
        "        if unrelated_question is True:\n",
        "          print('\\n Question cannot be answered using this dataset! \\n')\n",
        "          FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Question cannot be answered using this dataset! \\n'\n",
        "          append_2_bq_result=append_2_bq(model_id, question, 'Question cannot be answered using this dataset!', 'N', 'N', 'unrelated_question', '')\n",
        "\n",
        "          #print(generated_sql)\n",
        "\n",
        "  else:   ## Found the record on vector id\n",
        "    #print('\\n Found Question in Vector. Returning the SQL')\n",
        "    if EXECUTE_FINAL_SQL is True:\n",
        "        final_exec_result_df=execute_final_sql(search_sql_vector_by_id_return)\n",
        "        print('\\n Question: ' + question + '\\n')\n",
        "        print('\\n Final SQL Execution Result: \\n')\n",
        "        print(final_exec_result_df)\n",
        "        FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Final SQL Execution Result ... Question: \\n'\n",
        "        FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + question + '\\n'\n",
        "        FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n' + str(final_exec_result_df) + '\\n'\n",
        "\n",
        "    else:  # Do not execute final SQL\n",
        "        print(\"Not executing final SQL since EXECUTE_FINAL_SQL variable is False\\n \")\n",
        "        FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n Not executing final SQL since EXECUTE_FINAL_SQL variable is False \\n'\n",
        "    print('will call append to bq next')\n",
        "    FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n will call append to bq next \\n'\n",
        "    appen_2_bq_result=append_2_bq(model_id, question, search_sql_vector_by_id_return, 'Y', 'N', '', '')\n",
        "  final_answer=summarize_results( question, genefinal_exec_result_dfrated_sql,)\n",
        "  print('\\n Final Summarized Answer: \\n')\n",
        "  print(final_answer)\n",
        "  FULL_LOG_TEXT= FULL_LOG_TEXT + '\\n All Done! \\n'\n",
        "  return \"\\n All Done!\", generated_sql"
      ],
      "metadata": {
        "id": "mmNNCDIB6ceb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1709108182358,
          "user_tz": -330,
          "elapsed": 210,
          "user": {
            "displayName": "",
            "userId": ""
          }
        }
      },
      "id": "mmNNCDIB6ceb",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ask here"
      ],
      "metadata": {
        "id": "DLZ3kHijxhyg"
      },
      "id": "DLZ3kHijxhyg"
    },
    {
      "cell_type": "code",
      "source": [
        "# question=\"Display the top 5 users by the  most total number of orders. Include the first name and the number of orders.\"\n",
        "question = \"What are the customer details of Abhinav Saxena who is of age 27?\"\n",
        "# question = \"Show me summary analytics for Abhinav Saxena who is age 27\"\n",
        "start = time.time()\n",
        "call_gen_result, generated_sql=call_gen_sql(question)\n",
        "end = time.time()\n",
        "print(call_gen_result)\n",
        "print('\\n Entire flow (including SQL execution) was executed in '+ str(end - start) + ' seconds') # time in seconds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "xMTBnPhMX6IA",
        "executionInfo": {
          "status": "error",
          "timestamp": 1709112119151,
          "user_tz": -330,
          "elapsed": 8106,
          "user": {
            "displayName": "",
            "userId": ""
          }
        },
        "outputId": "40362a43-48f4-46c0-d579-de544226822d"
      },
      "id": "xMTBnPhMX6IA",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: |\u001b[32m          \u001b[0m|\n",
            "search_sql_vector_by_id_return :SQL Not Found in Vector DB\n",
            "Downloading: |\u001b[32m          \u001b[0m|\n",
            "\n",
            " No record near the query was found in the Vector DB. \n",
            "\n",
            "\n",
            " Initializing code chat model ...\n",
            "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "GenericGBQException",
          "evalue": "Reason: 400 Array inputs are not equal in length; error in ML.DISTANCE expression\n\nLocation: asia-southeast1\nJob ID: 3f489fbd-b756-41d0-a0b6-e4312b4e6b5c\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequest\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, max_results, progress_bar_type, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m             \u001b[0mquery_reply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, page_size, max_results, retry, timeout, start_index, job_retry)\u001b[0m\n\u001b[1;32m   1580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m             \u001b[0mdo_get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m             )\n\u001b[0;32m--> 349\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/job/query.py\u001b[0m in \u001b[0;36mdo_get_result\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1571\u001b[0;31m                 \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQueryJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/cloud/bigquery/job/base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, retry, timeout)\u001b[0m\n\u001b[1;32m    921\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mDEFAULT_RETRY\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"retry\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_AsyncJob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;31m# Pylint doesn't recognize that this is valid in this case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequest\u001b[0m: 400 Array inputs are not equal in length; error in ML.DISTANCE expression\n\nLocation: asia-southeast1\nJob ID: 3f489fbd-b756-41d0-a0b6-e4312b4e6b5c\n",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGenericGBQException\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-b31cdda9751b>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# question = \"Show me summary analytics for Abhinav Saxena who is age 27\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcall_gen_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated_sql\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcall_gen_sql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_gen_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-b8ee9ed4bb2b>\u001b[0m in \u001b[0;36mcall_gen_sql\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mretry_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mchat_session\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minit_chat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mtable_result_joined\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolumn_result_joined\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_tables_columns_bqvector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_result_joined\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-872f1042ae1d>\u001b[0m in \u001b[0;36mget_tables_columns_bqvector\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mtables_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_gbq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_gbq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable_sql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPROJECT_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mcolumns_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpandas_gbq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_gbq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_sql\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mproject_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPROJECT_ID\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtables_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mread_gbq\u001b[0;34m(query_or_table, project_id, index_col, col_order, reauth, auth_local_webserver, dialect, location, configuration, credentials, use_bqstorage_api, max_results, verbose, private_key, progress_bar_type, dtypes, auth_redirect_uri, client_id, client_secret)\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_or_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         final_df = connector.run_query(\n\u001b[0m\u001b[1;32m    944\u001b[0m             \u001b[0mquery_or_table\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0mconfiguration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfiguration\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mrun_query\u001b[0;34m(self, query, max_results, progress_bar_type, **kwargs)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mquery_reply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_error\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_http_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;31m# Avoid attempting to download results from DML queries, which have no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas_gbq/gbq.py\u001b[0m in \u001b[0;36mprocess_http_error\u001b[0;34m(ex)\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTableCreationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Reason: {error_message}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGenericGBQException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reason: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     def download_table(\n",
            "\u001b[0;31mGenericGBQException\u001b[0m: Reason: 400 Array inputs are not equal in length; error in ML.DISTANCE expression\n\nLocation: asia-southeast1\nJob ID: 3f489fbd-b756-41d0-a0b6-e4312b4e6b5c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3MiWVBa7zBnT"
      },
      "id": "3MiWVBa7zBnT",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}