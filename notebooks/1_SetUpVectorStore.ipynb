{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div style=\"display: flex; align-items: left;\">\n",
        "    <a href=\"https://sites.google.com/corp/google.com/genai-solutions/home?authuser=0\">\n",
        "        <img src=\"https://storage.googleapis.com/miscfilespublic/Linkedin%20Banner%20%E2%80%93%202.png\" style=\"margin-right\">\n",
        "    </a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copyright"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRyGcAepAPJ5"
      },
      "source": [
        "\n",
        "# **Open Data QnA: Vector Store Setup**\n",
        "\n",
        "---\n",
        "\n",
        "This notebook walks through the Vector Store Setup needed for running the Open Data QnA application. \n",
        "\n",
        "Currently supported Source DBs are: \n",
        "- PostgreSQL on Google Cloud SQL \n",
        "- BigQuery\n",
        "\n",
        "Furthermore, the following vector stores are supported \n",
        "- pgvector on PostgreSQL \n",
        "- BigQuery vector\n",
        "\n",
        "\n",
        "The notebook covers the following steps: \n",
        "> 1. Configuration: Intial GCP project, IAM permissions, Environment  and Databases setup including logging on Bigquery for analytics\n",
        "\n",
        "> 2. Creation of Table, Column and Known Good Query Embeddings in the Vector Store  for Retreival Augmented Generation(RAG)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsWGZW_fUJjN"
      },
      "source": [
        "### üìí Using this interactive notebook\n",
        "\n",
        "If you have not used this IDE with jupyter notebooks it will ask for installing Python + Jupyter extensions. Please go ahead install them\n",
        "\n",
        "Click the **run** icons ‚ñ∂Ô∏è  of each cell within this notebook.\n",
        "\n",
        "> üí° Alternatively, you can run the currently selected cell with `Ctrl + Enter` (or `‚åò + Enter` on a Mac).\n",
        "\n",
        "> ‚ö†Ô∏è **To avoid any errors**, wait for each section to finish in their order before clicking the next ‚Äúrun‚Äù icon.\n",
        "\n",
        "This sample must be connected to a **Google Cloud project**, but nothing else is needed other than your Google Cloud project.\n",
        "\n",
        "You can use an existing project. Alternatively, you can create a new Cloud project [with cloud credits for free.](https://cloud.google.com/free/docs/gcp-free-tier)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RicDCkdI-hmp"
      },
      "source": [
        "## üöß **0. Pre-requisites**\n",
        "\n",
        "Make sure that Google Cloud CLI is installed before moving to the next cell! You can refer to the link below for guidance\n",
        "\n",
        "Installation Guide: https://cloud.google.com/sdk/docs/install"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  **1. Setup GCP Project and Environment** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4W6FPnrYEE8"
      },
      "source": [
        "### üîó **1A. Connect Your Google Cloud Project**\n",
        "Time to connect your Google Cloud Project to this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@markdown Please fill in the value below with your GCP project ID and then run the cell.\n",
        "PROJECT_ID = input(\"Please enter the GCP Project ID\")\n",
        "\n",
        "# Quick input validations.\n",
        "assert PROJECT_ID, \"‚ö†Ô∏è Please provide your Google Cloud Project ID\"\n",
        "\n",
        "# Configure gcloud.\n",
        "!gcloud config set project {PROJECT_ID}\n",
        "print(f'Project has been set to {PROJECT_ID}')\n",
        "# !gcloud auth application-default set-quota-project {PROJECT_ID}\n",
        "\n",
        "import os\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "\n",
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(module_path+'/config.ini')\n",
        "\n",
        "config['GCP']['PROJECT_ID'] = PROJECT_ID\n",
        "\n",
        "with open(module_path+'/config.ini', 'w') as configfile:    # save\n",
        "    config.write(configfile)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yygMe6rPWxHS"
      },
      "source": [
        "### üîê **1B. Authenticate to Google Cloud**\n",
        "Authenticate to Google Cloud as the IAM user logged into this notebook in order to access your Google Cloud Project.\n",
        "You can do this within Google Colab or using the Application Default Credentials in the Google Cloud CLI.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"Colab Auth\"\"\" \n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "\n",
        "\n",
        "\"\"\"Google CLI Auth\"\"\"\n",
        "# !gcloud auth application-default login\n",
        "\n",
        "\n",
        "import google.auth\n",
        "credentials, project_id = google.auth.default()\n",
        "# credentials = google.auth.credentials.with_scopes_if_required(credentials)\n",
        "# authed_http = google.auth.transport.requests.AuthorizedSession(credentials)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚öôÔ∏è **1C. Enable Required API Services in the GCP Project**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Enable all the required APIs for the Open Data QnA solution\n",
        "\n",
        "!gcloud services enable \\\n",
        "  cloudapis.googleapis.com \\\n",
        "  cloudbuild.googleapis.com \\\n",
        "  cloudresourcemanager.googleapis.com \\\n",
        "  compute.googleapis.com \\\n",
        "  container.googleapis.com \\\n",
        "  containerregistry.googleapis.com \\\n",
        "  iam.googleapis.com \\\n",
        "  run.googleapis.com \\\n",
        "  sqladmin.googleapis.com \\\n",
        "  aiplatform.googleapis.com \\\n",
        "  artifactregistry.googleapis.com \\\n",
        "  bigquery.googleapis.com \\\n",
        "  firebase.googleapis.com \\\n",
        "  monitoring.googleapis.com \\\n",
        "  serviceusage.googleapis.com \\\n",
        "  storage.googleapis.com \\\n",
        "  orgpolicy.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CqBnkLLCDFz"
      },
      "source": [
        "### üíª **1D. Install Code Dependencies**\n",
        "Install the dependencies by runnign the poetry commands below \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install poetry\n",
        "! pip uninstall poetry -y\n",
        "! pip install poetry --quiet\n",
        "\n",
        "#Run the poetry commands below to set up the environment\n",
        "!poetry lock #resolve dependecies (also auto create poetry venv if not exists)\n",
        "!poetry install #installs dependencies\n",
        "!poetry env info #Displays the evn just created and the path to it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìå **Important Step: Activate your virtual environment**\n",
        "\n",
        "Check to ensure that Jupyter Kernel is set to the enviromnet just created by poetry\n",
        "\n",
        "Inorder to set the kernel to poetry env, do one of the below\n",
        "1. Run 'poetry shell' directly in the terminal (or)\n",
        "2. Manually select the python interpreter in your IDE\n",
        "\n",
        "\n",
        "Don't forget to switch your notebook kernel to the newly generated .venv environment after running the poetry command.\n",
        "\n",
        "If you cannot find it manually select the Python Interpreter path that you see when you run poetry shell (e.g. /home/admin_/Talk2Data/.venv/bin/python)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Jupyter Kernel Env verification\n",
        "verified_jupyter_kernel = False\n",
        "assert verified_jupyter_kernel, \"‚ö†Ô∏è Please ensure that the Jupyter Kernel is set to the evn created by poetry and change verified_jupyter_kernel to True before proceeding\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìà **1E. Set Up your Data Source and Vector Store**\n",
        "\n",
        "This section assumes that a datasource is already set up in your GCP project. If a datasource has not been set up, use the notebooks below to copy a public data set from BigQuery to Cloud SQL or BigQeury on your GCP project\n",
        "\n",
        "\n",
        "Enabled Data Sources:\n",
        "* PostgreSQL on Google Cloud SQL (Copy Sample Data: [0_CopyDataToCloudSqlPG.ipynb](0_CopyDataToCloudSqlPG.ipynb))\n",
        "* BigQuery (Copy Sample Data: [0_CopyDataToBigQuery.ipynb](0_CopyDataToBigQuery.ipynb))\n",
        "\n",
        "Enabled Vector Stores:\n",
        "* pgvector on PostgreSQL \n",
        "* BigQuery vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  **Choose Data Source and Vector Store**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data source details\n",
        "DATA_SOURCE = '' # Options: 'bigquery' and 'cloudsql-pg' i.e, PostgreSQL database on Google Cloud SQL\n",
        "\n",
        "# Please specify what you would like to use as vector store for embeddings\n",
        "VECTOR_STORE = '' # Options: 'bigquery-vector' i.e, Bigquery vector and 'cloudsql-pgvector' i.e, pgvector on PostgreSQL\n",
        "\n",
        "# If you have chosen 'cloudsql-pg' as DATA_SOURCE; provide information below\n",
        "PG_REGION = \"\" #@param {type:\"string\"}\n",
        "PG_INSTANCE = \"\"\n",
        "PG_DATABASE = \"\"\n",
        "PG_USER = \"\"\n",
        "PG_PASSWORD = \"\"\n",
        "PG_SCHEMA = '' # Name of the dataset that contains all the tables\n",
        "\n",
        "\n",
        "# If you have chosen 'bigquery' as DATA_SOURCE; provide information below\n",
        "BQ_DATASET_REGION = ''\n",
        "BQ_DATASET_NAME = ''\n",
        "\n",
        "# Input verification - Source\n",
        "assert DATA_SOURCE in {'bigquery', 'cloudsql-pg'}, \"‚ö†Ô∏è Invalid DATA_SOURCE. Must be 'bigquery' or 'cloudsql-pg'\"\n",
        "\n",
        "# Input verification - Vector Store\n",
        "assert VECTOR_STORE in {'bigquery-vector', 'cloudsql-pgvector'}, \"‚ö†Ô∏è Invalid VECTOR_STORE. Must be 'bigquery-vector' or 'cloudsql-pgvector'\"\n",
        "\n",
        "\n",
        "if DATA_SOURCE == 'bigquery':\n",
        "    assert BQ_DATASET_REGION, \"‚ö†Ô∏è Please provide the Data Set Region\"\n",
        "    assert BQ_DATASET_NAME, \"‚ö†Ô∏è Please provide the name of the dataset on Bigquery\"\n",
        "elif DATA_SOURCE == 'cloudsql-pg':\n",
        "    assert PG_REGION, \"‚ö†Ô∏è Please provide Region of the Cloud SQL Instance\"\n",
        "    assert PG_INSTANCE, \"‚ö†Ô∏è Please provide the name of the Cloud SQL Instance\"\n",
        "    assert PG_DATABASE, \"‚ö†Ô∏è Please provide the name of the PostgreSQL Database on the Cloud SQL Instance\"\n",
        "    assert PG_USER, \"‚ö†Ô∏è Please provide a username for the Cloud SQL Instance\"\n",
        "    assert PG_PASSWORD, \"‚ö†Ô∏è Please provide the Password for the PG_USER\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(module_path+'/config.ini')\n",
        "\n",
        "PROJECT_ID = config['GCP']['PROJECT_ID']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### **Authenticate and Set Quota Project for .venv**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import google.auth\n",
        "credentials, project_id = google.auth.default()\n",
        "\n",
        "import os\n",
        "os.environ['GOOGLE_CLOUD_QUOTA_PROJECT']=PROJECT_ID\n",
        "os.environ['GOOGLE_CLOUD_PROJECT']=PROJECT_ID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "####  **Database Setup for Vector Store**\n",
        "\n",
        "Create PostgreSQL Instance on CloudSQL if 'cloudsql-pgvector' is chosen as vector store\n",
        "\n",
        "Note that a PostgreSQL Instance on CloudSQL already exists if 'cloudsql-pg' is the data source. PostgreSQL Instance is created only if a different data store is chosen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@markdown Feel free to update PostgreSQL or BigQuery parameters.\n",
        "# If not updated, we will proceed with default values!\n",
        "\n",
        "# Create PostgreSQL Instance is data source is different from PostgreSQL Instance\n",
        "if VECTOR_STORE == 'cloudsql-pgvector' and DATA_SOURCE != 'cloudsql-pg':\n",
        "  # Parameters for PostgreSQL Instance\n",
        "  PG_REGION = \"us-central1\" #@param {type:\"string\"}\n",
        "  PG_INSTANCE = \"pg15-opendataqna\"\n",
        "  PG_DATABASE = \"opendataqna-db\"\n",
        "  PG_USER = \"pguser\"\n",
        "  PG_PASSWORD = \"pg123\"\n",
        "  PG_SCHEMA = 'pg-vector-store' \n",
        "\n",
        "\n",
        "  # check if Cloud SQL instance exists in the provided region\n",
        "  database_version = !gcloud sql instances describe {PG_INSTANCE} --format=\"value(databaseVersion)\"\n",
        "  if database_version[0].startswith(\"POSTGRES\"):\n",
        "    print(\"Found existing Postgres Cloud SQL Instance!\")\n",
        "  else:\n",
        "    print(\"Creating new Cloud SQL instance...\")\n",
        "    !gcloud sql instances create {PG_INSTANCE} --database-version=POSTGRES_15 \\\n",
        "      --region={PG_REGION} --cpu=1 --memory=4GB --root-password={PG_PASSWORD} \\\n",
        "      --database-flags=cloudsql.iam_authentication=On\n",
        "\n",
        "  # Create a database on the instance and a user with password\n",
        "  database_exists = !gcloud sql databases list --instance={PG_INSTANCE} | grep -z 'NAME: {PG_DATABASE}'\n",
        "  if database_exists:\n",
        "      print(\"Found existing Postgres Cloud SQL database!\")\n",
        "  else:\n",
        "      print(\"Creating new Cloud SQL database...\")\n",
        "      !gcloud sql databases create  {PG_DATABASE} --instance={PG_INSTANCE}\n",
        "  !gcloud sql users create {PG_USER} \\\n",
        "  --instance={PG_INSTANCE} \\\n",
        "  --password={PG_PASSWORD}\n",
        "\n",
        "\n",
        "# Create a new data set on Bigquery to use as Vector store; the same will be used for logging as well\n",
        "if VECTOR_STORE == 'bigquery-vector':\n",
        "  BQ_OPENDATAQNA_DATASET_NAME = \"opendataqna\" #@param {type:\"string\"} - name of the dataset in Vector Store\n",
        "  BQ_DATASET_REGION = \"europe-west9\" #@param {type:\"string\"}\n",
        "\n",
        "  from google.cloud import bigquery\n",
        "  import google.api_core \n",
        "  client=bigquery.Client(project=PROJECT_ID)\n",
        "  dataset_ref = f\"{PROJECT_ID}.{BQ_OPENDATAQNA_DATASET_NAME}\"\n",
        "\n",
        "\n",
        "  # Create the dataset if it does not exist already\n",
        "  try:\n",
        "      client.get_dataset(dataset_ref)\n",
        "      print(\"Destination Dataset exists\")\n",
        "  except google.api_core.exceptions.NotFound:\n",
        "      print(\"Cannot find the dataset hence creating.......\")\n",
        "      dataset=bigquery.Dataset(dataset_ref)\n",
        "      dataset.location=BQ_DATASET_REGION\n",
        "      client.create_dataset(dataset)\n",
        "      print(str(dataset_ref)+\" is created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üìù **1F. Set up Logging on Bigquery** \n",
        "\n",
        "If Bigquery is the vector store, the same database is used for logging. If not, a new database is created for logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@markdown Feel free to update PostgreSQL Instance Parameters.\n",
        "# If not updated, we will proceed with default values!\n",
        "\n",
        "# Set up database for logging on BigQuery if one has not been already set up for Vector Store\n",
        "if VECTOR_STORE != 'bigquery-vector':\n",
        "    BQ_OPENDATAQNA_DATASET_NAME = \"opendataqna\" #@param {type:\"string\"} - name of the dataset in Vector Store\n",
        "    BQ_DATASET_REGION = \"us-central1\" #@param {type:\"string\"}\n",
        "\n",
        "    from google.cloud import bigquery\n",
        "    import google.api_core \n",
        "\n",
        "    client=bigquery.Client(project=PROJECT_ID)\n",
        "    dataset_ref = f\"{PROJECT_ID}.{BQ_OPENDATAQNA_DATASET_NAME}\"\n",
        "\n",
        "\n",
        "    # Create the dataset if it does not exist already\n",
        "    try:\n",
        "        client.get_dataset(dataset_ref)\n",
        "        print(\"Destination Dataset for logging exists...\")\n",
        "    except google.api_core.exceptions.NotFound:\n",
        "        print(\"Cannot find the dataset hence creating.......\")\n",
        "        dataset=bigquery.Dataset(dataset_ref)\n",
        "        dataset.location=BQ_DATASET_REGION\n",
        "        client.create_dataset(dataset)\n",
        "        print(str(dataset_ref)+\" is created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üíæ **1G. Save Configuration to File** \n",
        "Save the configurations set in this notebook to  `config.ini`. The parameters from this file are used in subsequent notebooks and in various modeules in the repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "module_path = os.path.abspath(os.path.join('..'))\n",
        "\n",
        "import configparser\n",
        "config = configparser.ConfigParser()\n",
        "config.read(module_path+'/config.ini')\n",
        "\n",
        "config['GCP']['PROJECT_ID'] = PROJECT_ID\n",
        "config['CONFIG']['DATA_SOURCE'] = DATA_SOURCE\n",
        "config['CONFIG']['VECTOR_STORE'] = VECTOR_STORE\n",
        "config['BIGQUERY']['BQ_OPENDATAQNA_DATASET_NAME'] = BQ_OPENDATAQNA_DATASET_NAME\n",
        "\n",
        "# Save the parameters based on Data Source and Vector Store Choices\n",
        "if DATA_SOURCE == 'cloudsql-pg' or VECTOR_STORE == 'cloudsql-pgvector':\n",
        "    config['PGCLOUDSQL']['PG_INSTANCE'] = PG_INSTANCE\n",
        "    config['PGCLOUDSQL']['PG_DATABASE'] = PG_DATABASE\n",
        "    config['PGCLOUDSQL']['PG_USER'] = PG_USER\n",
        "    config['PGCLOUDSQL']['PG_PASSWORD'] = PG_PASSWORD\n",
        "    config['PGCLOUDSQL']['PG_REGION'] = PG_REGION\n",
        "    config['PGCLOUDSQL']['PG_SCHEMA'] = PG_SCHEMA\n",
        "\n",
        "if DATA_SOURCE := 'bigquery':\n",
        "    config['BIGQUERY']['BQ_DATASET_REGION'] = BQ_DATASET_REGION\n",
        "    config['BIGQUERY']['BQ_DATASET_NAME'] = BQ_DATASET_NAME\n",
        "\n",
        "with open(module_path+'/config.ini', 'w') as configfile:    # save\n",
        "    config.write(configfile)\n",
        "\n",
        "print('All configuration paramaters saved to file!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  **2. Create Embeddings in Vector Store for RAG** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  **2A. Create Table and Column Embeddings**\n",
        "\n",
        "In this step, table and column metadata is retreived from the data source and embeddings are generated for both"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Table and Column Embeddings\n",
        "from embeddings.retrieve_embeddings import retrieve_embeddings\n",
        "\n",
        "if DATA_SOURCE =='bigquery':\n",
        "    table_schema_embeddings, col_schema_embeddings = retrieve_embeddings(DATA_SOURCE, SCHEMA=BQ_DATASET_NAME)\n",
        "else: \n",
        "    table_schema_embeddings, col_schema_embeddings = retrieve_embeddings(DATA_SOURCE, SCHEMA=PG_SCHEMA)\n",
        "\n",
        "print(\"Table and Column embeddings are created\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üíæ **2B. Save the Table and Column Embeddings in the Vector Store**\n",
        "The table and column embeddings created in the above step are save to the Vector Store chosen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from embeddings.store_embeddings import store_schema_embeddings\n",
        "\n",
        "if VECTOR_STORE=='bigquery-vector':\n",
        "    await(store_schema_embeddings(table_details_embeddings=table_schema_embeddings, \n",
        "                                  tablecolumn_details_embeddings=col_schema_embeddings, \n",
        "                                  project_id=PROJECT_ID,\n",
        "                                  instance_name=None,\n",
        "                                  database_name=None,\n",
        "                                  schema=BQ_OPENDATAQNA_DATASET_NAME,\n",
        "                                  database_user=None,\n",
        "                                  database_password=None,\n",
        "                                  region=BQ_DATASET_REGION,\n",
        "                                  VECTOR_STORE = VECTOR_STORE\n",
        "                                  ))\n",
        "\n",
        "elif VECTOR_STORE=='cloudsql-pgvector':\n",
        "    await(store_schema_embeddings(table_details_embeddings=table_schema_embeddings, \n",
        "                                tablecolumn_details_embeddings=col_schema_embeddings, \n",
        "                                project_id=PROJECT_ID,\n",
        "                                instance_name=PG_INSTANCE,\n",
        "                                database_name=PG_DATABASE,\n",
        "                                schema=None,\n",
        "                                database_user=PG_USER,\n",
        "                                database_password=PG_PASSWORD,\n",
        "                                region=PG_REGION,\n",
        "                                VECTOR_STORE = VECTOR_STORE\n",
        "                                ))\n",
        "\n",
        "print(\"Table and Column embeddings are saved to vector store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### üóÑÔ∏è **2C. Load Known Good SQL into Vector Store**\n",
        "Known Good Queries are used to create query cache for Few shot examples. Creating a query cache is highly recommended for best outcomes! \n",
        "\n",
        "The following cell will load the Natural Language Question and Known Good SQL pairs into our Vector Store. There pairs are loaded from `known_good_sql.csv` file inside scripts folder. If you have your own Question-SQL examples, curate them in .csv file before running the cell below. \n",
        "\n",
        "If no Known Good Queries are available at this time to create query cache, you can use [3_LoadKnownGoodSQL.ipynb](3_LoadKnownGoodSQL.ipynb) to load them later!!\" Empty table for KGQ embedding will be created!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you have Known Good Queries, load them to known_good_sql.csv file; \n",
        "# These will be used as few shot examples for query generation. \n",
        "# This step is highly recommended for best outcomes!\n",
        "EXAMPLES = 'yes'# Options 'yes' or 'no'\n",
        "\n",
        "from embeddings.kgq_embeddings import setup_kgq_table\n",
        "# Delete any old tables and create a new table to KGQ embeddings\n",
        "if VECTOR_STORE=='bigquery-vector':\n",
        "    await(setup_kgq_table(project_id=PROJECT_ID,\n",
        "                          instance_name=None,\n",
        "                          database_name=None,\n",
        "                          schema=BQ_OPENDATAQNA_DATASET_NAME,\n",
        "                          database_user=None,\n",
        "                          database_password=None,\n",
        "                          region=BQ_DATASET_REGION,\n",
        "                          VECTOR_STORE = VECTOR_STORE\n",
        "                          ))\n",
        "\n",
        "elif VECTOR_STORE=='cloudsql-pgvector':\n",
        "    await(setup_kgq_table(project_id=PROJECT_ID,\n",
        "                          instance_name=PG_INSTANCE,\n",
        "                          database_name=PG_DATABASE,\n",
        "                          schema=None,\n",
        "                          database_user=PG_USER,\n",
        "                          database_password=PG_PASSWORD,\n",
        "                          region=PG_REGION,\n",
        "                          VECTOR_STORE = VECTOR_STORE\n",
        "                          ))\n",
        "\n",
        "\n",
        "if EXAMPLES == 'yes':\n",
        "    print(\"Examples are provided, creating KGQ embeddings and saving them to Vector store.....\")\n",
        "\n",
        "    import os\n",
        "    import pandas as pd\n",
        "    current_dir = os.getcwd()\n",
        "    root_dir = os.path.expanduser('~')  # Start at the user's home directory\n",
        "\n",
        "    while current_dir != root_dir:\n",
        "        for dirpath, dirnames, filenames in os.walk(current_dir):\n",
        "            config_path = os.path.join(dirpath, 'known_good_sql.csv')\n",
        "            if os.path.exists(config_path):\n",
        "                file_path = config_path  # Update root_dir to the found directory\n",
        "                break  # Stop outer loop once found\n",
        "\n",
        "        current_dir = os.path.dirname(current_dir)\n",
        "\n",
        "    print(\"Known Good SQL Found at Path :: \"+file_path)\n",
        "\n",
        "    # Load the file\n",
        "    df_kgq = pd.read_csv(file_path)\n",
        "    df_kgq = df_kgq.loc[:, [\"prompt\", \"sql\", \"database_name\"]]\n",
        "    df_kgq = df_kgq.dropna()\n",
        "\n",
        "    from embeddings.kgq_embeddings import store_kgq_embeddings\n",
        "    # Add KGQ to the vector store\n",
        "    if VECTOR_STORE=='bigquery-vector':\n",
        "        await(store_kgq_embeddings(df_kgq,\n",
        "                                   project_id=PROJECT_ID,\n",
        "                                    instance_name=None,\n",
        "                                    database_name=BQ_OPENDATAQNA_DATASET_NAME,\n",
        "                                    schema=BQ_DATASET_NAME,\n",
        "                                    database_user=None,\n",
        "                                    database_password=None,\n",
        "                                    region=BQ_DATASET_REGION,\n",
        "                                    VECTOR_STORE = VECTOR_STORE\n",
        "                                    ))\n",
        "\n",
        "    elif VECTOR_STORE=='cloudsql-pgvector':\n",
        "        await(store_kgq_embeddings(df_kgq,\n",
        "                                   project_id=PROJECT_ID,\n",
        "                                    instance_name=PG_INSTANCE,\n",
        "                                    database_name=PG_DATABASE,\n",
        "                                    schema=None,\n",
        "                                    database_user=PG_USER,\n",
        "                                    database_password=PG_PASSWORD,\n",
        "                                    region=PG_REGION,\n",
        "                                    VECTOR_STORE = VECTOR_STORE\n",
        "                                    ))\n",
        "    print('Done!!')\n",
        "\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è WARNING: No Known Good Queries are provided to create query cache for Few shot examples!\")\n",
        "    print(\"Creating a query cache is highly recommended for best outcomes\")\n",
        "    print(\"If no Known Good Queries for the dataset are availabe at this time, you can use 3_LoadKnownGoodSQL.ipynb to load them later!!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ü•Å If all the above steps are executed suucessfully, the following should be set up:\n",
        "\n",
        "* GCP project and all the required IAM permissions\n",
        "\n",
        "* Environment to run the solution\n",
        "\n",
        "* Data source and Vector store for the solution"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
